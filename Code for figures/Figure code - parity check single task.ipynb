{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44c12ea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\plogo\\anaconda3\\envs\\cnn\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Importing libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from matplotlib import style\n",
    "import itertools\n",
    "from scipy.optimize import curve_fit\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32b9e33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "\n",
    "n_tasks = 1 # number of tasks created\n",
    "len_taskcode = 8 # length of the code defining the task. This will go at the beginning of each entry\n",
    "num_checks = 3  # Length of the associated integer list for each task. Must be less than len_message\n",
    "len_message = 12  # Maximum integer value in the associated integer list\n",
    "#num_samples_list = np.logspace(start=3, stop=5, num=10, base=10, dtype=int) # number of samples created in the dataset\n",
    "num_samples_list = [400]\n",
    "samples_per_task = 200\n",
    "# Define hyperparameters\n",
    "input_size = len_taskcode + len_message\n",
    "output_size = 1\n",
    "learning_rate = 0.005\n",
    "batch_size = 32\n",
    "num_epochs = 30000\n",
    "test_frac = 0.2\n",
    "num_layers = 6\n",
    "hidden_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe378a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary functions\n",
    "\n",
    "def generate_random_binary_string(length):\n",
    "    binary_string = ''.join(np.random.choice(['0', '1'], size=length))\n",
    "    return binary_string\n",
    "\n",
    "def generate_dict(n_tasks, len_taskcode, num_checks, len_message):\n",
    "    unique_strings = set()\n",
    "    tasks_dict = {}\n",
    "    \n",
    "    if n_tasks > np.power(2, len_taskcode):\n",
    "        print(\"Error: n_tasks is too large\")\n",
    "        return False\n",
    "\n",
    "    while len(unique_strings) < n_tasks:\n",
    "        binary_string = generate_random_binary_string(len_taskcode)\n",
    "\n",
    "        if binary_string not in unique_strings:\n",
    "            unique_strings.add(binary_string)\n",
    "\n",
    "            integer_list = np.random.choice(range(len_message), size=num_checks, replace=False).tolist()\n",
    "            tasks_dict[binary_string] = integer_list\n",
    "\n",
    "    return tasks_dict\n",
    "\n",
    "\n",
    "def generate_dataset(tasks_dict, num_samples):\n",
    "  data = np.zeros((num_samples, len_taskcode + len_message))\n",
    "  value = np.zeros(num_samples)\n",
    "  for i in range(num_samples):\n",
    "    rand_task = np.random.choice(list(tasks_dict))\n",
    "    rand_checkbits = tasks_dict[rand_task]\n",
    "    message = generate_random_binary_string(len_message)\n",
    "    parity_bit = 0\n",
    "    for j in rand_checkbits:\n",
    "      parity_bit += int(message[j])\n",
    "    parity = parity_bit % 2\n",
    "    data[i] = np.concatenate((np.array(list(rand_task)), np.array(list(message))))\n",
    "    value[i] = parity\n",
    "  return [data, value]\n",
    "\n",
    "def generate_dataset_for_task(task_code, num_samples):\n",
    "  data = np.zeros((num_samples, len_taskcode + len_message))\n",
    "  value = np.zeros(num_samples)\n",
    "  for i in range(num_samples):\n",
    "    rand_task = task_code\n",
    "    rand_checkbits = tasks_dict[rand_task]\n",
    "    message = generate_random_binary_string(len_message)\n",
    "    parity_bit = 0\n",
    "    for j in rand_checkbits:\n",
    "      parity_bit += int(message[j])\n",
    "    parity = parity_bit % 2\n",
    "    data[i] = np.concatenate((np.array(list(rand_task)), np.array(list(message))))\n",
    "    value[i] = parity\n",
    "  return [data, value]\n",
    "\n",
    "# IS NOT USING BATCH NORMALISATION\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size, output_size, num_layers, hidden_size):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(nn.Linear(input_size, hidden_size))\n",
    "        \n",
    "        for _ in range(num_layers - 2):\n",
    "            self.layers.append(nn.Linear(hidden_size, hidden_size))\n",
    "        \n",
    "        self.layers.append(nn.Linear(hidden_size, output_size))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers[:-1]:\n",
    "            x = F.relu(layer(x))\n",
    "        \n",
    "        x = self.layers[-1](x)\n",
    "        return x\n",
    "\n",
    "# Define a custom dataset\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.data = dataframe.iloc[:, :-1].values\n",
    "        self.target = dataframe.iloc[:, -1].values\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.from_numpy(self.data[idx]).float()\n",
    "        y = torch.tensor(self.target[idx]).float()\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "007bb931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tasks_dict =  dict_items([('11001111', [9, 5, 1])])\n",
      "400 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 5499/30000 [13:11<58:45,  6.95it/s]   \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 46\u001b[0m\n\u001b[0;32m     42\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m inputs, labels \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m---> 46\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;66;03m# Compute loss\u001b[39;00m\n\u001b[0;32m     49\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\cnn\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[3], line 71\u001b[0m, in \u001b[0;36mNeuralNetwork.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]:\n\u001b[1;32m---> 71\u001b[0m         x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     73\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m](x)\n\u001b[0;32m     74\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\cnn\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\cnn\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training the model and creating the plot.\n",
    "\n",
    "for num_samples in num_samples_list:\n",
    "    # Train the model\n",
    "    tasks_dict = generate_dict(n_tasks, len_taskcode, num_checks, len_message)\n",
    "    print(\"tasks_dict = \", tasks_dict.items()) # To spot any irregularities or patterns in the features being learned\n",
    "    [data, value] = generate_dataset(tasks_dict, num_samples)\n",
    "\n",
    "    # Create a dataframe for training\n",
    "    df = pd.DataFrame(np.concatenate((data, value.reshape(-1, 1)), axis=1), columns=[f'feature_{i}' for i in range(len_taskcode + len_message)] + ['target'])\n",
    "\n",
    "    # Split the data into training and test sets\n",
    "    train_df, test_df = train_test_split(df, test_size=test_frac)\n",
    "\n",
    "    # Create instances of the neural network\n",
    "    model = NeuralNetwork(input_size, output_size, num_layers, hidden_size)\n",
    "\n",
    "    # Define loss function and optimizer (same as before)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Create DataLoaders for the training and test data\n",
    "    train_dataset = CustomDataset(train_df)\n",
    "    test_dataset = CustomDataset(test_df)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    train_loss_list = np.empty(num_epochs)\n",
    "    test_loss_list = np.empty(num_epochs)\n",
    "\n",
    "    train_accuracy_list = np.empty(num_epochs)\n",
    "    test_accuracy_list = np.empty(num_epochs)\n",
    "\n",
    "    accuracy_array = np.zeros((n_tasks, num_epochs)) # such that each row is the accuracy for that specific task over all epochs\n",
    "\n",
    "    print(f'{num_samples} samples')\n",
    "\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        model.train()\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(outputs, labels.unsqueeze(1))\n",
    "\n",
    "            # Compute predictions\n",
    "            predictions = (outputs >= 0.5).squeeze().long()\n",
    "\n",
    "            # Compute accuracy\n",
    "            train_correct += (predictions == labels).sum().item()\n",
    "            train_total += labels.size(0)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        # Calculate average train loss\n",
    "        train_loss = train_loss / len(train_dataset)\n",
    "        # Calculate average train accuracy\n",
    "        train_accuracy = train_correct / train_total\n",
    "\n",
    "        # Evaluate the model on the test set\n",
    "        model.eval()\n",
    "        test_loss = 0.0\n",
    "        test_correct = 0\n",
    "        test_total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in test_loader:\n",
    "                outputs = model(inputs)\n",
    "                # Compute predictions\n",
    "                predictions = (outputs >= 0.5).squeeze().long()\n",
    "\n",
    "                # Compute accuracy\n",
    "                test_correct += (predictions == labels).sum().item()\n",
    "                test_total += labels.size(0)\n",
    "\n",
    "                loss = criterion(outputs, labels.unsqueeze(1))\n",
    "                test_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "            # Calculate average test loss\n",
    "            test_loss = test_loss / len(test_dataset)\n",
    "\n",
    "        # Calculate average test accuracy\n",
    "        test_accuracy = test_correct / test_total\n",
    "\n",
    "        # Update the loss lists\n",
    "        train_loss_list[epoch] = train_loss\n",
    "        test_loss_list[epoch] = test_loss\n",
    "\n",
    "        train_accuracy_list[epoch] = train_accuracy\n",
    "        test_accuracy_list[epoch] = test_accuracy\n",
    "\n",
    "        # Find accuracy data\n",
    "        tasks_list = list(tasks_dict.keys())\n",
    "        for i in range(n_tasks):\n",
    "          task_code = tasks_list[i]\n",
    "          #print(f\"task_code = \", task_code)\n",
    "          [data_per_task, value_per_task] = generate_dataset_for_task(task_code, samples_per_task)\n",
    "          #print(data_per_task)\n",
    "          df_per_task = pd.DataFrame(np.concatenate((data_per_task, value_per_task.reshape(-1, 1)), axis=1), columns=[f'feature_{i}' for i in range(len_taskcode + len_message)] + ['target'])\n",
    "          #print(df_per_task)\n",
    "          dataset_per_task = CustomDataset(df_per_task)\n",
    "          loader_per_task = DataLoader(dataset_per_task, batch_size=batch_size, shuffle=True)\n",
    "          model.eval()\n",
    "          task_correct = 0\n",
    "          task_total = 0\n",
    "          with torch.no_grad():\n",
    "            for inputs, labels in loader_per_task:\n",
    "                outputs = model(inputs)\n",
    "                # Compute predictions\n",
    "                predictions = (outputs >= 0.5).squeeze().long()\n",
    "\n",
    "                # Compute accuracy\n",
    "                task_correct += (predictions == labels).sum().item()\n",
    "                task_total += labels.size(0)\n",
    "\n",
    "            task_accuracy = task_correct / task_total\n",
    "            accuracy_array[(i, epoch)] = task_accuracy\n",
    "\n",
    "\n",
    "    # Create subplots with 1 row and 2 columns\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "    # Plot the training and test losses on the first subplot\n",
    "    axs[0].plot(range(1, num_epochs+1), train_loss_list, label='Train Loss')\n",
    "    axs[0].plot(range(1, num_epochs+1), test_loss_list, label='Test Loss')\n",
    "    axs[0].set_xlabel('Epoch')\n",
    "    axs[0].set_ylabel('Loss')\n",
    "    axs[0].legend()\n",
    "\n",
    "    # Plot the training and test accuracies on the second subplot\n",
    "    axs[1].plot(range(1, num_epochs+1), train_accuracy_list, label='Train Accuracy')\n",
    "    axs[1].plot(range(1, num_epochs+1), test_accuracy_list, label='Test Accuracy')\n",
    "    axs[1].set_xlabel('Epoch')\n",
    "    axs[1].set_ylabel('Accuracy')\n",
    "    axs[1].legend()\n",
    "\n",
    "    plt.suptitle(f'Model performance for {num_samples} data points')\n",
    "    # Show the subplots\n",
    "    plt.show()\n",
    "    \n",
    "# Display model parameter number. If model is changed, should go into the loop\n",
    "pytorch_total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total model parameters = {pytorch_total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9aa2ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the plot only considering the area around which learning occurs\n",
    "\n",
    "# Create subplots with 1 row and 2 columns\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "start = 2000\n",
    "end = 3400\n",
    "x = range(start, end)\n",
    "new_train_loss_list = train_loss_list[start:end]\n",
    "new_test_loss_list = test_loss_list[start:end]\n",
    "new_train_accuracy_list = train_accuracy_list[start:end]\n",
    "new_test_accuracy_list = test_accuracy_list[start:end]\n",
    "\n",
    "# Plot the training and test losses on the first subplot\n",
    "axs[0].plot(x, new_train_loss_list, label='Train Loss')\n",
    "axs[0].plot(x, new_test_loss_list, label='Test Loss')\n",
    "axs[0].set_xlabel('Epoch')\n",
    "axs[0].set_ylabel('Loss')\n",
    "axs[0].legend()\n",
    "\n",
    "# Plot the training and test accuracies on the second subplot\n",
    "axs[1].plot(x, new_train_accuracy_list, label='Train Accuracy')\n",
    "axs[1].plot(x, new_test_accuracy_list, label='Test Accuracy')\n",
    "axs[1].set_xlabel('Epoch')\n",
    "axs[1].set_ylabel('Accuracy')\n",
    "axs[1].legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61551781",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
