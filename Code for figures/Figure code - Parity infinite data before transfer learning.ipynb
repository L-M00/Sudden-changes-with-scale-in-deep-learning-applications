{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d844f84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library import cell\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from matplotlib import style\n",
    "import itertools\n",
    "from scipy.optimize import curve_fit\n",
    "import seaborn as sns\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13a5bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function cell\n",
    "\n",
    "def generate_random_binary_string(length):\n",
    "    binary_string = ''.join(np.random.choice(['0', '1'], size=length))\n",
    "    return binary_string\n",
    "\n",
    "def generate_dict(n_tasks, len_taskcode, num_checks, len_message):\n",
    "    unique_strings = set()\n",
    "    tasks_dict = {}\n",
    "    if n_tasks > np.power(2, len_taskcode):\n",
    "        print(\"Error: n_tasks is too large\")\n",
    "        return False\n",
    "    while len(unique_strings) < n_tasks:\n",
    "        binary_string = generate_random_binary_string(len_taskcode)\n",
    "        if binary_string not in unique_strings:\n",
    "            unique_strings.add(binary_string)\n",
    "            integer_list = np.random.choice(range(len_message), size=num_checks, replace=False).tolist()\n",
    "            tasks_dict[binary_string] = integer_list\n",
    "    return tasks_dict\n",
    "\n",
    "# Currently following a probability distribution given by the probabilities list\n",
    "def generate_dataset(tasks_dict, num_samples):\n",
    "    data = np.zeros((num_samples, len_taskcode + len_message))\n",
    "    value = np.zeros(num_samples)\n",
    "    \n",
    "    task_list = list(tasks_dict)\n",
    "    rank = np.arange(1, len(task_list) + 1)\n",
    "    #probabilities = [1/(x**2) for x in rank]\n",
    "    #probabilities = [np.exp(-0.1*x) for x in rank]\n",
    "    #probabilities = [0.9, 0.09, 0.009, 0.001]\n",
    "    probabilities = [1/(2**x) for x in rank]\n",
    "    task_probabilities = probabilities / np.sum(probabilities)  # normalize probabilities\n",
    "    #print('tasks dict = ', tasks_dict)\n",
    "    #print('task_probabilities = ', task_probabilities)\n",
    "    for i in range(num_samples):\n",
    "        rand_task = np.random.choice(task_list, p=task_probabilities)\n",
    "        rand_checkbits = tasks_dict[rand_task]\n",
    "        message = generate_random_binary_string(len_message)\n",
    "        parity_bit = 0\n",
    "        for j in rand_checkbits:\n",
    "            parity_bit += int(message[j])\n",
    "        parity = parity_bit % 2\n",
    "        data[i] = np.concatenate((np.array(list(rand_task)), np.array(list(message))))\n",
    "        value[i] = parity\n",
    "    \n",
    "    return [data, value]\n",
    "\n",
    "def generate_dataset_for_task(task_code, num_samples):\n",
    "  data = np.zeros((num_samples, len_taskcode + len_message))\n",
    "  value = np.zeros(num_samples)\n",
    "  for i in range(num_samples):\n",
    "    rand_task = task_code\n",
    "    rand_checkbits = tasks_dict[rand_task]\n",
    "    message = generate_random_binary_string(len_message)\n",
    "    parity_bit = 0\n",
    "    for j in rand_checkbits:\n",
    "      parity_bit += int(message[j])\n",
    "    parity = parity_bit % 2\n",
    "    data[i] = np.concatenate((np.array(list(rand_task)), np.array(list(message))))\n",
    "    value[i] = parity\n",
    "  return [data, value]\n",
    "\n",
    "# IS NOT USING BATCH NORMALISATION\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size, output_size, num_layers, hidden_size):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(nn.Linear(input_size, hidden_size))\n",
    "        \n",
    "        for _ in range(num_layers - 2):\n",
    "            self.layers.append(nn.Linear(hidden_size, hidden_size))\n",
    "        \n",
    "        self.layers.append(nn.Linear(hidden_size, output_size))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers[:-1]:\n",
    "            x = F.relu(layer(x))\n",
    "        \n",
    "        x = self.layers[-1](x)\n",
    "        return x\n",
    "\n",
    "# Define a custom dataset\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.data = dataframe.iloc[:, :-1].values\n",
    "        self.target = dataframe.iloc[:, -1].values\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.from_numpy(self.data[idx]).float()\n",
    "        y = torch.tensor(self.target[idx]).float()\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9d9242",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting functions\n",
    "\n",
    "def epoch_plots(num_epochs, loss_list, accuracy_list):\n",
    "    # Create subplots with 1 row and 2 columns\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "    # Plot the training and test losses on the first subplot\n",
    "    axs[0].plot(range(1, num_epochs+1), loss_list, label='Loss')\n",
    "    axs[0].set_xlabel('Epoch')\n",
    "    axs[0].set_ylabel('Loss')\n",
    "    axs[0].legend()\n",
    "\n",
    "    # Plot the training and test accuracies on the second subplot\n",
    "    axs[1].plot(range(1, num_epochs+1), accuracy_list, label='Accuracy')\n",
    "    axs[1].set_xlabel('Epoch')\n",
    "    axs[1].set_ylabel('Accuracy')\n",
    "    axs[1].legend()\n",
    "\n",
    "    # Adjust the spacing between subplots\n",
    "    #plt.tight_layout()\n",
    "    plt.suptitle(f'Model performance')\n",
    "    # Show the subplots\n",
    "    plt.show()\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465c09f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the run before the transfer learning test. A standard parity check run\n",
    "# Note that when generating data a 1/2^n distribution is being applied.\n",
    "\n",
    "# Parameters\n",
    "n_tasks = 16 # number of tasks created\n",
    "len_taskcode = 8 # length of the code defining the task. This will go at the beginning of each entry\n",
    "num_checks = 3  # Length of the associated integer list for each task. Must be less than len_message\n",
    "len_message = 8  # Maximum integer value in the associated integer list\n",
    "#num_samples_list = np.logspace(start=3, stop=5, num=10, base=10, dtype=int) # number of samples created in the dataset\n",
    "num_samples = 5000 # This is the number of samples created per epoch\n",
    "samples_per_task = 200\n",
    "task_sample_freq = 10 # How many epochs between sampling of tasks\n",
    "# Define hyperparameters\n",
    "input_size = len_taskcode + len_message\n",
    "output_size = 1\n",
    "learning_rate = 0.001\n",
    "batch_size = 32\n",
    "num_epochs = 7000 # To be halted post grok\n",
    "num_layers = 8\n",
    "hidden_size = 256\n",
    "\n",
    "plot_freq = 500 # How many epoch between creating a plot of progress\n",
    "#moving_avg = 1 # How the averaging is done in accuracy by task plots. Seems to cause bugs\n",
    "\n",
    "# Train the model\n",
    "tasks_dict = generate_dict(n_tasks, len_taskcode, num_checks, len_message) # Moved intentionally earlier\n",
    "print(\"tasks_dict = \", tasks_dict.items())\n",
    "\n",
    "# Create instances of the neural network\n",
    "model = NeuralNetwork(input_size, output_size, num_layers, hidden_size)\n",
    "\n",
    "# Define loss function and optimizer (same as before)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "loss_list = np.empty(num_epochs)\n",
    "accuracy_list = np.empty(num_epochs)\n",
    "accuracy_array = np.zeros((n_tasks, num_epochs//task_sample_freq)) # such that each row is the accuracy for that specific task over all epochs\n",
    "acc_counter = 0\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    \n",
    "    [data, value] = generate_dataset(tasks_dict, num_samples)\n",
    "    # Create a dataframe for training\n",
    "    df = pd.DataFrame(np.concatenate((data, value.reshape(-1, 1)), axis=1), columns=[f'feature_{i}' for i in range(len_taskcode + len_message)] + ['target'])\n",
    "    \n",
    "    # Create DataLoaders for the training and test data\n",
    "    dataset = CustomDataset(df)\n",
    "    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.train()\n",
    "\n",
    "    for inputs, labels in data_loader:\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, labels.unsqueeze(1))\n",
    "\n",
    "        # Compute predictions\n",
    "        predictions = (outputs >= 0.5).squeeze().long()\n",
    "\n",
    "        # Compute accuracy\n",
    "        correct += (predictions == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss += loss.item() * inputs.size(0)\n",
    "\n",
    "    # Calculate average train loss\n",
    "    avg_loss = loss / len(dataset)\n",
    "    # Calculate average train accuracy\n",
    "    avg_accuracy = correct / total\n",
    "\n",
    "    # Update the loss list\n",
    "    loss_list[epoch] = avg_loss\n",
    "    accuracy_list[epoch] = avg_accuracy\n",
    "    \n",
    "    if epoch%task_sample_freq == 0:\n",
    "        # Find accuracy data\n",
    "        tasks_list = list(tasks_dict.keys())\n",
    "        for i in range(n_tasks):\n",
    "          task_code = tasks_list[i]\n",
    "          #print(f\"task_code = \", task_code)\n",
    "          [data_per_task, value_per_task] = generate_dataset_for_task(task_code, samples_per_task)\n",
    "          #print(data_per_task)\n",
    "          df_per_task = pd.DataFrame(np.concatenate((data_per_task, value_per_task.reshape(-1, 1)), axis=1), columns=[f'feature_{i}' for i in range(len_taskcode + len_message)] + ['target'])\n",
    "          #print(df_per_task)\n",
    "          dataset_per_task = CustomDataset(df_per_task)\n",
    "          loader_per_task = DataLoader(dataset_per_task, batch_size=batch_size, shuffle=True)\n",
    "          model.eval()\n",
    "          task_correct = 0\n",
    "          task_total = 0\n",
    "          with torch.no_grad():\n",
    "            for inputs, labels in loader_per_task:\n",
    "                outputs = model(inputs)\n",
    "                # Compute predictions\n",
    "                predictions = (outputs >= 0.5).squeeze().long()\n",
    "\n",
    "                # Compute accuracy\n",
    "                task_correct += (predictions == labels).sum().item()\n",
    "                task_total += labels.size(0)\n",
    "                #print(f'inputs = ', inputs)\n",
    "                #print(f'labels.size(0) = ', labels.size(0))\n",
    "\n",
    "                #loss = criterion(outputs, labels.unsqueeze(1)) #if in future I want to calculate loss\n",
    "                #test_loss += loss.item() * inputs.size(0)\n",
    "            task_accuracy = task_correct / task_total\n",
    "            #print(f\"task_accuracy = \", task_accuracy)\n",
    "            accuracy_array[(i, acc_counter)] = task_accuracy\n",
    "        acc_counter += 1\n",
    "        \n",
    "    if epoch % plot_freq == 0 and epoch != 0:\n",
    "        epoch_plots(epoch, loss_list[:epoch], accuracy_list[:epoch])\n",
    "\n",
    "epoch_plots(num_epochs, loss_list, accuracy_list)\n",
    "    \n",
    "# Display model parameter number. If model is changed, should go into the loop\n",
    "pytorch_total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total model parameters = {pytorch_total_params}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
