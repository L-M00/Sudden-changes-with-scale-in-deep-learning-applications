{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38c091c-b166-4ce2-8e14-9b2c33be8aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "\n",
    "!pip install cellpylib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef43913-c56d-4a3d-9635-8393e1e18a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "\n",
    "import cellpylib as cpl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch import Tensor\n",
    "import seaborn as sns\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a5eeba-055c-44d4-9eb0-332e3a6d0485",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining plots and the neural network architecture uses\n",
    "\n",
    "# Functions (by themselves)\n",
    "\n",
    "# Model Initialisation, WITH BATCH NORMALISATION\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "        self.bn2 = nn.BatchNorm1d(num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.bn2(out)\n",
    "        return out\n",
    "\n",
    "def create_data(data_size, programmes_considered, number_of_samples, timesteps):\n",
    "  # creating data set, by randomly initialising number_of_samples times with random programmes.\n",
    "  dataset = np.empty(shape=(number_of_samples, data_size), dtype=int) # each row is data_size length, with number_of_samples rows\n",
    "  labels = np.empty(shape=(1, number_of_samples), dtype=int)\n",
    "  #print(f\"labels = \", labels)\n",
    "  #print(dataset)\n",
    "  for i in range(number_of_samples):\n",
    "\n",
    "    #randomly selecting a rule number\n",
    "    rule_number = np.random.choice(programmes_considered)\n",
    "    #print(f\"Considering rule_number = \", rule_number)\n",
    "    cellular_automaton = cpl.init_random(data_size)\n",
    "    cellular_automaton = cpl.evolve(cellular_automaton, timesteps=timesteps, memoize=True, apply_rule=lambda n, c, t: cpl.nks_rule(n, rule_number))\n",
    "    #print(cellular_automaton[-1])\n",
    "    dataset[i] = cellular_automaton[-1]\n",
    "    labels[:,i] = rule_number\n",
    "\n",
    "  return [dataset, labels]\n",
    "\n",
    "def data_split(data):\n",
    "\n",
    "  np.random.shuffle(data) #randomly select parts of the dataset\n",
    "  #train_ratio = train_ratio # this reserves 80% for training, 20% for testing\n",
    "  split_index = int(len(data) * train_ratio)\n",
    "\n",
    "  train_data = data[:split_index]\n",
    "  test_data = data[split_index:]\n",
    "  #print(f\"train_data = \", train_data)\n",
    "  #print(f\"test_data = \", test_data)\n",
    "\n",
    "  # Separate the dataset and labels from the training and testing sets\n",
    "  train_dataset, train_labels = zip(*train_data)\n",
    "  test_dataset, test_labels = zip(*test_data)\n",
    "\n",
    "  data_split = [train_dataset, train_labels, test_dataset, test_labels]\n",
    "  return data_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c445bb9-9902-47d4-bd4f-e51df570d5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code is copied over from the google drive. Now with significant alterations\n",
    "from tqdm import tqdm\n",
    "#@title Set System Parameters\n",
    "data_size = 100 # the number of data points in each row of data\n",
    "programmes_considered = np.arange(0,256,1) # the set of programmes being considered. For the 1D case it makes sense to consider all 0 to 255 programmes.\n",
    "number_of_samples = 2560*2 # the number of random times the output of a programme will be calculated, given random inputs\n",
    "timesteps = 100 # the number of timesteps which each programme is run for before the output is used to train the model\n",
    "\n",
    "#laterly used parameters\n",
    "num_epochs = 300  # Number of training epochs\n",
    "hidden_size = 256  # Update with the desired size of the hidden layer\n",
    "learning_rate = 0.001 # learning rate used later in the optimizer\n",
    "batch_size = 32 # Batch size used when creating the train and test datasets. Note that 5 is likely much too low, and 32 would be more suitable for this problem.\n",
    "train_ratio = 0.8 # Specifies how much of the set will be used to training vs testing\n",
    "\n",
    "num_repeats = 3\n",
    "\n",
    "fig, axs = plt.subplots(1, num_repeats, figsize=(5 * num_repeats, 5), sharey=True)\n",
    "\n",
    "for repeat_idx in range(num_repeats):\n",
    "    \n",
    "    # Define the input size, hidden size, and number of classes\n",
    "    input_size = data_size  # Update with the actual input size\n",
    "    #hidden_size = 64  # Update with the desired size of the hidden layer\n",
    "    num_classes = len(programmes_considered)+1  # Number of potential classes\n",
    "    \n",
    "    # Create an instance of the neural network\n",
    "    model = NeuralNetwork(input_size, hidden_size, num_classes)\n",
    "    \n",
    "    #@title Evaluating the train and test splits\n",
    "    [dataset, labels] = create_data(data_size, programmes_considered, number_of_samples, timesteps)\n",
    "    labels = labels[0]\n",
    "    # Shifting the labels such that they are indexed from 0. Required for cross entropy to work\n",
    "    labels = [x - min(labels) for x in labels]\n",
    "    data = [(data_sample, label) for data_sample, label in zip(dataset, labels)]\n",
    "    [train_dataset, train_labels, test_dataset, test_labels] = data_split(data)\n",
    "    \n",
    "    #@title Setting up Training\n",
    "    \n",
    "    # Define the loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Prepare your dataset and data loaders (train_loader and test_loader)\n",
    "    # Create a TensorDataset from the train_dataset and train_labels\n",
    "    #dataset = torch.utils.data.TensorDataset([train_dataset, train_labels])\n",
    "    tensor_train_dataset = TensorDataset(Tensor(train_dataset), Tensor(train_labels))\n",
    "    tensor_test_dataset = TensorDataset(Tensor(test_dataset), Tensor(test_labels))\n",
    "    \n",
    "    train_loader = DataLoader(tensor_train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(tensor_test_dataset, batch_size=batch_size, shuffle=True)\n",
    "    #print(f\"train_loader = \", train_loader)\n",
    "    \n",
    "    # Creating variables to track the change in error over time\n",
    "    training_loss = np.empty(num_epochs)\n",
    "    test_loss = np.empty(num_epochs)\n",
    "    \n",
    "    # Training loop\n",
    "    #num_epochs = 100  # Number of training epochs\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        for data, labels in train_loader:\n",
    "            #print(f\"data = \", data)\n",
    "            #print(f\"labels = \", labels)\n",
    "            labels = labels.long() #required for the calculation of CrossEntropyLoss\n",
    "            #print(f\"labels = \", labels)\n",
    "            # Forward pass\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, labels)\n",
    "    \n",
    "            # monitoring test loss during training\n",
    "            for data, labels in test_loader:\n",
    "                labels_test = labels.long()\n",
    "                outputs_test = model(data)\n",
    "                loss_test = criterion(outputs_test, labels_test)\n",
    "                test_loss[epoch] = loss_test.item()\n",
    "    \n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "        # Print the loss after each epoch\n",
    "        #if epoch%10==0:\n",
    "        #    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}\")\n",
    "        training_loss[epoch] = loss.item()\n",
    "    \n",
    "    # Evaluation on the training dataset (relevant for overparametrisation or otherwise deep learning systems)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for data, labels in train_loader:\n",
    "            outputs = model(data)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "        accuracy = 100 * correct / total\n",
    "        print(f\"Accuracy on the train set: {accuracy}%\")\n",
    "    \n",
    "    # Evaluation on the testing dataset\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for data, labels in test_loader:\n",
    "            outputs = model(data)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "        accuracy = 100 * correct / total\n",
    "        print(f\"Accuracy on the test set: {accuracy}%\")\n",
    "    \n",
    "    parameter_number = sum(p.numel() for p in model.parameters())\n",
    "    epochs = np.arange(0, num_epochs, 1)\n",
    "    axs[repeat_idx].plot(epochs, training_loss, test_loss)\n",
    "    axs[repeat_idx].set_xlabel(\"Epoch\")\n",
    "    axs[repeat_idx].set_ylabel(\"Cross Entropy Loss\")\n",
    "    axs[repeat_idx].title.set_text(f\"Run {repeat_idx+1}\")\n",
    "    axs[repeat_idx].set_ylim(bottom = 0)\n",
    "    axs[repeat_idx].legend([\"Training Loss\", \"Test Loss\"])\n",
    "\n",
    "#plt.tight_layout()\n",
    "plt.suptitle(f'Model Performance')\n",
    "# Show the subplots\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
