{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7dacc9ce-7dea-498c-95b7-144f7fdc75ed",
   "metadata": {},
   "source": [
    "This notebook serves to test generations of cellular automata of different forms, presently focusing on 1 dimensional automata.\n",
    "\n",
    "This builds on the 'Ceuular Automata Local Host' file. The intention is to set it up to train on 'infinte data' (i.e. generate the data as training is performed) with the statistical changes required to see grokking.\n",
    "\n",
    "Perhaps this can be used for data compression? Train a network to see patterns which can be generated by simple programmes, and then a separate programme to correct that output with the true data in as little space as possible (i.e. turn lossy into lossless compression). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85bd4ec-f736-4cde-b07c-8b2c3b1d4df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "\n",
    "!pip install cellpylib\n",
    "\n",
    "import cellpylib as cpl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch import Tensor\n",
    "import seaborn as sns\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "04036f71-d460-4427-8bcf-b1c11320ce52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Parameters\n",
    "\n",
    "# Data generation parameters\n",
    "data_size = 100 # the number of data points in each row of data\n",
    "#programmes_considered = np.arange(0,256,1) # the set of programmes being considered. For the 1D case it makes sense to consider all 0 to 255 programmes.\n",
    "#number_of_samples = 2000 # the number of random times the output of a programme will be calculated, given random inputs\n",
    "timesteps = 100 # the number of timesteps which each programme is run for before the output is used to train the model\n",
    "\n",
    "# Training parameters\n",
    "num_epochs = 2000  # Number of training epochs\n",
    "hidden_size = 256  # Update with the desired size of the hidden layer\n",
    "learning_rate = 0.001 # learning rate used later in the optimizer\n",
    "batch_size = 32 # Batch size used when creating the train and test datasets. Note that 5 is likely much too low, and 32 would be more suitable for this problem.\n",
    "train_ratio = 0.99 # Specifies how much of the set will be used to training vs testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "1159bad9-1186-478a-9408-7855062cae7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Programme distribution\n",
    "\n",
    "programmes_prob_distribution = []\n",
    "for i in range(256):\n",
    "    programmes_prob_distribution.append((i+10)**(-1))\n",
    "programmes_prob_distribution = np.array(programmes_prob_distribution) \n",
    "# Note that this distribution will be normalised inside the data pre-processing step if not already normalised here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "454bed33-e0a2-4f2e-8d0e-babb41f33c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Initialisation / Training setup\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "        self.bn2 = nn.BatchNorm1d(num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.bn2(out)\n",
    "        return out\n",
    "\n",
    "# Define the input size, hidden size, and number of classes\n",
    "input_size = data_size  # Update with the actual input size\n",
    "#hidden_size = 64  # Update with the desired size of the hidden layer\n",
    "#num_classes = len(programmes_considered)+1  # Number of potential classes\n",
    "num_classes = 256 #Number of potential classes, here stuck at 256\n",
    "\n",
    "# Create an instance of the neural network\n",
    "model = NeuralNetwork(input_size, hidden_size, num_classes)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "3f9263fd-ea13-4f3a-980d-ef1e3ee1c536",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data generation functions (where the programmes_considered have a probability distribution)\n",
    "\n",
    "def create_data(data_size, programmes_prob_distribution, number_of_samples, timesteps):\n",
    "\n",
    "    # Creating the dataset and labels variables to be populated later\n",
    "    dataset = np.empty(shape=(number_of_samples, data_size), dtype=int) # each row is data_size length, with number_of_samples rows\n",
    "    labels = np.empty(shape=(1, number_of_samples), dtype=int)\n",
    "\n",
    "    # Stating the space of considered programmes\n",
    "    programmes = np.arange(0,256,1)\n",
    "\n",
    "    # Normalising the distribution in case it is not already normalised\n",
    "    programmes_total = sum(programmes_prob_distribution)\n",
    "    programmes_prob_distribution_norm = [x / programmes_total for x in programmes_prob_distribution]\n",
    "    \n",
    "    for i in range(number_of_samples):\n",
    "\n",
    "        # Randomly selecting a rule number according to the probability distribution given\n",
    "        rule_number = np.random.choice(a = programmes, size=None, replace=True, p = programmes_prob_distribution_norm)\n",
    "        #print(f\"Considering rule_number = \", rule_number)\n",
    "        cellular_automaton = cpl.init_random(data_size)\n",
    "        cellular_automaton = cpl.evolve(cellular_automaton, timesteps=timesteps, memoize=True, apply_rule=lambda n, c, t: cpl.nks_rule(n, rule_number))\n",
    "        #print(cellular_automaton[-1])\n",
    "        dataset[i] = cellular_automaton[-1]\n",
    "        labels[:,i] = rule_number\n",
    "\n",
    "    return [dataset, labels]\n",
    "\n",
    "\n",
    "def data_split(data, train_ratio):\n",
    "\n",
    "    np.random.shuffle(data) #randomly select parts of the dataset\n",
    "    #train_ratio = train_ratio # this reserves 80% for training, 20% for testing\n",
    "    split_index = int(len(data) * train_ratio)\n",
    "    \n",
    "    train_data = data[:split_index]\n",
    "    test_data = data[split_index:]\n",
    "    #print(f\"train_data = \", train_data)\n",
    "    #print(f\"test_data = \", test_data)\n",
    "    \n",
    "    # Separate the dataset and labels from the training and testing sets\n",
    "    train_dataset, train_labels = zip(*train_data)\n",
    "    test_dataset, test_labels = zip(*test_data)\n",
    "    \n",
    "    data_split = [train_dataset, train_labels, test_dataset, test_labels]\n",
    "    return data_split\n",
    "\n",
    "def data_loader(data_size, programmes_prob_distribution, number_of_samples, timesteps, train_ratio):\n",
    "\n",
    "    # Generate the data according to input parameters\n",
    "    [dataset, labels] = create_data(data_size, programmes_prob_distribution, number_of_samples, timesteps)\n",
    "    labels = labels[0] # Deal with the fact that the output is a list of a single list\n",
    "\n",
    "    # Shifting the labels such that they are indexed from 0. Required for cross entropy to work\n",
    "    #labels = [x - min(labels) for x in labels] #!!! Not currently shifting labels in a test to alter them later - may help with training in smaller batches\n",
    "    # Use data_split\n",
    "    data = [(data_sample, label) for data_sample, label in zip(dataset, labels)]\n",
    "    [train_dataset, train_labels, test_dataset, test_labels] = data_split(data, train_ratio)\n",
    "\n",
    "    tensor_train_dataset = TensorDataset(Tensor(train_dataset), Tensor(train_labels))\n",
    "    tensor_test_dataset = TensorDataset(Tensor(test_dataset), Tensor(test_labels))\n",
    "    \n",
    "    train_loader = DataLoader(tensor_train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(tensor_test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    return [train_loader, test_loader]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "9a7cf225-70ad-4308-b30d-c741faf1eb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the data only for a specified programme. Returns only a single vector\n",
    "\n",
    "def create_data_single(data_size, rule_number, number_of_samples, timesteps): #here, the rule_number parameter is the programme being considered\n",
    "\n",
    "    dataset = np.empty(shape=(number_of_samples, data_size), dtype=int) # each row is data_size length, with number_of_samples rows\n",
    "    labels = np.empty(shape=(1, number_of_samples), dtype=int)\n",
    "    \n",
    "    for i in range(number_of_samples):\n",
    "        cellular_automaton = cpl.init_random(data_size)\n",
    "        cellular_automaton = cpl.evolve(cellular_automaton, timesteps=timesteps, memoize=True, apply_rule=lambda n, c, t: cpl.nks_rule(n, rule_number))\n",
    "        #print(cellular_automaton[-1])\n",
    "        dataset[i] = cellular_automaton[-1]\n",
    "        labels[:,i] = rule_number\n",
    "\n",
    "    return [dataset, labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "c1c775f2-929c-44b7-924f-48023aa2a9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop (includes data generation). Note that here training and test loss cease to make much sense\n",
    "\n",
    "def main_train(data_size, programmes_prob_distribution, batch_size, timesteps, train_ratio, num_epochs):\n",
    "\n",
    "    # Initisalise training and test loss tracking variables\n",
    "    training_loss = np.empty(num_epochs)\n",
    "\n",
    "    # State which programmes are being considered. In this case, it's all of them.\n",
    "    programmes_considered = np.arange(0,256,1)\n",
    "\n",
    "    # Initialise an array to track not only the general training and test loss, but also the accuracy on individual programme classification during training.\n",
    "    # This is to attempt to see grokking.\n",
    "    # Form: Each row of loss_array is an epoch, each column of loss_array is a binary 1 or 0 based on whether or not it was correctly classified. \n",
    "    # Average over this later\n",
    "    #loss_array = np.empty\n",
    "    \n",
    "    # Each epoch here trains over 1 batch size of data (which at the moment is 32). Each epoch is therefore smaller and better controlled.\n",
    "    for epoch in range(num_epochs):\n",
    "        [train_loader, test_loader] = data_loader(data_size, programmes_prob_distribution, batch_size, timesteps, train_ratio)\n",
    "        for data, labels in train_loader:\n",
    "            # Forward pass\n",
    "            outputs = model(data)\n",
    "    \n",
    "            #Shifting labels for loss calculation\n",
    "            shifted_labels = labels - torch.min(labels)\n",
    "            shifted_labels = shifted_labels.long()\n",
    "            loss = criterion(outputs, shifted_labels)\n",
    "                \n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "        # Print the loss after each epoch\n",
    "        #if epoch%10==0:\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}\")\n",
    "        training_loss[epoch] = loss.item()\n",
    "\n",
    "    return [training_loss, test_loss]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0dfb6cc-8f48-44bd-ab44-22d531aab42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Produce lineplot of data with MatPlotLib\n",
    "#num_epochs = 10000\n",
    "epochs = np.arange(0,num_epochs, 1)\n",
    "\n",
    "parameter_number = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "#plt.plot(epochs, training_loss, test_loss)\n",
    "plt.plot(epochs, training_loss)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Cross Entropy Loss\")\n",
    "plt.title(\"Loss during training, \" + str(number_of_samples) + \" samples, \" + str(data_size) + \" width per entry, \" + str(parameter_number) + \" parameters\")\n",
    "plt.ylim(bottom = 0)\n",
    "#plt.legend([\"Training Loss\", \"Test Loss\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640c724c-aefe-4045-adba-7978d474b674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Produce a Seaborn plot (taking a moving average, and adding standard deviation information)\n",
    "\n",
    "# Redefining epochs here, in case it is not carried over from previous cells. Likely unneccesary\n",
    "epochs = np.arange(0, num_epochs, 1)\n",
    "\n",
    "parameter_number = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "# Taking values from nearby epochs and averaging\n",
    "moving_avg = 5 # The size of the averaging window being used.\n",
    "reshaped_training_loss = np.reshape(training_loss, (-1, moving_avg)) #note that if the length of training_loss is not divisible by 10, the final elements are ignored\n",
    "reshaped_test_loss = np.reshape(test_loss, (-1, moving_avg))\n",
    "reshaped_epochs = np.reshape(epochs, (-1, moving_avg))\n",
    "filtered_epochs = reshaped_epochs[:,0]\n",
    "repeated_filtered_epochs = np.repeat(filtered_epochs, moving_avg)\n",
    "\n",
    "pandas_df = pd.DataFrame({'Training Loss': reshaped_training_loss.flatten(), 'Test Loss': reshaped_test_loss.flatten(), 'Epochs': repeated_filtered_epochs})\n",
    "pandas_df_melted = pd.melt(pandas_df, id_vars = 'Epochs', value_vars = ['Training Loss', 'Test Loss'], var_name='line', value_name = 'Values')\n",
    "\n",
    "\n",
    "sns.lineplot(data=pandas_df_melted, x='Epochs', y='Values', hue='line')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Cross Entropy Loss')\n",
    "plt.title(\"Loss during training, \" + str(number_of_samples) + \" samples, \" + str(data_size) + \" width per entry, \" + str(parameter_number) + \" parameters\")\n",
    "plt.legend(loc='best')\n",
    "plt.rcParams['figure.dpi'] = 300\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "6093ae88-67e2-4fe5-a8d5-8fbe0b6db6d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/2000], Loss: 6.035799026489258\n",
      "Epoch [2/2000], Loss: 6.107973098754883\n",
      "Epoch [3/2000], Loss: 6.181026458740234\n",
      "Epoch [4/2000], Loss: 5.949923992156982\n",
      "Epoch [5/2000], Loss: 6.057840824127197\n",
      "Epoch [6/2000], Loss: 6.040886878967285\n",
      "Epoch [7/2000], Loss: 5.961490154266357\n",
      "Epoch [8/2000], Loss: 5.804179668426514\n",
      "Epoch [9/2000], Loss: 5.974383354187012\n",
      "Epoch [10/2000], Loss: 6.325131893157959\n",
      "Epoch [11/2000], Loss: 6.1790666580200195\n",
      "Epoch [12/2000], Loss: 6.044686794281006\n",
      "Epoch [13/2000], Loss: 5.8875732421875\n",
      "Epoch [14/2000], Loss: 5.920075416564941\n",
      "Epoch [15/2000], Loss: 5.8804612159729\n",
      "Epoch [16/2000], Loss: 6.161624431610107\n",
      "Epoch [17/2000], Loss: 5.791683197021484\n",
      "Epoch [18/2000], Loss: 6.111616611480713\n",
      "Epoch [19/2000], Loss: 5.610601425170898\n",
      "Epoch [20/2000], Loss: 5.720091819763184\n",
      "Epoch [21/2000], Loss: 5.876699447631836\n",
      "Epoch [22/2000], Loss: 5.9889397621154785\n",
      "Epoch [23/2000], Loss: 6.122483730316162\n",
      "Epoch [24/2000], Loss: 5.849305152893066\n",
      "Epoch [25/2000], Loss: 6.016754627227783\n",
      "Epoch [26/2000], Loss: 5.78202486038208\n",
      "Epoch [27/2000], Loss: 6.0212321281433105\n",
      "Epoch [28/2000], Loss: 6.204768657684326\n",
      "Epoch [29/2000], Loss: 5.96631383895874\n",
      "Epoch [30/2000], Loss: 5.852604866027832\n",
      "Epoch [31/2000], Loss: 6.050906181335449\n",
      "Epoch [32/2000], Loss: 6.1020097732543945\n",
      "Epoch [33/2000], Loss: 5.769863128662109\n",
      "Epoch [34/2000], Loss: 6.1113386154174805\n",
      "Epoch [35/2000], Loss: 6.160729885101318\n",
      "Epoch [36/2000], Loss: 5.632446765899658\n",
      "Epoch [37/2000], Loss: 6.265365123748779\n",
      "Epoch [38/2000], Loss: 5.838468074798584\n",
      "Epoch [39/2000], Loss: 5.840796947479248\n",
      "Epoch [40/2000], Loss: 6.022998809814453\n",
      "Epoch [41/2000], Loss: 6.129358768463135\n",
      "Epoch [42/2000], Loss: 6.063231945037842\n",
      "Epoch [43/2000], Loss: 6.026919841766357\n",
      "Epoch [44/2000], Loss: 5.962349891662598\n",
      "Epoch [45/2000], Loss: 6.1513495445251465\n",
      "Epoch [46/2000], Loss: 5.8143086433410645\n",
      "Epoch [47/2000], Loss: 5.687425136566162\n",
      "Epoch [48/2000], Loss: 5.877739906311035\n",
      "Epoch [49/2000], Loss: 6.0137457847595215\n",
      "Epoch [50/2000], Loss: 5.6507391929626465\n",
      "Epoch [51/2000], Loss: 6.063626766204834\n",
      "Epoch [52/2000], Loss: 6.114102363586426\n",
      "Epoch [53/2000], Loss: 5.962586402893066\n",
      "Epoch [54/2000], Loss: 6.067408561706543\n",
      "Epoch [55/2000], Loss: 6.233127593994141\n",
      "Epoch [56/2000], Loss: 5.974524974822998\n",
      "Epoch [57/2000], Loss: 6.095799922943115\n",
      "Epoch [58/2000], Loss: 6.028637886047363\n",
      "Epoch [59/2000], Loss: 6.057885646820068\n",
      "Epoch [60/2000], Loss: 6.071169376373291\n",
      "Epoch [61/2000], Loss: 5.817498683929443\n",
      "Epoch [62/2000], Loss: 5.928385257720947\n",
      "Epoch [63/2000], Loss: 5.857665061950684\n",
      "Epoch [64/2000], Loss: 5.777506351470947\n",
      "Epoch [65/2000], Loss: 5.7669677734375\n",
      "Epoch [66/2000], Loss: 5.81740140914917\n",
      "Epoch [67/2000], Loss: 6.075380802154541\n",
      "Epoch [68/2000], Loss: 5.825084686279297\n",
      "Epoch [69/2000], Loss: 6.1117844581604\n",
      "Epoch [70/2000], Loss: 5.775152683258057\n",
      "Epoch [71/2000], Loss: 5.968190670013428\n",
      "Epoch [72/2000], Loss: 5.980781078338623\n",
      "Epoch [73/2000], Loss: 5.635992050170898\n",
      "Epoch [74/2000], Loss: 5.994873523712158\n",
      "Epoch [75/2000], Loss: 5.9757890701293945\n",
      "Epoch [76/2000], Loss: 5.833187580108643\n",
      "Epoch [77/2000], Loss: 5.942745208740234\n",
      "Epoch [78/2000], Loss: 5.7388410568237305\n",
      "Epoch [79/2000], Loss: 5.983514308929443\n",
      "Epoch [80/2000], Loss: 6.304248809814453\n",
      "Epoch [81/2000], Loss: 5.852220058441162\n",
      "Epoch [82/2000], Loss: 5.882174491882324\n",
      "Epoch [83/2000], Loss: 5.8525071144104\n",
      "Epoch [84/2000], Loss: 6.122462272644043\n",
      "Epoch [85/2000], Loss: 6.0319061279296875\n",
      "Epoch [86/2000], Loss: 5.768961429595947\n",
      "Epoch [87/2000], Loss: 5.982604503631592\n",
      "Epoch [88/2000], Loss: 5.93203592300415\n",
      "Epoch [89/2000], Loss: 6.045125484466553\n",
      "Epoch [90/2000], Loss: 5.894516944885254\n",
      "Epoch [91/2000], Loss: 5.716369152069092\n",
      "Epoch [92/2000], Loss: 5.888055801391602\n",
      "Epoch [93/2000], Loss: 5.673267841339111\n",
      "Epoch [94/2000], Loss: 6.288608551025391\n",
      "Epoch [95/2000], Loss: 5.952230453491211\n",
      "Epoch [96/2000], Loss: 5.691091060638428\n",
      "Epoch [97/2000], Loss: 5.812628746032715\n",
      "Epoch [98/2000], Loss: 5.996700286865234\n",
      "Epoch [99/2000], Loss: 5.852389335632324\n",
      "Epoch [100/2000], Loss: 5.853015899658203\n",
      "Epoch [101/2000], Loss: 5.701447010040283\n",
      "Epoch [102/2000], Loss: 5.932689189910889\n",
      "Epoch [103/2000], Loss: 5.80477237701416\n",
      "Epoch [104/2000], Loss: 5.599720478057861\n",
      "Epoch [105/2000], Loss: 5.543345928192139\n",
      "Epoch [106/2000], Loss: 5.921116828918457\n",
      "Epoch [107/2000], Loss: 6.124253273010254\n",
      "Epoch [108/2000], Loss: 5.829759120941162\n",
      "Epoch [109/2000], Loss: 5.755350589752197\n",
      "Epoch [110/2000], Loss: 5.833109378814697\n",
      "Epoch [111/2000], Loss: 6.045089244842529\n",
      "Epoch [112/2000], Loss: 6.046982765197754\n",
      "Epoch [113/2000], Loss: 6.192378044128418\n",
      "Epoch [114/2000], Loss: 6.211664199829102\n",
      "Epoch [115/2000], Loss: 5.884769916534424\n",
      "Epoch [116/2000], Loss: 5.7257399559021\n",
      "Epoch [117/2000], Loss: 5.682096004486084\n",
      "Epoch [118/2000], Loss: 5.902271747589111\n",
      "Epoch [119/2000], Loss: 5.57516622543335\n",
      "Epoch [120/2000], Loss: 6.008042812347412\n",
      "Epoch [121/2000], Loss: 6.194132328033447\n",
      "Epoch [122/2000], Loss: 6.047879219055176\n",
      "Epoch [123/2000], Loss: 5.662082195281982\n",
      "Epoch [124/2000], Loss: 6.089907646179199\n",
      "Epoch [125/2000], Loss: 5.774378776550293\n",
      "Epoch [126/2000], Loss: 5.711003303527832\n",
      "Epoch [127/2000], Loss: 6.018154144287109\n",
      "Epoch [128/2000], Loss: 5.944003105163574\n",
      "Epoch [129/2000], Loss: 5.829916954040527\n",
      "Epoch [130/2000], Loss: 6.151208877563477\n",
      "Epoch [131/2000], Loss: 5.7978835105896\n",
      "Epoch [132/2000], Loss: 5.7293381690979\n",
      "Epoch [133/2000], Loss: 5.669198513031006\n",
      "Epoch [134/2000], Loss: 5.667788505554199\n",
      "Epoch [135/2000], Loss: 5.87091588973999\n",
      "Epoch [136/2000], Loss: 6.150506496429443\n",
      "Epoch [137/2000], Loss: 6.22499942779541\n",
      "Epoch [138/2000], Loss: 5.854588508605957\n",
      "Epoch [139/2000], Loss: 5.780657768249512\n",
      "Epoch [140/2000], Loss: 5.765002250671387\n",
      "Epoch [141/2000], Loss: 6.001794338226318\n",
      "Epoch [142/2000], Loss: 5.890589237213135\n",
      "Epoch [143/2000], Loss: 5.73566198348999\n",
      "Epoch [144/2000], Loss: 6.019045352935791\n",
      "Epoch [145/2000], Loss: 5.985823631286621\n",
      "Epoch [146/2000], Loss: 5.7396721839904785\n",
      "Epoch [147/2000], Loss: 5.846465110778809\n",
      "Epoch [148/2000], Loss: 5.738551616668701\n",
      "Epoch [149/2000], Loss: 5.786901473999023\n",
      "Epoch [150/2000], Loss: 5.808141231536865\n",
      "Epoch [151/2000], Loss: 5.444024562835693\n",
      "Epoch [152/2000], Loss: 5.879177093505859\n",
      "Epoch [153/2000], Loss: 5.78867769241333\n",
      "Epoch [154/2000], Loss: 5.817169189453125\n",
      "Epoch [155/2000], Loss: 5.914243221282959\n",
      "Epoch [156/2000], Loss: 6.093234062194824\n",
      "Epoch [157/2000], Loss: 6.12440299987793\n",
      "Epoch [158/2000], Loss: 5.983901023864746\n",
      "Epoch [159/2000], Loss: 5.646765232086182\n",
      "Epoch [160/2000], Loss: 5.7029500007629395\n",
      "Epoch [161/2000], Loss: 6.073246479034424\n",
      "Epoch [162/2000], Loss: 5.642903804779053\n",
      "Epoch [163/2000], Loss: 5.700349807739258\n",
      "Epoch [164/2000], Loss: 6.012609481811523\n",
      "Epoch [165/2000], Loss: 6.231240749359131\n",
      "Epoch [166/2000], Loss: 6.167629241943359\n",
      "Epoch [167/2000], Loss: 6.105090141296387\n",
      "Epoch [168/2000], Loss: 6.132896900177002\n",
      "Epoch [169/2000], Loss: 5.9311909675598145\n",
      "Epoch [170/2000], Loss: 5.896759510040283\n",
      "Epoch [171/2000], Loss: 5.661360263824463\n",
      "Epoch [172/2000], Loss: 5.943800449371338\n",
      "Epoch [173/2000], Loss: 5.4812092781066895\n",
      "Epoch [174/2000], Loss: 5.758481025695801\n",
      "Epoch [175/2000], Loss: 5.923274040222168\n",
      "Epoch [176/2000], Loss: 6.060445785522461\n",
      "Epoch [177/2000], Loss: 5.837728023529053\n",
      "Epoch [178/2000], Loss: 5.374629020690918\n",
      "Epoch [179/2000], Loss: 6.028347969055176\n",
      "Epoch [180/2000], Loss: 5.891613006591797\n",
      "Epoch [181/2000], Loss: 5.855833053588867\n",
      "Epoch [182/2000], Loss: 5.8154616355896\n",
      "Epoch [183/2000], Loss: 5.663129806518555\n",
      "Epoch [184/2000], Loss: 5.926296234130859\n",
      "Epoch [185/2000], Loss: 5.9320220947265625\n",
      "Epoch [186/2000], Loss: 5.969614028930664\n",
      "Epoch [187/2000], Loss: 5.860985279083252\n",
      "Epoch [188/2000], Loss: 6.017279148101807\n",
      "Epoch [189/2000], Loss: 5.939516544342041\n",
      "Epoch [190/2000], Loss: 5.842965126037598\n",
      "Epoch [191/2000], Loss: 5.895064830780029\n",
      "Epoch [192/2000], Loss: 5.67014741897583\n",
      "Epoch [193/2000], Loss: 6.16766357421875\n",
      "Epoch [194/2000], Loss: 5.584588527679443\n",
      "Epoch [195/2000], Loss: 5.636758804321289\n",
      "Epoch [196/2000], Loss: 6.0330424308776855\n",
      "Epoch [197/2000], Loss: 6.10945463180542\n",
      "Epoch [198/2000], Loss: 5.89852237701416\n",
      "Epoch [199/2000], Loss: 5.821003437042236\n",
      "Epoch [200/2000], Loss: 5.719024658203125\n",
      "Epoch [201/2000], Loss: 5.696435928344727\n",
      "Epoch [202/2000], Loss: 5.951329708099365\n",
      "Epoch [203/2000], Loss: 6.004568099975586\n",
      "Epoch [204/2000], Loss: 5.85537052154541\n",
      "Epoch [205/2000], Loss: 5.969961166381836\n",
      "Epoch [206/2000], Loss: 5.813626289367676\n",
      "Epoch [207/2000], Loss: 5.843937873840332\n",
      "Epoch [208/2000], Loss: 5.630189895629883\n",
      "Epoch [209/2000], Loss: 5.697080612182617\n",
      "Epoch [210/2000], Loss: 5.470088005065918\n",
      "Epoch [211/2000], Loss: 6.009008407592773\n",
      "Epoch [212/2000], Loss: 6.349388599395752\n",
      "Epoch [213/2000], Loss: 5.594471454620361\n",
      "Epoch [214/2000], Loss: 5.736637115478516\n",
      "Epoch [215/2000], Loss: 5.820436477661133\n",
      "Epoch [216/2000], Loss: 5.842878341674805\n",
      "Epoch [217/2000], Loss: 5.987155914306641\n",
      "Epoch [218/2000], Loss: 5.520277976989746\n",
      "Epoch [219/2000], Loss: 5.918931484222412\n",
      "Epoch [220/2000], Loss: 5.779746055603027\n",
      "Epoch [221/2000], Loss: 5.943569660186768\n",
      "Epoch [222/2000], Loss: 5.709079265594482\n",
      "Epoch [223/2000], Loss: 5.895522594451904\n",
      "Epoch [224/2000], Loss: 5.777299404144287\n",
      "Epoch [225/2000], Loss: 5.525243282318115\n",
      "Epoch [226/2000], Loss: 5.934765815734863\n",
      "Epoch [227/2000], Loss: 6.062537670135498\n",
      "Epoch [228/2000], Loss: 5.989841938018799\n",
      "Epoch [229/2000], Loss: 6.027624607086182\n",
      "Epoch [230/2000], Loss: 5.9795660972595215\n",
      "Epoch [231/2000], Loss: 5.715127468109131\n",
      "Epoch [232/2000], Loss: 5.911386489868164\n",
      "Epoch [233/2000], Loss: 5.858003616333008\n",
      "Epoch [234/2000], Loss: 5.6535539627075195\n",
      "Epoch [235/2000], Loss: 5.7566819190979\n",
      "Epoch [236/2000], Loss: 6.241955757141113\n",
      "Epoch [237/2000], Loss: 5.937152862548828\n",
      "Epoch [238/2000], Loss: 5.984171390533447\n",
      "Epoch [239/2000], Loss: 5.558883190155029\n",
      "Epoch [240/2000], Loss: 5.814702033996582\n",
      "Epoch [241/2000], Loss: 5.553500175476074\n",
      "Epoch [242/2000], Loss: 5.7851691246032715\n",
      "Epoch [243/2000], Loss: 6.019396781921387\n",
      "Epoch [244/2000], Loss: 6.040226936340332\n",
      "Epoch [245/2000], Loss: 5.909666538238525\n",
      "Epoch [246/2000], Loss: 5.814291477203369\n",
      "Epoch [247/2000], Loss: 5.8507232666015625\n",
      "Epoch [248/2000], Loss: 5.654497146606445\n",
      "Epoch [249/2000], Loss: 5.960338115692139\n",
      "Epoch [250/2000], Loss: 5.700223445892334\n",
      "Epoch [251/2000], Loss: 5.579746723175049\n",
      "Epoch [252/2000], Loss: 5.957681179046631\n",
      "Epoch [253/2000], Loss: 6.045629024505615\n",
      "Epoch [254/2000], Loss: 5.981324672698975\n",
      "Epoch [255/2000], Loss: 6.093385219573975\n",
      "Epoch [256/2000], Loss: 6.093446731567383\n",
      "Epoch [257/2000], Loss: 5.60045862197876\n",
      "Epoch [258/2000], Loss: 5.617434978485107\n",
      "Epoch [259/2000], Loss: 5.8910040855407715\n",
      "Epoch [260/2000], Loss: 5.9343953132629395\n",
      "Epoch [261/2000], Loss: 5.968980312347412\n",
      "Epoch [262/2000], Loss: 5.932104587554932\n",
      "Epoch [263/2000], Loss: 5.584303379058838\n",
      "Epoch [264/2000], Loss: 5.962488651275635\n",
      "Epoch [265/2000], Loss: 5.61637020111084\n",
      "Epoch [266/2000], Loss: 5.899886608123779\n",
      "Epoch [267/2000], Loss: 5.778067111968994\n",
      "Epoch [268/2000], Loss: 5.948550224304199\n",
      "Epoch [269/2000], Loss: 5.68047571182251\n",
      "Epoch [270/2000], Loss: 5.5596442222595215\n",
      "Epoch [271/2000], Loss: 5.768637657165527\n",
      "Epoch [272/2000], Loss: 5.8658905029296875\n",
      "Epoch [273/2000], Loss: 5.653491497039795\n",
      "Epoch [274/2000], Loss: 5.9361724853515625\n",
      "Epoch [275/2000], Loss: 5.7461676597595215\n",
      "Epoch [276/2000], Loss: 5.717314720153809\n",
      "Epoch [277/2000], Loss: 6.076601028442383\n",
      "Epoch [278/2000], Loss: 5.888862133026123\n",
      "Epoch [279/2000], Loss: 5.672435283660889\n",
      "Epoch [280/2000], Loss: 5.762175559997559\n",
      "Epoch [281/2000], Loss: 5.525171279907227\n",
      "Epoch [282/2000], Loss: 5.66650915145874\n",
      "Epoch [283/2000], Loss: 6.051799774169922\n",
      "Epoch [284/2000], Loss: 5.786016941070557\n",
      "Epoch [285/2000], Loss: 6.1277384757995605\n",
      "Epoch [286/2000], Loss: 5.880843639373779\n",
      "Epoch [287/2000], Loss: 5.645529270172119\n",
      "Epoch [288/2000], Loss: 5.872166156768799\n",
      "Epoch [289/2000], Loss: 6.01940393447876\n",
      "Epoch [290/2000], Loss: 5.840909004211426\n",
      "Epoch [291/2000], Loss: 6.041213512420654\n",
      "Epoch [292/2000], Loss: 5.6647725105285645\n",
      "Epoch [293/2000], Loss: 5.938839435577393\n",
      "Epoch [294/2000], Loss: 5.736019611358643\n",
      "Epoch [295/2000], Loss: 5.7091193199157715\n",
      "Epoch [296/2000], Loss: 5.7560648918151855\n",
      "Epoch [297/2000], Loss: 5.764795303344727\n",
      "Epoch [298/2000], Loss: 5.816592693328857\n",
      "Epoch [299/2000], Loss: 5.935649871826172\n",
      "Epoch [300/2000], Loss: 5.747335433959961\n",
      "Epoch [301/2000], Loss: 5.7889790534973145\n",
      "Epoch [302/2000], Loss: 5.935087203979492\n",
      "Epoch [303/2000], Loss: 5.691191673278809\n",
      "Epoch [304/2000], Loss: 5.45143985748291\n",
      "Epoch [305/2000], Loss: 5.898228168487549\n",
      "Epoch [306/2000], Loss: 5.956554889678955\n",
      "Epoch [307/2000], Loss: 5.95238733291626\n",
      "Epoch [308/2000], Loss: 5.831576347351074\n",
      "Epoch [309/2000], Loss: 5.9597249031066895\n",
      "Epoch [310/2000], Loss: 5.760467052459717\n",
      "Epoch [311/2000], Loss: 5.771142959594727\n",
      "Epoch [312/2000], Loss: 6.019866943359375\n",
      "Epoch [313/2000], Loss: 5.776572227478027\n",
      "Epoch [314/2000], Loss: 5.3202691078186035\n",
      "Epoch [315/2000], Loss: 6.092655181884766\n",
      "Epoch [316/2000], Loss: 6.003335952758789\n",
      "Epoch [317/2000], Loss: 6.087896347045898\n",
      "Epoch [318/2000], Loss: 5.549835681915283\n",
      "Epoch [319/2000], Loss: 5.742432117462158\n",
      "Epoch [320/2000], Loss: 5.893854141235352\n",
      "Epoch [321/2000], Loss: 5.914788246154785\n",
      "Epoch [322/2000], Loss: 5.944159507751465\n",
      "Epoch [323/2000], Loss: 5.575247287750244\n",
      "Epoch [324/2000], Loss: 5.830723762512207\n",
      "Epoch [325/2000], Loss: 5.931707859039307\n",
      "Epoch [326/2000], Loss: 5.825470924377441\n",
      "Epoch [327/2000], Loss: 6.152135848999023\n",
      "Epoch [328/2000], Loss: 5.528990745544434\n",
      "Epoch [329/2000], Loss: 5.763163089752197\n",
      "Epoch [330/2000], Loss: 5.755244255065918\n",
      "Epoch [331/2000], Loss: 6.055028915405273\n",
      "Epoch [332/2000], Loss: 5.837250709533691\n",
      "Epoch [333/2000], Loss: 5.834323406219482\n",
      "Epoch [334/2000], Loss: 5.925802707672119\n",
      "Epoch [335/2000], Loss: 6.116930961608887\n",
      "Epoch [336/2000], Loss: 6.171295166015625\n",
      "Epoch [337/2000], Loss: 5.7507734298706055\n",
      "Epoch [338/2000], Loss: 5.683915615081787\n",
      "Epoch [339/2000], Loss: 5.618582725524902\n",
      "Epoch [340/2000], Loss: 6.183450222015381\n",
      "Epoch [341/2000], Loss: 5.934315204620361\n",
      "Epoch [342/2000], Loss: 5.711902141571045\n",
      "Epoch [343/2000], Loss: 5.840824604034424\n",
      "Epoch [344/2000], Loss: 6.009027004241943\n",
      "Epoch [345/2000], Loss: 5.8303446769714355\n",
      "Epoch [346/2000], Loss: 6.153911590576172\n",
      "Epoch [347/2000], Loss: 5.821356296539307\n",
      "Epoch [348/2000], Loss: 5.798173904418945\n",
      "Epoch [349/2000], Loss: 5.977053642272949\n",
      "Epoch [350/2000], Loss: 5.974799156188965\n",
      "Epoch [351/2000], Loss: 5.649990081787109\n",
      "Epoch [352/2000], Loss: 5.711404323577881\n",
      "Epoch [353/2000], Loss: 5.761184215545654\n",
      "Epoch [354/2000], Loss: 6.114717960357666\n",
      "Epoch [355/2000], Loss: 5.582951545715332\n",
      "Epoch [356/2000], Loss: 5.896322727203369\n",
      "Epoch [357/2000], Loss: 5.561829566955566\n",
      "Epoch [358/2000], Loss: 5.774008274078369\n",
      "Epoch [359/2000], Loss: 5.891341209411621\n",
      "Epoch [360/2000], Loss: 6.140835762023926\n",
      "Epoch [361/2000], Loss: 6.1107563972473145\n",
      "Epoch [362/2000], Loss: 5.525161266326904\n",
      "Epoch [363/2000], Loss: 5.880908489227295\n",
      "Epoch [364/2000], Loss: 5.782742500305176\n",
      "Epoch [365/2000], Loss: 5.470820903778076\n",
      "Epoch [366/2000], Loss: 5.744289875030518\n",
      "Epoch [367/2000], Loss: 5.8727898597717285\n",
      "Epoch [368/2000], Loss: 5.697973251342773\n",
      "Epoch [369/2000], Loss: 6.135497093200684\n",
      "Epoch [370/2000], Loss: 5.655431270599365\n",
      "Epoch [371/2000], Loss: 5.706787109375\n",
      "Epoch [372/2000], Loss: 5.87925386428833\n",
      "Epoch [373/2000], Loss: 5.387058258056641\n",
      "Epoch [374/2000], Loss: 5.66054105758667\n",
      "Epoch [375/2000], Loss: 5.7850141525268555\n",
      "Epoch [376/2000], Loss: 5.485624313354492\n",
      "Epoch [377/2000], Loss: 5.938942909240723\n",
      "Epoch [378/2000], Loss: 5.733036994934082\n",
      "Epoch [379/2000], Loss: 5.648695468902588\n",
      "Epoch [380/2000], Loss: 5.794161796569824\n",
      "Epoch [381/2000], Loss: 5.458033084869385\n",
      "Epoch [382/2000], Loss: 5.571652889251709\n",
      "Epoch [383/2000], Loss: 6.081332206726074\n",
      "Epoch [384/2000], Loss: 5.79196834564209\n",
      "Epoch [385/2000], Loss: 5.711156368255615\n",
      "Epoch [386/2000], Loss: 5.817377090454102\n",
      "Epoch [387/2000], Loss: 5.870691776275635\n",
      "Epoch [388/2000], Loss: 6.275381565093994\n",
      "Epoch [389/2000], Loss: 5.569741249084473\n",
      "Epoch [390/2000], Loss: 6.159115314483643\n",
      "Epoch [391/2000], Loss: 5.718906402587891\n",
      "Epoch [392/2000], Loss: 5.594706058502197\n",
      "Epoch [393/2000], Loss: 5.959779262542725\n",
      "Epoch [394/2000], Loss: 5.621763706207275\n",
      "Epoch [395/2000], Loss: 5.548323631286621\n",
      "Epoch [396/2000], Loss: 5.949683666229248\n",
      "Epoch [397/2000], Loss: 6.001309871673584\n",
      "Epoch [398/2000], Loss: 5.447820663452148\n",
      "Epoch [399/2000], Loss: 5.776735305786133\n",
      "Epoch [400/2000], Loss: 5.790600299835205\n",
      "Epoch [401/2000], Loss: 5.557275295257568\n",
      "Epoch [402/2000], Loss: 5.809411525726318\n",
      "Epoch [403/2000], Loss: 5.658426761627197\n",
      "Epoch [404/2000], Loss: 5.947213172912598\n",
      "Epoch [405/2000], Loss: 6.027098178863525\n",
      "Epoch [406/2000], Loss: 5.83149528503418\n",
      "Epoch [407/2000], Loss: 5.935823440551758\n",
      "Epoch [408/2000], Loss: 5.717154502868652\n",
      "Epoch [409/2000], Loss: 5.491845607757568\n",
      "Epoch [410/2000], Loss: 5.912703037261963\n",
      "Epoch [411/2000], Loss: 5.679742336273193\n",
      "Epoch [412/2000], Loss: 5.772561073303223\n",
      "Epoch [413/2000], Loss: 5.856883525848389\n",
      "Epoch [414/2000], Loss: 5.786186695098877\n",
      "Epoch [415/2000], Loss: 5.7047648429870605\n",
      "Epoch [416/2000], Loss: 5.743273735046387\n",
      "Epoch [417/2000], Loss: 5.669510841369629\n",
      "Epoch [418/2000], Loss: 5.662475109100342\n",
      "Epoch [419/2000], Loss: 5.857930660247803\n",
      "Epoch [420/2000], Loss: 5.446433067321777\n",
      "Epoch [421/2000], Loss: 5.512408256530762\n",
      "Epoch [422/2000], Loss: 6.32987117767334\n",
      "Epoch [423/2000], Loss: 5.984048366546631\n",
      "Epoch [424/2000], Loss: 5.826051235198975\n",
      "Epoch [425/2000], Loss: 5.574851036071777\n",
      "Epoch [426/2000], Loss: 5.907444000244141\n",
      "Epoch [427/2000], Loss: 5.7966485023498535\n",
      "Epoch [428/2000], Loss: 5.637397289276123\n",
      "Epoch [429/2000], Loss: 5.840297222137451\n",
      "Epoch [430/2000], Loss: 5.781838893890381\n",
      "Epoch [431/2000], Loss: 6.009244918823242\n",
      "Epoch [432/2000], Loss: 5.720983982086182\n",
      "Epoch [433/2000], Loss: 5.868356227874756\n",
      "Epoch [434/2000], Loss: 5.542394638061523\n",
      "Epoch [435/2000], Loss: 5.667483806610107\n",
      "Epoch [436/2000], Loss: 5.403200626373291\n",
      "Epoch [437/2000], Loss: 5.7758941650390625\n",
      "Epoch [438/2000], Loss: 5.92090368270874\n",
      "Epoch [439/2000], Loss: 6.014285564422607\n",
      "Epoch [440/2000], Loss: 5.911550521850586\n",
      "Epoch [441/2000], Loss: 5.766815185546875\n",
      "Epoch [442/2000], Loss: 5.699458122253418\n",
      "Epoch [443/2000], Loss: 5.908932685852051\n",
      "Epoch [444/2000], Loss: 5.84670352935791\n",
      "Epoch [445/2000], Loss: 5.528636455535889\n",
      "Epoch [446/2000], Loss: 5.899394989013672\n",
      "Epoch [447/2000], Loss: 5.583955764770508\n",
      "Epoch [448/2000], Loss: 5.700933456420898\n",
      "Epoch [449/2000], Loss: 5.989569664001465\n",
      "Epoch [450/2000], Loss: 5.928730487823486\n",
      "Epoch [451/2000], Loss: 5.781184673309326\n",
      "Epoch [452/2000], Loss: 5.842535018920898\n",
      "Epoch [453/2000], Loss: 5.805591583251953\n",
      "Epoch [454/2000], Loss: 5.81596040725708\n",
      "Epoch [455/2000], Loss: 6.104162693023682\n",
      "Epoch [456/2000], Loss: 5.761867046356201\n",
      "Epoch [457/2000], Loss: 5.760812282562256\n",
      "Epoch [458/2000], Loss: 5.786879539489746\n",
      "Epoch [459/2000], Loss: 5.611942291259766\n",
      "Epoch [460/2000], Loss: 5.796511650085449\n",
      "Epoch [461/2000], Loss: 6.061074733734131\n",
      "Epoch [462/2000], Loss: 5.751270294189453\n",
      "Epoch [463/2000], Loss: 5.717917442321777\n",
      "Epoch [464/2000], Loss: 5.735860347747803\n",
      "Epoch [465/2000], Loss: 5.690425872802734\n",
      "Epoch [466/2000], Loss: 5.642025470733643\n",
      "Epoch [467/2000], Loss: 5.664793014526367\n",
      "Epoch [468/2000], Loss: 6.400607109069824\n",
      "Epoch [469/2000], Loss: 5.542760848999023\n",
      "Epoch [470/2000], Loss: 5.917943000793457\n",
      "Epoch [471/2000], Loss: 5.648616790771484\n",
      "Epoch [472/2000], Loss: 5.4893412590026855\n",
      "Epoch [473/2000], Loss: 5.462871551513672\n",
      "Epoch [474/2000], Loss: 5.594249725341797\n",
      "Epoch [475/2000], Loss: 5.920905113220215\n",
      "Epoch [476/2000], Loss: 5.819368839263916\n",
      "Epoch [477/2000], Loss: 5.633551597595215\n",
      "Epoch [478/2000], Loss: 5.722121715545654\n",
      "Epoch [479/2000], Loss: 5.7096686363220215\n",
      "Epoch [480/2000], Loss: 5.857400894165039\n",
      "Epoch [481/2000], Loss: 5.780706405639648\n",
      "Epoch [482/2000], Loss: 5.624614715576172\n",
      "Epoch [483/2000], Loss: 5.723179817199707\n",
      "Epoch [484/2000], Loss: 6.064797401428223\n",
      "Epoch [485/2000], Loss: 5.502893447875977\n",
      "Epoch [486/2000], Loss: 6.0112810134887695\n",
      "Epoch [487/2000], Loss: 5.738786220550537\n",
      "Epoch [488/2000], Loss: 6.005580425262451\n",
      "Epoch [489/2000], Loss: 5.695155143737793\n",
      "Epoch [490/2000], Loss: 6.037251949310303\n",
      "Epoch [491/2000], Loss: 5.9653544425964355\n",
      "Epoch [492/2000], Loss: 5.742517471313477\n",
      "Epoch [493/2000], Loss: 5.944294452667236\n",
      "Epoch [494/2000], Loss: 5.811530590057373\n",
      "Epoch [495/2000], Loss: 5.408738136291504\n",
      "Epoch [496/2000], Loss: 5.544454574584961\n",
      "Epoch [497/2000], Loss: 6.004078388214111\n",
      "Epoch [498/2000], Loss: 5.653377532958984\n",
      "Epoch [499/2000], Loss: 5.569295406341553\n",
      "Epoch [500/2000], Loss: 5.6504950523376465\n",
      "Epoch [501/2000], Loss: 5.644864559173584\n",
      "Epoch [502/2000], Loss: 5.824752330780029\n",
      "Epoch [503/2000], Loss: 5.584659576416016\n",
      "Epoch [504/2000], Loss: 5.898737907409668\n",
      "Epoch [505/2000], Loss: 5.739912033081055\n",
      "Epoch [506/2000], Loss: 5.597168445587158\n",
      "Epoch [507/2000], Loss: 5.608366966247559\n",
      "Epoch [508/2000], Loss: 5.443365097045898\n",
      "Epoch [509/2000], Loss: 5.9095377922058105\n",
      "Epoch [510/2000], Loss: 5.916346549987793\n",
      "Epoch [511/2000], Loss: 5.711787700653076\n",
      "Epoch [512/2000], Loss: 5.878409385681152\n",
      "Epoch [513/2000], Loss: 5.581236839294434\n",
      "Epoch [514/2000], Loss: 5.634831428527832\n",
      "Epoch [515/2000], Loss: 5.866366863250732\n",
      "Epoch [516/2000], Loss: 5.842452049255371\n",
      "Epoch [517/2000], Loss: 5.6802287101745605\n",
      "Epoch [518/2000], Loss: 5.586439609527588\n",
      "Epoch [519/2000], Loss: 5.980850696563721\n",
      "Epoch [520/2000], Loss: 5.776090145111084\n",
      "Epoch [521/2000], Loss: 5.989240646362305\n",
      "Epoch [522/2000], Loss: 6.0054240226745605\n",
      "Epoch [523/2000], Loss: 5.710368633270264\n",
      "Epoch [524/2000], Loss: 5.9944682121276855\n",
      "Epoch [525/2000], Loss: 5.402426242828369\n",
      "Epoch [526/2000], Loss: 5.9115986824035645\n",
      "Epoch [527/2000], Loss: 5.7227463722229\n",
      "Epoch [528/2000], Loss: 5.584320545196533\n",
      "Epoch [529/2000], Loss: 5.758764266967773\n",
      "Epoch [530/2000], Loss: 5.732064247131348\n",
      "Epoch [531/2000], Loss: 5.881991386413574\n",
      "Epoch [532/2000], Loss: 5.536874294281006\n",
      "Epoch [533/2000], Loss: 5.53394889831543\n",
      "Epoch [534/2000], Loss: 5.600558757781982\n",
      "Epoch [535/2000], Loss: 5.680354118347168\n",
      "Epoch [536/2000], Loss: 5.779213905334473\n",
      "Epoch [537/2000], Loss: 5.498536109924316\n",
      "Epoch [538/2000], Loss: 5.87129020690918\n",
      "Epoch [539/2000], Loss: 5.9239630699157715\n",
      "Epoch [540/2000], Loss: 5.8109517097473145\n",
      "Epoch [541/2000], Loss: 5.8348283767700195\n",
      "Epoch [542/2000], Loss: 5.545723915100098\n",
      "Epoch [543/2000], Loss: 5.67789888381958\n",
      "Epoch [544/2000], Loss: 5.621020317077637\n",
      "Epoch [545/2000], Loss: 5.763164043426514\n",
      "Epoch [546/2000], Loss: 5.979471683502197\n",
      "Epoch [547/2000], Loss: 5.805386543273926\n",
      "Epoch [548/2000], Loss: 5.771246433258057\n",
      "Epoch [549/2000], Loss: 5.888376712799072\n",
      "Epoch [550/2000], Loss: 5.71262264251709\n",
      "Epoch [551/2000], Loss: 6.0891523361206055\n",
      "Epoch [552/2000], Loss: 5.938150405883789\n",
      "Epoch [553/2000], Loss: 5.467408657073975\n",
      "Epoch [554/2000], Loss: 5.755784511566162\n",
      "Epoch [555/2000], Loss: 5.646815776824951\n",
      "Epoch [556/2000], Loss: 5.603880405426025\n",
      "Epoch [557/2000], Loss: 6.204470157623291\n",
      "Epoch [558/2000], Loss: 5.885395050048828\n",
      "Epoch [559/2000], Loss: 5.5735764503479\n",
      "Epoch [560/2000], Loss: 5.8289666175842285\n",
      "Epoch [561/2000], Loss: 5.641554355621338\n",
      "Epoch [562/2000], Loss: 5.734775066375732\n",
      "Epoch [563/2000], Loss: 5.955600738525391\n",
      "Epoch [564/2000], Loss: 5.633756637573242\n",
      "Epoch [565/2000], Loss: 5.596557140350342\n",
      "Epoch [566/2000], Loss: 5.6375555992126465\n",
      "Epoch [567/2000], Loss: 5.6296706199646\n",
      "Epoch [568/2000], Loss: 6.004796504974365\n",
      "Epoch [569/2000], Loss: 5.357940673828125\n",
      "Epoch [570/2000], Loss: 5.688982009887695\n",
      "Epoch [571/2000], Loss: 6.04282808303833\n",
      "Epoch [572/2000], Loss: 5.7183051109313965\n",
      "Epoch [573/2000], Loss: 5.710474491119385\n",
      "Epoch [574/2000], Loss: 5.677935600280762\n",
      "Epoch [575/2000], Loss: 5.737158298492432\n",
      "Epoch [576/2000], Loss: 5.882493495941162\n",
      "Epoch [577/2000], Loss: 6.047100067138672\n",
      "Epoch [578/2000], Loss: 5.5108418464660645\n",
      "Epoch [579/2000], Loss: 5.749609470367432\n",
      "Epoch [580/2000], Loss: 5.801394462585449\n",
      "Epoch [581/2000], Loss: 5.936156749725342\n",
      "Epoch [582/2000], Loss: 5.496181011199951\n",
      "Epoch [583/2000], Loss: 5.786770343780518\n",
      "Epoch [584/2000], Loss: 5.620265483856201\n",
      "Epoch [585/2000], Loss: 5.907567977905273\n",
      "Epoch [586/2000], Loss: 5.726257801055908\n",
      "Epoch [587/2000], Loss: 5.407626152038574\n",
      "Epoch [588/2000], Loss: 5.572607517242432\n",
      "Epoch [589/2000], Loss: 5.642101764678955\n",
      "Epoch [590/2000], Loss: 5.650981903076172\n",
      "Epoch [591/2000], Loss: 5.791108131408691\n",
      "Epoch [592/2000], Loss: 5.309768199920654\n",
      "Epoch [593/2000], Loss: 5.828318119049072\n",
      "Epoch [594/2000], Loss: 5.622869491577148\n",
      "Epoch [595/2000], Loss: 5.988949775695801\n",
      "Epoch [596/2000], Loss: 5.710019588470459\n",
      "Epoch [597/2000], Loss: 5.703141689300537\n",
      "Epoch [598/2000], Loss: 5.944882392883301\n",
      "Epoch [599/2000], Loss: 5.4151482582092285\n",
      "Epoch [600/2000], Loss: 5.64575719833374\n",
      "Epoch [601/2000], Loss: 5.648221015930176\n",
      "Epoch [602/2000], Loss: 5.551466941833496\n",
      "Epoch [603/2000], Loss: 5.8502278327941895\n",
      "Epoch [604/2000], Loss: 5.684330940246582\n",
      "Epoch [605/2000], Loss: 5.597104072570801\n",
      "Epoch [606/2000], Loss: 6.061311721801758\n",
      "Epoch [607/2000], Loss: 5.717403888702393\n",
      "Epoch [608/2000], Loss: 5.513846397399902\n",
      "Epoch [609/2000], Loss: 5.688473701477051\n",
      "Epoch [610/2000], Loss: 6.130423069000244\n",
      "Epoch [611/2000], Loss: 5.740169048309326\n",
      "Epoch [612/2000], Loss: 5.619967937469482\n",
      "Epoch [613/2000], Loss: 5.641923904418945\n",
      "Epoch [614/2000], Loss: 6.023555755615234\n",
      "Epoch [615/2000], Loss: 6.124281883239746\n",
      "Epoch [616/2000], Loss: 5.581854820251465\n",
      "Epoch [617/2000], Loss: 5.272887706756592\n",
      "Epoch [618/2000], Loss: 5.658931732177734\n",
      "Epoch [619/2000], Loss: 5.6696457862854\n",
      "Epoch [620/2000], Loss: 5.717301368713379\n",
      "Epoch [621/2000], Loss: 5.407317638397217\n",
      "Epoch [622/2000], Loss: 5.984982013702393\n",
      "Epoch [623/2000], Loss: 5.380131244659424\n",
      "Epoch [624/2000], Loss: 5.638129711151123\n",
      "Epoch [625/2000], Loss: 5.597568511962891\n",
      "Epoch [626/2000], Loss: 5.680258750915527\n",
      "Epoch [627/2000], Loss: 5.775606632232666\n",
      "Epoch [628/2000], Loss: 5.458499908447266\n",
      "Epoch [629/2000], Loss: 5.9020209312438965\n",
      "Epoch [630/2000], Loss: 5.648015975952148\n",
      "Epoch [631/2000], Loss: 5.160632610321045\n",
      "Epoch [632/2000], Loss: 5.4773359298706055\n",
      "Epoch [633/2000], Loss: 5.892274379730225\n",
      "Epoch [634/2000], Loss: 5.4275641441345215\n",
      "Epoch [635/2000], Loss: 5.400732040405273\n",
      "Epoch [636/2000], Loss: 5.107046127319336\n",
      "Epoch [637/2000], Loss: 5.619153022766113\n",
      "Epoch [638/2000], Loss: 5.944044589996338\n",
      "Epoch [639/2000], Loss: 5.5937371253967285\n",
      "Epoch [640/2000], Loss: 5.562342643737793\n",
      "Epoch [641/2000], Loss: 5.8344244956970215\n",
      "Epoch [642/2000], Loss: 5.595122337341309\n",
      "Epoch [643/2000], Loss: 5.610941410064697\n",
      "Epoch [644/2000], Loss: 5.5377278327941895\n",
      "Epoch [645/2000], Loss: 5.640157222747803\n",
      "Epoch [646/2000], Loss: 5.760757923126221\n",
      "Epoch [647/2000], Loss: 5.57817268371582\n",
      "Epoch [648/2000], Loss: 6.138641834259033\n",
      "Epoch [649/2000], Loss: 5.519197463989258\n",
      "Epoch [650/2000], Loss: 5.737899303436279\n",
      "Epoch [651/2000], Loss: 5.909002780914307\n",
      "Epoch [652/2000], Loss: 5.634889602661133\n",
      "Epoch [653/2000], Loss: 5.457508087158203\n",
      "Epoch [654/2000], Loss: 5.806103706359863\n",
      "Epoch [655/2000], Loss: 5.777029991149902\n",
      "Epoch [656/2000], Loss: 5.900263786315918\n",
      "Epoch [657/2000], Loss: 5.439477920532227\n",
      "Epoch [658/2000], Loss: 5.635015964508057\n",
      "Epoch [659/2000], Loss: 5.7639336585998535\n",
      "Epoch [660/2000], Loss: 5.8311238288879395\n",
      "Epoch [661/2000], Loss: 6.156064033508301\n",
      "Epoch [662/2000], Loss: 5.6793212890625\n",
      "Epoch [663/2000], Loss: 5.491316795349121\n",
      "Epoch [664/2000], Loss: 5.546639442443848\n",
      "Epoch [665/2000], Loss: 5.736955642700195\n",
      "Epoch [666/2000], Loss: 5.540050506591797\n",
      "Epoch [667/2000], Loss: 5.798253536224365\n",
      "Epoch [668/2000], Loss: 5.6067609786987305\n",
      "Epoch [669/2000], Loss: 5.688160419464111\n",
      "Epoch [670/2000], Loss: 6.050800800323486\n",
      "Epoch [671/2000], Loss: 5.700830936431885\n",
      "Epoch [672/2000], Loss: 5.500744819641113\n",
      "Epoch [673/2000], Loss: 5.627326488494873\n",
      "Epoch [674/2000], Loss: 5.62355899810791\n",
      "Epoch [675/2000], Loss: 5.435222625732422\n",
      "Epoch [676/2000], Loss: 5.8281941413879395\n",
      "Epoch [677/2000], Loss: 5.946776390075684\n",
      "Epoch [678/2000], Loss: 5.797300338745117\n",
      "Epoch [679/2000], Loss: 5.649951457977295\n",
      "Epoch [680/2000], Loss: 5.886961936950684\n",
      "Epoch [681/2000], Loss: 5.760491847991943\n",
      "Epoch [682/2000], Loss: 6.003844738006592\n",
      "Epoch [683/2000], Loss: 5.424411296844482\n",
      "Epoch [684/2000], Loss: 5.757866859436035\n",
      "Epoch [685/2000], Loss: 5.776207447052002\n",
      "Epoch [686/2000], Loss: 5.320005893707275\n",
      "Epoch [687/2000], Loss: 5.593347549438477\n",
      "Epoch [688/2000], Loss: 5.654248237609863\n",
      "Epoch [689/2000], Loss: 6.443592548370361\n",
      "Epoch [690/2000], Loss: 5.6064863204956055\n",
      "Epoch [691/2000], Loss: 5.816224098205566\n",
      "Epoch [692/2000], Loss: 5.607707500457764\n",
      "Epoch [693/2000], Loss: 5.886338233947754\n",
      "Epoch [694/2000], Loss: 6.071627616882324\n",
      "Epoch [695/2000], Loss: 5.831201076507568\n",
      "Epoch [696/2000], Loss: 5.544783592224121\n",
      "Epoch [697/2000], Loss: 5.801478862762451\n",
      "Epoch [698/2000], Loss: 5.5690412521362305\n",
      "Epoch [699/2000], Loss: 5.427012920379639\n",
      "Epoch [700/2000], Loss: 5.452782154083252\n",
      "Epoch [701/2000], Loss: 5.969016075134277\n",
      "Epoch [702/2000], Loss: 5.672306537628174\n",
      "Epoch [703/2000], Loss: 5.7400689125061035\n",
      "Epoch [704/2000], Loss: 5.654026985168457\n",
      "Epoch [705/2000], Loss: 5.513076305389404\n",
      "Epoch [706/2000], Loss: 5.859206199645996\n",
      "Epoch [707/2000], Loss: 5.69638204574585\n",
      "Epoch [708/2000], Loss: 5.469839572906494\n",
      "Epoch [709/2000], Loss: 6.039041042327881\n",
      "Epoch [710/2000], Loss: 5.807869911193848\n",
      "Epoch [711/2000], Loss: 5.781141757965088\n",
      "Epoch [712/2000], Loss: 5.639681339263916\n",
      "Epoch [713/2000], Loss: 5.9538679122924805\n",
      "Epoch [714/2000], Loss: 5.898016929626465\n",
      "Epoch [715/2000], Loss: 5.509214401245117\n",
      "Epoch [716/2000], Loss: 5.45889139175415\n",
      "Epoch [717/2000], Loss: 5.763274192810059\n",
      "Epoch [718/2000], Loss: 5.8993730545043945\n",
      "Epoch [719/2000], Loss: 5.668944358825684\n",
      "Epoch [720/2000], Loss: 5.704648017883301\n",
      "Epoch [721/2000], Loss: 5.867734432220459\n",
      "Epoch [722/2000], Loss: 5.922968864440918\n",
      "Epoch [723/2000], Loss: 5.791907787322998\n",
      "Epoch [724/2000], Loss: 5.672729969024658\n",
      "Epoch [725/2000], Loss: 5.9784345626831055\n",
      "Epoch [726/2000], Loss: 5.546689033508301\n",
      "Epoch [727/2000], Loss: 5.647691249847412\n",
      "Epoch [728/2000], Loss: 5.370073318481445\n",
      "Epoch [729/2000], Loss: 5.643498420715332\n",
      "Epoch [730/2000], Loss: 5.649456024169922\n",
      "Epoch [731/2000], Loss: 5.41929817199707\n",
      "Epoch [732/2000], Loss: 5.593505382537842\n",
      "Epoch [733/2000], Loss: 5.713342666625977\n",
      "Epoch [734/2000], Loss: 6.010026454925537\n",
      "Epoch [735/2000], Loss: 5.54622745513916\n",
      "Epoch [736/2000], Loss: 5.793895244598389\n",
      "Epoch [737/2000], Loss: 5.35875129699707\n",
      "Epoch [738/2000], Loss: 5.865965843200684\n",
      "Epoch [739/2000], Loss: 5.7209577560424805\n",
      "Epoch [740/2000], Loss: 5.92251443862915\n",
      "Epoch [741/2000], Loss: 5.705273628234863\n",
      "Epoch [742/2000], Loss: 5.528761863708496\n",
      "Epoch [743/2000], Loss: 5.594330787658691\n",
      "Epoch [744/2000], Loss: 5.564366817474365\n",
      "Epoch [745/2000], Loss: 5.729772090911865\n",
      "Epoch [746/2000], Loss: 5.6478776931762695\n",
      "Epoch [747/2000], Loss: 5.929730415344238\n",
      "Epoch [748/2000], Loss: 5.743375301361084\n",
      "Epoch [749/2000], Loss: 5.629850387573242\n",
      "Epoch [750/2000], Loss: 5.798569679260254\n",
      "Epoch [751/2000], Loss: 5.931963920593262\n",
      "Epoch [752/2000], Loss: 5.793909549713135\n",
      "Epoch [753/2000], Loss: 5.677984714508057\n",
      "Epoch [754/2000], Loss: 5.919448375701904\n",
      "Epoch [755/2000], Loss: 5.500957489013672\n",
      "Epoch [756/2000], Loss: 5.486286640167236\n",
      "Epoch [757/2000], Loss: 5.887352466583252\n",
      "Epoch [758/2000], Loss: 6.03484582901001\n",
      "Epoch [759/2000], Loss: 5.911871910095215\n",
      "Epoch [760/2000], Loss: 5.55907678604126\n",
      "Epoch [761/2000], Loss: 5.3713507652282715\n",
      "Epoch [762/2000], Loss: 5.85968017578125\n",
      "Epoch [763/2000], Loss: 5.817502498626709\n",
      "Epoch [764/2000], Loss: 5.947835445404053\n",
      "Epoch [765/2000], Loss: 5.650956153869629\n",
      "Epoch [766/2000], Loss: 5.926111221313477\n",
      "Epoch [767/2000], Loss: 5.735390663146973\n",
      "Epoch [768/2000], Loss: 5.726046085357666\n",
      "Epoch [769/2000], Loss: 5.661380290985107\n",
      "Epoch [770/2000], Loss: 5.48702335357666\n",
      "Epoch [771/2000], Loss: 5.591497898101807\n",
      "Epoch [772/2000], Loss: 5.872735023498535\n",
      "Epoch [773/2000], Loss: 5.5678300857543945\n",
      "Epoch [774/2000], Loss: 5.684604167938232\n",
      "Epoch [775/2000], Loss: 5.518768310546875\n",
      "Epoch [776/2000], Loss: 5.51371955871582\n",
      "Epoch [777/2000], Loss: 5.778451919555664\n",
      "Epoch [778/2000], Loss: 5.940403461456299\n",
      "Epoch [779/2000], Loss: 6.140214443206787\n",
      "Epoch [780/2000], Loss: 5.659884929656982\n",
      "Epoch [781/2000], Loss: 5.848326683044434\n",
      "Epoch [782/2000], Loss: 5.643739700317383\n",
      "Epoch [783/2000], Loss: 5.6371588706970215\n",
      "Epoch [784/2000], Loss: 5.630680561065674\n",
      "Epoch [785/2000], Loss: 5.510323524475098\n",
      "Epoch [786/2000], Loss: 5.575518608093262\n",
      "Epoch [787/2000], Loss: 5.6796650886535645\n",
      "Epoch [788/2000], Loss: 5.764719009399414\n",
      "Epoch [789/2000], Loss: 5.8108086585998535\n",
      "Epoch [790/2000], Loss: 5.568964004516602\n",
      "Epoch [791/2000], Loss: 6.2298150062561035\n",
      "Epoch [792/2000], Loss: 5.7554168701171875\n",
      "Epoch [793/2000], Loss: 5.829733371734619\n",
      "Epoch [794/2000], Loss: 6.175085067749023\n",
      "Epoch [795/2000], Loss: 5.910971641540527\n",
      "Epoch [796/2000], Loss: 5.717993259429932\n",
      "Epoch [797/2000], Loss: 5.67484712600708\n",
      "Epoch [798/2000], Loss: 5.7245049476623535\n",
      "Epoch [799/2000], Loss: 5.401090145111084\n",
      "Epoch [800/2000], Loss: 6.026715278625488\n",
      "Epoch [801/2000], Loss: 5.876410007476807\n",
      "Epoch [802/2000], Loss: 5.6811370849609375\n",
      "Epoch [803/2000], Loss: 5.896204948425293\n",
      "Epoch [804/2000], Loss: 6.036043643951416\n",
      "Epoch [805/2000], Loss: 5.669344425201416\n",
      "Epoch [806/2000], Loss: 5.773987293243408\n",
      "Epoch [807/2000], Loss: 5.449301719665527\n",
      "Epoch [808/2000], Loss: 5.770295143127441\n",
      "Epoch [809/2000], Loss: 5.746358394622803\n",
      "Epoch [810/2000], Loss: 5.487399578094482\n",
      "Epoch [811/2000], Loss: 5.509451389312744\n",
      "Epoch [812/2000], Loss: 5.620190620422363\n",
      "Epoch [813/2000], Loss: 5.855663776397705\n",
      "Epoch [814/2000], Loss: 5.725554943084717\n",
      "Epoch [815/2000], Loss: 5.543300151824951\n",
      "Epoch [816/2000], Loss: 5.772764205932617\n",
      "Epoch [817/2000], Loss: 6.142905235290527\n",
      "Epoch [818/2000], Loss: 5.407080173492432\n",
      "Epoch [819/2000], Loss: 5.199224472045898\n",
      "Epoch [820/2000], Loss: 5.9481611251831055\n",
      "Epoch [821/2000], Loss: 6.014383792877197\n",
      "Epoch [822/2000], Loss: 5.912553310394287\n",
      "Epoch [823/2000], Loss: 5.499817848205566\n",
      "Epoch [824/2000], Loss: 5.389634609222412\n",
      "Epoch [825/2000], Loss: 5.463867664337158\n",
      "Epoch [826/2000], Loss: 5.523408889770508\n",
      "Epoch [827/2000], Loss: 5.334325313568115\n",
      "Epoch [828/2000], Loss: 5.647512912750244\n",
      "Epoch [829/2000], Loss: 5.590372085571289\n",
      "Epoch [830/2000], Loss: 5.408372402191162\n",
      "Epoch [831/2000], Loss: 5.513956546783447\n",
      "Epoch [832/2000], Loss: 5.646711349487305\n",
      "Epoch [833/2000], Loss: 5.817540168762207\n",
      "Epoch [834/2000], Loss: 5.729687213897705\n",
      "Epoch [835/2000], Loss: 5.834386825561523\n",
      "Epoch [836/2000], Loss: 5.668404579162598\n",
      "Epoch [837/2000], Loss: 5.574084758758545\n",
      "Epoch [838/2000], Loss: 5.687661647796631\n",
      "Epoch [839/2000], Loss: 5.618342399597168\n",
      "Epoch [840/2000], Loss: 5.686733722686768\n",
      "Epoch [841/2000], Loss: 5.360856056213379\n",
      "Epoch [842/2000], Loss: 5.5667805671691895\n",
      "Epoch [843/2000], Loss: 6.03567361831665\n",
      "Epoch [844/2000], Loss: 5.69260835647583\n",
      "Epoch [845/2000], Loss: 5.5561113357543945\n",
      "Epoch [846/2000], Loss: 5.91365909576416\n",
      "Epoch [847/2000], Loss: 5.497693061828613\n",
      "Epoch [848/2000], Loss: 5.792273044586182\n",
      "Epoch [849/2000], Loss: 5.659853935241699\n",
      "Epoch [850/2000], Loss: 5.929866790771484\n",
      "Epoch [851/2000], Loss: 6.175385475158691\n",
      "Epoch [852/2000], Loss: 5.684127330780029\n",
      "Epoch [853/2000], Loss: 5.702049732208252\n",
      "Epoch [854/2000], Loss: 5.575206279754639\n",
      "Epoch [855/2000], Loss: 5.2704901695251465\n",
      "Epoch [856/2000], Loss: 5.593766689300537\n",
      "Epoch [857/2000], Loss: 5.989114761352539\n",
      "Epoch [858/2000], Loss: 5.453924179077148\n",
      "Epoch [859/2000], Loss: 5.20880126953125\n",
      "Epoch [860/2000], Loss: 6.166980743408203\n",
      "Epoch [861/2000], Loss: 5.65751314163208\n",
      "Epoch [862/2000], Loss: 5.84632682800293\n",
      "Epoch [863/2000], Loss: 5.401917457580566\n",
      "Epoch [864/2000], Loss: 5.708088397979736\n",
      "Epoch [865/2000], Loss: 5.637641429901123\n",
      "Epoch [866/2000], Loss: 6.0361504554748535\n",
      "Epoch [867/2000], Loss: 5.5725016593933105\n",
      "Epoch [868/2000], Loss: 5.844486236572266\n",
      "Epoch [869/2000], Loss: 5.591858863830566\n",
      "Epoch [870/2000], Loss: 5.899949550628662\n",
      "Epoch [871/2000], Loss: 5.7355475425720215\n",
      "Epoch [872/2000], Loss: 5.6477532386779785\n",
      "Epoch [873/2000], Loss: 5.640540599822998\n",
      "Epoch [874/2000], Loss: 5.728414535522461\n",
      "Epoch [875/2000], Loss: 5.6250901222229\n",
      "Epoch [876/2000], Loss: 5.536057949066162\n",
      "Epoch [877/2000], Loss: 5.599825382232666\n",
      "Epoch [878/2000], Loss: 5.6873860359191895\n",
      "Epoch [879/2000], Loss: 5.334112644195557\n",
      "Epoch [880/2000], Loss: 6.021836757659912\n",
      "Epoch [881/2000], Loss: 5.850803375244141\n",
      "Epoch [882/2000], Loss: 5.188211441040039\n",
      "Epoch [883/2000], Loss: 5.99367094039917\n",
      "Epoch [884/2000], Loss: 5.801486968994141\n",
      "Epoch [885/2000], Loss: 6.106051921844482\n",
      "Epoch [886/2000], Loss: 5.49891471862793\n",
      "Epoch [887/2000], Loss: 5.806578636169434\n",
      "Epoch [888/2000], Loss: 6.074726581573486\n",
      "Epoch [889/2000], Loss: 5.929141521453857\n",
      "Epoch [890/2000], Loss: 5.783818244934082\n",
      "Epoch [891/2000], Loss: 5.731006622314453\n",
      "Epoch [892/2000], Loss: 5.605042934417725\n",
      "Epoch [893/2000], Loss: 5.669888019561768\n",
      "Epoch [894/2000], Loss: 5.953349590301514\n",
      "Epoch [895/2000], Loss: 5.816489219665527\n",
      "Epoch [896/2000], Loss: 5.853878498077393\n",
      "Epoch [897/2000], Loss: 5.318173408508301\n",
      "Epoch [898/2000], Loss: 5.866230010986328\n",
      "Epoch [899/2000], Loss: 5.379696369171143\n",
      "Epoch [900/2000], Loss: 5.641482830047607\n",
      "Epoch [901/2000], Loss: 5.78036642074585\n",
      "Epoch [902/2000], Loss: 5.585551738739014\n",
      "Epoch [903/2000], Loss: 5.559887886047363\n",
      "Epoch [904/2000], Loss: 5.99395227432251\n",
      "Epoch [905/2000], Loss: 5.793834209442139\n",
      "Epoch [906/2000], Loss: 5.41095495223999\n",
      "Epoch [907/2000], Loss: 5.942175388336182\n",
      "Epoch [908/2000], Loss: 5.65395450592041\n",
      "Epoch [909/2000], Loss: 5.78620719909668\n",
      "Epoch [910/2000], Loss: 5.573700904846191\n",
      "Epoch [911/2000], Loss: 5.321371078491211\n",
      "Epoch [912/2000], Loss: 5.521655082702637\n",
      "Epoch [913/2000], Loss: 6.0180840492248535\n",
      "Epoch [914/2000], Loss: 5.562364101409912\n",
      "Epoch [915/2000], Loss: 5.696579456329346\n",
      "Epoch [916/2000], Loss: 5.592622756958008\n",
      "Epoch [917/2000], Loss: 5.593878746032715\n",
      "Epoch [918/2000], Loss: 5.76760721206665\n",
      "Epoch [919/2000], Loss: 5.3111467361450195\n",
      "Epoch [920/2000], Loss: 5.9279327392578125\n",
      "Epoch [921/2000], Loss: 5.602138996124268\n",
      "Epoch [922/2000], Loss: 5.989981651306152\n",
      "Epoch [923/2000], Loss: 5.8301568031311035\n",
      "Epoch [924/2000], Loss: 5.972834587097168\n",
      "Epoch [925/2000], Loss: 5.377986431121826\n",
      "Epoch [926/2000], Loss: 5.585324287414551\n",
      "Epoch [927/2000], Loss: 5.917061805725098\n",
      "Epoch [928/2000], Loss: 5.881722450256348\n",
      "Epoch [929/2000], Loss: 5.923937797546387\n",
      "Epoch [930/2000], Loss: 5.524373531341553\n",
      "Epoch [931/2000], Loss: 5.311063766479492\n",
      "Epoch [932/2000], Loss: 5.756101608276367\n",
      "Epoch [933/2000], Loss: 5.597620010375977\n",
      "Epoch [934/2000], Loss: 5.763103008270264\n",
      "Epoch [935/2000], Loss: 5.972282886505127\n",
      "Epoch [936/2000], Loss: 5.908290863037109\n",
      "Epoch [937/2000], Loss: 5.537447929382324\n",
      "Epoch [938/2000], Loss: 5.776976585388184\n",
      "Epoch [939/2000], Loss: 5.936607837677002\n",
      "Epoch [940/2000], Loss: 5.483375072479248\n",
      "Epoch [941/2000], Loss: 5.797717571258545\n",
      "Epoch [942/2000], Loss: 5.747194766998291\n",
      "Epoch [943/2000], Loss: 5.376605033874512\n",
      "Epoch [944/2000], Loss: 5.656327724456787\n",
      "Epoch [945/2000], Loss: 5.880110740661621\n",
      "Epoch [946/2000], Loss: 5.971903324127197\n",
      "Epoch [947/2000], Loss: 5.611963272094727\n",
      "Epoch [948/2000], Loss: 5.302760601043701\n",
      "Epoch [949/2000], Loss: 5.581219673156738\n",
      "Epoch [950/2000], Loss: 5.72163200378418\n",
      "Epoch [951/2000], Loss: 5.4740071296691895\n",
      "Epoch [952/2000], Loss: 5.7838335037231445\n",
      "Epoch [953/2000], Loss: 6.007254123687744\n",
      "Epoch [954/2000], Loss: 5.524564266204834\n",
      "Epoch [955/2000], Loss: 5.756750583648682\n",
      "Epoch [956/2000], Loss: 5.786172389984131\n",
      "Epoch [957/2000], Loss: 5.546289443969727\n",
      "Epoch [958/2000], Loss: 5.411757946014404\n",
      "Epoch [959/2000], Loss: 5.786225318908691\n",
      "Epoch [960/2000], Loss: 5.279801845550537\n",
      "Epoch [961/2000], Loss: 5.915506839752197\n",
      "Epoch [962/2000], Loss: 5.50324010848999\n",
      "Epoch [963/2000], Loss: 5.9079132080078125\n",
      "Epoch [964/2000], Loss: 5.846384525299072\n",
      "Epoch [965/2000], Loss: 5.800760269165039\n",
      "Epoch [966/2000], Loss: 5.664921283721924\n",
      "Epoch [967/2000], Loss: 5.566396713256836\n",
      "Epoch [968/2000], Loss: 5.648618698120117\n",
      "Epoch [969/2000], Loss: 5.970597743988037\n",
      "Epoch [970/2000], Loss: 5.478403568267822\n",
      "Epoch [971/2000], Loss: 5.553310394287109\n",
      "Epoch [972/2000], Loss: 5.646883487701416\n",
      "Epoch [973/2000], Loss: 5.642590045928955\n",
      "Epoch [974/2000], Loss: 5.7326436042785645\n",
      "Epoch [975/2000], Loss: 5.994609832763672\n",
      "Epoch [976/2000], Loss: 5.588680267333984\n",
      "Epoch [977/2000], Loss: 5.446157932281494\n",
      "Epoch [978/2000], Loss: 5.386917591094971\n",
      "Epoch [979/2000], Loss: 5.596113681793213\n",
      "Epoch [980/2000], Loss: 5.3478875160217285\n",
      "Epoch [981/2000], Loss: 5.783096790313721\n",
      "Epoch [982/2000], Loss: 5.24540376663208\n",
      "Epoch [983/2000], Loss: 5.440329551696777\n",
      "Epoch [984/2000], Loss: 5.763620853424072\n",
      "Epoch [985/2000], Loss: 5.5882344245910645\n",
      "Epoch [986/2000], Loss: 5.98584508895874\n",
      "Epoch [987/2000], Loss: 5.588132381439209\n",
      "Epoch [988/2000], Loss: 5.777798175811768\n",
      "Epoch [989/2000], Loss: 5.584050178527832\n",
      "Epoch [990/2000], Loss: 5.4966888427734375\n",
      "Epoch [991/2000], Loss: 5.726083755493164\n",
      "Epoch [992/2000], Loss: 5.63835334777832\n",
      "Epoch [993/2000], Loss: 5.586338043212891\n",
      "Epoch [994/2000], Loss: 5.76429557800293\n",
      "Epoch [995/2000], Loss: 5.730207920074463\n",
      "Epoch [996/2000], Loss: 5.717233180999756\n",
      "Epoch [997/2000], Loss: 5.9361371994018555\n",
      "Epoch [998/2000], Loss: 5.596620082855225\n",
      "Epoch [999/2000], Loss: 5.992193698883057\n",
      "Epoch [1000/2000], Loss: 5.7297868728637695\n",
      "Epoch [1001/2000], Loss: 5.542341709136963\n",
      "Epoch [1002/2000], Loss: 5.385557651519775\n",
      "Epoch [1003/2000], Loss: 5.523305416107178\n",
      "Epoch [1004/2000], Loss: 5.7134504318237305\n",
      "Epoch [1005/2000], Loss: 5.769850730895996\n",
      "Epoch [1006/2000], Loss: 5.453080177307129\n",
      "Epoch [1007/2000], Loss: 5.621712684631348\n",
      "Epoch [1008/2000], Loss: 5.966598987579346\n",
      "Epoch [1009/2000], Loss: 5.539048194885254\n",
      "Epoch [1010/2000], Loss: 5.505947113037109\n",
      "Epoch [1011/2000], Loss: 5.625115394592285\n",
      "Epoch [1012/2000], Loss: 5.616490840911865\n",
      "Epoch [1013/2000], Loss: 5.806623935699463\n",
      "Epoch [1014/2000], Loss: 5.565907001495361\n",
      "Epoch [1015/2000], Loss: 5.601924419403076\n",
      "Epoch [1016/2000], Loss: 5.717463970184326\n",
      "Epoch [1017/2000], Loss: 5.82357931137085\n",
      "Epoch [1018/2000], Loss: 5.55739164352417\n",
      "Epoch [1019/2000], Loss: 5.529353141784668\n",
      "Epoch [1020/2000], Loss: 5.820128917694092\n",
      "Epoch [1021/2000], Loss: 5.737246036529541\n",
      "Epoch [1022/2000], Loss: 5.421543121337891\n",
      "Epoch [1023/2000], Loss: 5.6884942054748535\n",
      "Epoch [1024/2000], Loss: 5.764241695404053\n",
      "Epoch [1025/2000], Loss: 5.304556369781494\n",
      "Epoch [1026/2000], Loss: 5.943392276763916\n",
      "Epoch [1027/2000], Loss: 5.51575231552124\n",
      "Epoch [1028/2000], Loss: 5.695804119110107\n",
      "Epoch [1029/2000], Loss: 5.293372631072998\n",
      "Epoch [1030/2000], Loss: 5.550473690032959\n",
      "Epoch [1031/2000], Loss: 5.348371982574463\n",
      "Epoch [1032/2000], Loss: 5.711656093597412\n",
      "Epoch [1033/2000], Loss: 5.409600734710693\n",
      "Epoch [1034/2000], Loss: 5.556319236755371\n",
      "Epoch [1035/2000], Loss: 5.748825550079346\n",
      "Epoch [1036/2000], Loss: 5.7141642570495605\n",
      "Epoch [1037/2000], Loss: 5.737483978271484\n",
      "Epoch [1038/2000], Loss: 5.6757707595825195\n",
      "Epoch [1039/2000], Loss: 5.585946559906006\n",
      "Epoch [1040/2000], Loss: 5.637133598327637\n",
      "Epoch [1041/2000], Loss: 5.56213903427124\n",
      "Epoch [1042/2000], Loss: 5.652055263519287\n",
      "Epoch [1043/2000], Loss: 5.678915023803711\n",
      "Epoch [1044/2000], Loss: 5.603960037231445\n",
      "Epoch [1045/2000], Loss: 6.119562149047852\n",
      "Epoch [1046/2000], Loss: 5.482776165008545\n",
      "Epoch [1047/2000], Loss: 5.613007545471191\n",
      "Epoch [1048/2000], Loss: 5.574160099029541\n",
      "Epoch [1049/2000], Loss: 5.704452991485596\n",
      "Epoch [1050/2000], Loss: 5.918460845947266\n",
      "Epoch [1051/2000], Loss: 5.542415618896484\n",
      "Epoch [1052/2000], Loss: 5.5779900550842285\n",
      "Epoch [1053/2000], Loss: 5.69067907333374\n",
      "Epoch [1054/2000], Loss: 5.665102958679199\n",
      "Epoch [1055/2000], Loss: 5.5975751876831055\n",
      "Epoch [1056/2000], Loss: 5.84813928604126\n",
      "Epoch [1057/2000], Loss: 5.565897464752197\n",
      "Epoch [1058/2000], Loss: 5.795563220977783\n",
      "Epoch [1059/2000], Loss: 5.8437652587890625\n",
      "Epoch [1060/2000], Loss: 5.479645252227783\n",
      "Epoch [1061/2000], Loss: 5.5701093673706055\n",
      "Epoch [1062/2000], Loss: 5.615424633026123\n",
      "Epoch [1063/2000], Loss: 5.733688831329346\n",
      "Epoch [1064/2000], Loss: 5.790156364440918\n",
      "Epoch [1065/2000], Loss: 5.5197038650512695\n",
      "Epoch [1066/2000], Loss: 5.778752326965332\n",
      "Epoch [1067/2000], Loss: 6.0863189697265625\n",
      "Epoch [1068/2000], Loss: 5.187595367431641\n",
      "Epoch [1069/2000], Loss: 5.986887454986572\n",
      "Epoch [1070/2000], Loss: 5.236134052276611\n",
      "Epoch [1071/2000], Loss: 5.334620475769043\n",
      "Epoch [1072/2000], Loss: 5.642102241516113\n",
      "Epoch [1073/2000], Loss: 5.60524845123291\n",
      "Epoch [1074/2000], Loss: 5.531757354736328\n",
      "Epoch [1075/2000], Loss: 5.720610618591309\n",
      "Epoch [1076/2000], Loss: 6.177062034606934\n",
      "Epoch [1077/2000], Loss: 5.326580047607422\n",
      "Epoch [1078/2000], Loss: 5.681185245513916\n",
      "Epoch [1079/2000], Loss: 5.805347919464111\n",
      "Epoch [1080/2000], Loss: 5.455306053161621\n",
      "Epoch [1081/2000], Loss: 5.423502445220947\n",
      "Epoch [1082/2000], Loss: 5.335958003997803\n",
      "Epoch [1083/2000], Loss: 5.501012802124023\n",
      "Epoch [1084/2000], Loss: 5.59609317779541\n",
      "Epoch [1085/2000], Loss: 5.51560115814209\n",
      "Epoch [1086/2000], Loss: 5.962250232696533\n",
      "Epoch [1087/2000], Loss: 5.645304203033447\n",
      "Epoch [1088/2000], Loss: 5.38847017288208\n",
      "Epoch [1089/2000], Loss: 5.503954887390137\n",
      "Epoch [1090/2000], Loss: 5.66318941116333\n",
      "Epoch [1091/2000], Loss: 5.602544784545898\n",
      "Epoch [1092/2000], Loss: 5.751328468322754\n",
      "Epoch [1093/2000], Loss: 5.618851184844971\n",
      "Epoch [1094/2000], Loss: 5.4649505615234375\n",
      "Epoch [1095/2000], Loss: 5.3817315101623535\n",
      "Epoch [1096/2000], Loss: 5.355998516082764\n",
      "Epoch [1097/2000], Loss: 5.226522445678711\n",
      "Epoch [1098/2000], Loss: 5.357983112335205\n",
      "Epoch [1099/2000], Loss: 5.798983097076416\n",
      "Epoch [1100/2000], Loss: 5.611034870147705\n",
      "Epoch [1101/2000], Loss: 5.978188991546631\n",
      "Epoch [1102/2000], Loss: 5.497097492218018\n",
      "Epoch [1103/2000], Loss: 5.522204875946045\n",
      "Epoch [1104/2000], Loss: 5.778085231781006\n",
      "Epoch [1105/2000], Loss: 5.499161243438721\n",
      "Epoch [1106/2000], Loss: 5.438739776611328\n",
      "Epoch [1107/2000], Loss: 5.46779203414917\n",
      "Epoch [1108/2000], Loss: 5.337441444396973\n",
      "Epoch [1109/2000], Loss: 5.506971836090088\n",
      "Epoch [1110/2000], Loss: 5.625411510467529\n",
      "Epoch [1111/2000], Loss: 5.33103084564209\n",
      "Epoch [1112/2000], Loss: 5.56980037689209\n",
      "Epoch [1113/2000], Loss: 5.781893253326416\n",
      "Epoch [1114/2000], Loss: 5.472012042999268\n",
      "Epoch [1115/2000], Loss: 5.883406162261963\n",
      "Epoch [1116/2000], Loss: 5.774746417999268\n",
      "Epoch [1117/2000], Loss: 5.696706295013428\n",
      "Epoch [1118/2000], Loss: 5.311596393585205\n",
      "Epoch [1119/2000], Loss: 5.390537261962891\n",
      "Epoch [1120/2000], Loss: 5.771502494812012\n",
      "Epoch [1121/2000], Loss: 5.447066783905029\n",
      "Epoch [1122/2000], Loss: 5.848086357116699\n",
      "Epoch [1123/2000], Loss: 5.317241668701172\n",
      "Epoch [1124/2000], Loss: 5.743755340576172\n",
      "Epoch [1125/2000], Loss: 5.342379570007324\n",
      "Epoch [1126/2000], Loss: 5.587100982666016\n",
      "Epoch [1127/2000], Loss: 5.445658206939697\n",
      "Epoch [1128/2000], Loss: 5.428897380828857\n",
      "Epoch [1129/2000], Loss: 5.264963626861572\n",
      "Epoch [1130/2000], Loss: 5.606165885925293\n",
      "Epoch [1131/2000], Loss: 5.489697456359863\n",
      "Epoch [1132/2000], Loss: 5.873685836791992\n",
      "Epoch [1133/2000], Loss: 5.917309284210205\n",
      "Epoch [1134/2000], Loss: 5.249074935913086\n",
      "Epoch [1135/2000], Loss: 5.912414073944092\n",
      "Epoch [1136/2000], Loss: 5.477579593658447\n",
      "Epoch [1137/2000], Loss: 5.738641262054443\n",
      "Epoch [1138/2000], Loss: 5.6814799308776855\n",
      "Epoch [1139/2000], Loss: 6.008408546447754\n",
      "Epoch [1140/2000], Loss: 5.4227118492126465\n",
      "Epoch [1141/2000], Loss: 5.6675639152526855\n",
      "Epoch [1142/2000], Loss: 5.6206841468811035\n",
      "Epoch [1143/2000], Loss: 5.7296223640441895\n",
      "Epoch [1144/2000], Loss: 5.556790828704834\n",
      "Epoch [1145/2000], Loss: 5.456357479095459\n",
      "Epoch [1146/2000], Loss: 5.410767078399658\n",
      "Epoch [1147/2000], Loss: 5.596499919891357\n",
      "Epoch [1148/2000], Loss: 5.452284812927246\n",
      "Epoch [1149/2000], Loss: 5.59828519821167\n",
      "Epoch [1150/2000], Loss: 5.674595355987549\n",
      "Epoch [1151/2000], Loss: 5.374725818634033\n",
      "Epoch [1152/2000], Loss: 5.2224555015563965\n",
      "Epoch [1153/2000], Loss: 5.389856338500977\n",
      "Epoch [1154/2000], Loss: 5.9676923751831055\n",
      "Epoch [1155/2000], Loss: 5.559489727020264\n",
      "Epoch [1156/2000], Loss: 5.217685222625732\n",
      "Epoch [1157/2000], Loss: 5.507038593292236\n",
      "Epoch [1158/2000], Loss: 5.515523433685303\n",
      "Epoch [1159/2000], Loss: 5.532215118408203\n",
      "Epoch [1160/2000], Loss: 5.847439765930176\n",
      "Epoch [1161/2000], Loss: 5.810817241668701\n",
      "Epoch [1162/2000], Loss: 5.514218807220459\n",
      "Epoch [1163/2000], Loss: 5.664252758026123\n",
      "Epoch [1164/2000], Loss: 5.7711873054504395\n",
      "Epoch [1165/2000], Loss: 5.396933078765869\n",
      "Epoch [1166/2000], Loss: 5.505202293395996\n",
      "Epoch [1167/2000], Loss: 5.30954122543335\n",
      "Epoch [1168/2000], Loss: 5.991633892059326\n",
      "Epoch [1169/2000], Loss: 5.905576229095459\n",
      "Epoch [1170/2000], Loss: 5.464161396026611\n",
      "Epoch [1171/2000], Loss: 5.340563774108887\n",
      "Epoch [1172/2000], Loss: 5.787281513214111\n",
      "Epoch [1173/2000], Loss: 5.686844348907471\n",
      "Epoch [1174/2000], Loss: 5.596418857574463\n",
      "Epoch [1175/2000], Loss: 5.768991947174072\n",
      "Epoch [1176/2000], Loss: 5.563412189483643\n",
      "Epoch [1177/2000], Loss: 5.436671733856201\n",
      "Epoch [1178/2000], Loss: 5.530820846557617\n",
      "Epoch [1179/2000], Loss: 5.148428440093994\n",
      "Epoch [1180/2000], Loss: 5.56680965423584\n",
      "Epoch [1181/2000], Loss: 5.683107376098633\n",
      "Epoch [1182/2000], Loss: 5.31125020980835\n",
      "Epoch [1183/2000], Loss: 5.66469669342041\n",
      "Epoch [1184/2000], Loss: 5.4682488441467285\n",
      "Epoch [1185/2000], Loss: 5.588730335235596\n",
      "Epoch [1186/2000], Loss: 5.722874641418457\n",
      "Epoch [1187/2000], Loss: 5.609276294708252\n",
      "Epoch [1188/2000], Loss: 5.446445941925049\n",
      "Epoch [1189/2000], Loss: 5.3906354904174805\n",
      "Epoch [1190/2000], Loss: 5.502216339111328\n",
      "Epoch [1191/2000], Loss: 5.455102920532227\n",
      "Epoch [1192/2000], Loss: 5.848246097564697\n",
      "Epoch [1193/2000], Loss: 5.758957862854004\n",
      "Epoch [1194/2000], Loss: 5.746486186981201\n",
      "Epoch [1195/2000], Loss: 5.372777938842773\n",
      "Epoch [1196/2000], Loss: 5.760615825653076\n",
      "Epoch [1197/2000], Loss: 5.619967937469482\n",
      "Epoch [1198/2000], Loss: 5.824532985687256\n",
      "Epoch [1199/2000], Loss: 5.287516117095947\n",
      "Epoch [1200/2000], Loss: 5.386661529541016\n",
      "Epoch [1201/2000], Loss: 5.686628341674805\n",
      "Epoch [1202/2000], Loss: 5.773130416870117\n",
      "Epoch [1203/2000], Loss: 5.907711505889893\n",
      "Epoch [1204/2000], Loss: 5.481652736663818\n",
      "Epoch [1205/2000], Loss: 5.199158668518066\n",
      "Epoch [1206/2000], Loss: 6.025903224945068\n",
      "Epoch [1207/2000], Loss: 5.703415393829346\n",
      "Epoch [1208/2000], Loss: 5.719494342803955\n",
      "Epoch [1209/2000], Loss: 5.437760829925537\n",
      "Epoch [1210/2000], Loss: 5.542202472686768\n",
      "Epoch [1211/2000], Loss: 5.411649227142334\n",
      "Epoch [1212/2000], Loss: 5.782801628112793\n",
      "Epoch [1213/2000], Loss: 5.427273273468018\n",
      "Epoch [1214/2000], Loss: 5.486145496368408\n",
      "Epoch [1215/2000], Loss: 5.37105655670166\n",
      "Epoch [1216/2000], Loss: 5.8522562980651855\n",
      "Epoch [1217/2000], Loss: 5.280526161193848\n",
      "Epoch [1218/2000], Loss: 5.5788421630859375\n",
      "Epoch [1219/2000], Loss: 5.349900245666504\n",
      "Epoch [1220/2000], Loss: 5.949370384216309\n",
      "Epoch [1221/2000], Loss: 5.762784004211426\n",
      "Epoch [1222/2000], Loss: 5.492962837219238\n",
      "Epoch [1223/2000], Loss: 5.626968860626221\n",
      "Epoch [1224/2000], Loss: 5.472834587097168\n",
      "Epoch [1225/2000], Loss: 5.48607873916626\n",
      "Epoch [1226/2000], Loss: 5.398119926452637\n",
      "Epoch [1227/2000], Loss: 5.733800888061523\n",
      "Epoch [1228/2000], Loss: 5.485007286071777\n",
      "Epoch [1229/2000], Loss: 5.71859884262085\n",
      "Epoch [1230/2000], Loss: 5.545151710510254\n",
      "Epoch [1231/2000], Loss: 5.786435127258301\n",
      "Epoch [1232/2000], Loss: 5.42044734954834\n",
      "Epoch [1233/2000], Loss: 5.538346290588379\n",
      "Epoch [1234/2000], Loss: 5.737655162811279\n",
      "Epoch [1235/2000], Loss: 6.099059581756592\n",
      "Epoch [1236/2000], Loss: 5.837809085845947\n",
      "Epoch [1237/2000], Loss: 5.291158199310303\n",
      "Epoch [1238/2000], Loss: 5.7003326416015625\n",
      "Epoch [1239/2000], Loss: 5.478952407836914\n",
      "Epoch [1240/2000], Loss: 5.8366546630859375\n",
      "Epoch [1241/2000], Loss: 5.576667308807373\n",
      "Epoch [1242/2000], Loss: 5.830921173095703\n",
      "Epoch [1243/2000], Loss: 5.4705023765563965\n",
      "Epoch [1244/2000], Loss: 5.491557598114014\n",
      "Epoch [1245/2000], Loss: 5.812452793121338\n",
      "Epoch [1246/2000], Loss: 5.569723606109619\n",
      "Epoch [1247/2000], Loss: 5.544142246246338\n",
      "Epoch [1248/2000], Loss: 5.346380710601807\n",
      "Epoch [1249/2000], Loss: 5.573904514312744\n",
      "Epoch [1250/2000], Loss: 5.42777681350708\n",
      "Epoch [1251/2000], Loss: 5.877316474914551\n",
      "Epoch [1252/2000], Loss: 5.656916618347168\n",
      "Epoch [1253/2000], Loss: 5.994088649749756\n",
      "Epoch [1254/2000], Loss: 6.147797107696533\n",
      "Epoch [1255/2000], Loss: 5.650968551635742\n",
      "Epoch [1256/2000], Loss: 5.574184894561768\n",
      "Epoch [1257/2000], Loss: 5.604826927185059\n",
      "Epoch [1258/2000], Loss: 5.490062713623047\n",
      "Epoch [1259/2000], Loss: 5.48972749710083\n",
      "Epoch [1260/2000], Loss: 5.340899467468262\n",
      "Epoch [1261/2000], Loss: 5.318512916564941\n",
      "Epoch [1262/2000], Loss: 5.61693811416626\n",
      "Epoch [1263/2000], Loss: 5.7205610275268555\n",
      "Epoch [1264/2000], Loss: 5.117812156677246\n",
      "Epoch [1265/2000], Loss: 5.772024154663086\n",
      "Epoch [1266/2000], Loss: 5.5889458656311035\n",
      "Epoch [1267/2000], Loss: 5.577794551849365\n",
      "Epoch [1268/2000], Loss: 5.428201198577881\n",
      "Epoch [1269/2000], Loss: 5.335690975189209\n",
      "Epoch [1270/2000], Loss: 5.466343402862549\n",
      "Epoch [1271/2000], Loss: 5.4003167152404785\n",
      "Epoch [1272/2000], Loss: 5.90678596496582\n",
      "Epoch [1273/2000], Loss: 5.479237079620361\n",
      "Epoch [1274/2000], Loss: 5.721246719360352\n",
      "Epoch [1275/2000], Loss: 5.833957672119141\n",
      "Epoch [1276/2000], Loss: 5.5888142585754395\n",
      "Epoch [1277/2000], Loss: 5.585445880889893\n",
      "Epoch [1278/2000], Loss: 5.751652240753174\n",
      "Epoch [1279/2000], Loss: 5.829007625579834\n",
      "Epoch [1280/2000], Loss: 5.861972332000732\n",
      "Epoch [1281/2000], Loss: 5.473683834075928\n",
      "Epoch [1282/2000], Loss: 5.766421794891357\n",
      "Epoch [1283/2000], Loss: 5.395046234130859\n",
      "Epoch [1284/2000], Loss: 5.480218887329102\n",
      "Epoch [1285/2000], Loss: 5.565485000610352\n",
      "Epoch [1286/2000], Loss: 5.566580295562744\n",
      "Epoch [1287/2000], Loss: 5.362801551818848\n",
      "Epoch [1288/2000], Loss: 5.719120502471924\n",
      "Epoch [1289/2000], Loss: 5.7063727378845215\n",
      "Epoch [1290/2000], Loss: 5.319533824920654\n",
      "Epoch [1291/2000], Loss: 5.450537204742432\n",
      "Epoch [1292/2000], Loss: 5.834733963012695\n",
      "Epoch [1293/2000], Loss: 5.547346591949463\n",
      "Epoch [1294/2000], Loss: 5.415862083435059\n",
      "Epoch [1295/2000], Loss: 5.833791732788086\n",
      "Epoch [1296/2000], Loss: 5.456613063812256\n",
      "Epoch [1297/2000], Loss: 5.834609031677246\n",
      "Epoch [1298/2000], Loss: 5.653026103973389\n",
      "Epoch [1299/2000], Loss: 5.68528413772583\n",
      "Epoch [1300/2000], Loss: 5.7459492683410645\n",
      "Epoch [1301/2000], Loss: 5.81660795211792\n",
      "Epoch [1302/2000], Loss: 5.432465553283691\n",
      "Epoch [1303/2000], Loss: 5.743503093719482\n",
      "Epoch [1304/2000], Loss: 5.654880046844482\n",
      "Epoch [1305/2000], Loss: 5.83590841293335\n",
      "Epoch [1306/2000], Loss: 5.516480922698975\n",
      "Epoch [1307/2000], Loss: 5.512813091278076\n",
      "Epoch [1308/2000], Loss: 5.65015172958374\n",
      "Epoch [1309/2000], Loss: 5.883848190307617\n",
      "Epoch [1310/2000], Loss: 5.76046895980835\n",
      "Epoch [1311/2000], Loss: 5.881383895874023\n",
      "Epoch [1312/2000], Loss: 5.374466419219971\n",
      "Epoch [1313/2000], Loss: 5.458611488342285\n",
      "Epoch [1314/2000], Loss: 5.5687079429626465\n",
      "Epoch [1315/2000], Loss: 5.589283466339111\n",
      "Epoch [1316/2000], Loss: 5.4047532081604\n",
      "Epoch [1317/2000], Loss: 5.3390326499938965\n",
      "Epoch [1318/2000], Loss: 5.861777305603027\n",
      "Epoch [1319/2000], Loss: 5.718935012817383\n",
      "Epoch [1320/2000], Loss: 5.898624420166016\n",
      "Epoch [1321/2000], Loss: 5.841175079345703\n",
      "Epoch [1322/2000], Loss: 5.956066608428955\n",
      "Epoch [1323/2000], Loss: 5.538756370544434\n",
      "Epoch [1324/2000], Loss: 5.745893478393555\n",
      "Epoch [1325/2000], Loss: 5.7168426513671875\n",
      "Epoch [1326/2000], Loss: 5.445289134979248\n",
      "Epoch [1327/2000], Loss: 5.583917617797852\n",
      "Epoch [1328/2000], Loss: 5.545003414154053\n",
      "Epoch [1329/2000], Loss: 5.548791885375977\n",
      "Epoch [1330/2000], Loss: 5.407900810241699\n",
      "Epoch [1331/2000], Loss: 5.786635875701904\n",
      "Epoch [1332/2000], Loss: 5.368362903594971\n",
      "Epoch [1333/2000], Loss: 5.557518482208252\n",
      "Epoch [1334/2000], Loss: 5.533419609069824\n",
      "Epoch [1335/2000], Loss: 5.636956691741943\n",
      "Epoch [1336/2000], Loss: 5.396006107330322\n",
      "Epoch [1337/2000], Loss: 5.738783836364746\n",
      "Epoch [1338/2000], Loss: 5.864469528198242\n",
      "Epoch [1339/2000], Loss: 5.50734281539917\n",
      "Epoch [1340/2000], Loss: 5.432168960571289\n",
      "Epoch [1341/2000], Loss: 5.49720573425293\n",
      "Epoch [1342/2000], Loss: 5.6200103759765625\n",
      "Epoch [1343/2000], Loss: 5.721372604370117\n",
      "Epoch [1344/2000], Loss: 5.497701168060303\n",
      "Epoch [1345/2000], Loss: 5.827317237854004\n",
      "Epoch [1346/2000], Loss: 5.3393354415893555\n",
      "Epoch [1347/2000], Loss: 6.068308353424072\n",
      "Epoch [1348/2000], Loss: 5.52672004699707\n",
      "Epoch [1349/2000], Loss: 5.609555721282959\n",
      "Epoch [1350/2000], Loss: 5.796337127685547\n",
      "Epoch [1351/2000], Loss: 5.76052713394165\n",
      "Epoch [1352/2000], Loss: 5.732464790344238\n",
      "Epoch [1353/2000], Loss: 5.323030948638916\n",
      "Epoch [1354/2000], Loss: 5.347834587097168\n",
      "Epoch [1355/2000], Loss: 5.620940685272217\n",
      "Epoch [1356/2000], Loss: 5.91729736328125\n",
      "Epoch [1357/2000], Loss: 6.0413994789123535\n",
      "Epoch [1358/2000], Loss: 5.870344638824463\n",
      "Epoch [1359/2000], Loss: 5.511242866516113\n",
      "Epoch [1360/2000], Loss: 5.846599102020264\n",
      "Epoch [1361/2000], Loss: 5.556002140045166\n",
      "Epoch [1362/2000], Loss: 5.323275566101074\n",
      "Epoch [1363/2000], Loss: 5.298166751861572\n",
      "Epoch [1364/2000], Loss: 5.277621746063232\n",
      "Epoch [1365/2000], Loss: 5.751277923583984\n",
      "Epoch [1366/2000], Loss: 5.346070766448975\n",
      "Epoch [1367/2000], Loss: 5.50526762008667\n",
      "Epoch [1368/2000], Loss: 5.561067581176758\n",
      "Epoch [1369/2000], Loss: 5.627717971801758\n",
      "Epoch [1370/2000], Loss: 5.667154788970947\n",
      "Epoch [1371/2000], Loss: 5.381155014038086\n",
      "Epoch [1372/2000], Loss: 5.808597564697266\n",
      "Epoch [1373/2000], Loss: 5.604796886444092\n",
      "Epoch [1374/2000], Loss: 5.9549455642700195\n",
      "Epoch [1375/2000], Loss: 5.771014213562012\n",
      "Epoch [1376/2000], Loss: 5.504915714263916\n",
      "Epoch [1377/2000], Loss: 5.517617702484131\n",
      "Epoch [1378/2000], Loss: 5.457041263580322\n",
      "Epoch [1379/2000], Loss: 5.4278950691223145\n",
      "Epoch [1380/2000], Loss: 5.444098472595215\n",
      "Epoch [1381/2000], Loss: 5.461118698120117\n",
      "Epoch [1382/2000], Loss: 5.844418048858643\n",
      "Epoch [1383/2000], Loss: 5.735666751861572\n",
      "Epoch [1384/2000], Loss: 5.5777058601379395\n",
      "Epoch [1385/2000], Loss: 5.464810848236084\n",
      "Epoch [1386/2000], Loss: 5.572089195251465\n",
      "Epoch [1387/2000], Loss: 5.579418182373047\n",
      "Epoch [1388/2000], Loss: 5.707403182983398\n",
      "Epoch [1389/2000], Loss: 5.585862636566162\n",
      "Epoch [1390/2000], Loss: 5.412365913391113\n",
      "Epoch [1391/2000], Loss: 5.320125102996826\n",
      "Epoch [1392/2000], Loss: 5.372191905975342\n",
      "Epoch [1393/2000], Loss: 5.5889129638671875\n",
      "Epoch [1394/2000], Loss: 5.443939685821533\n",
      "Epoch [1395/2000], Loss: 5.336524486541748\n",
      "Epoch [1396/2000], Loss: 5.565710067749023\n",
      "Epoch [1397/2000], Loss: 5.184983730316162\n",
      "Epoch [1398/2000], Loss: 5.443447113037109\n",
      "Epoch [1399/2000], Loss: 5.660583972930908\n",
      "Epoch [1400/2000], Loss: 5.2747721672058105\n",
      "Epoch [1401/2000], Loss: 5.196261405944824\n",
      "Epoch [1402/2000], Loss: 5.667261600494385\n",
      "Epoch [1403/2000], Loss: 5.79302978515625\n",
      "Epoch [1404/2000], Loss: 5.647189617156982\n",
      "Epoch [1405/2000], Loss: 5.549986362457275\n",
      "Epoch [1406/2000], Loss: 5.223879337310791\n",
      "Epoch [1407/2000], Loss: 5.393925666809082\n",
      "Epoch [1408/2000], Loss: 5.687483310699463\n",
      "Epoch [1409/2000], Loss: 5.727668285369873\n",
      "Epoch [1410/2000], Loss: 5.934878826141357\n",
      "Epoch [1411/2000], Loss: 5.705446720123291\n",
      "Epoch [1412/2000], Loss: 5.454377174377441\n",
      "Epoch [1413/2000], Loss: 5.478211402893066\n",
      "Epoch [1414/2000], Loss: 5.456905841827393\n",
      "Epoch [1415/2000], Loss: 5.641905784606934\n",
      "Epoch [1416/2000], Loss: 5.421557903289795\n",
      "Epoch [1417/2000], Loss: 5.556064128875732\n",
      "Epoch [1418/2000], Loss: 5.722530364990234\n",
      "Epoch [1419/2000], Loss: 5.682836055755615\n",
      "Epoch [1420/2000], Loss: 5.789088726043701\n",
      "Epoch [1421/2000], Loss: 5.702794075012207\n",
      "Epoch [1422/2000], Loss: 6.1330485343933105\n",
      "Epoch [1423/2000], Loss: 5.525761604309082\n",
      "Epoch [1424/2000], Loss: 5.882169723510742\n",
      "Epoch [1425/2000], Loss: 5.264251232147217\n",
      "Epoch [1426/2000], Loss: 5.521737098693848\n",
      "Epoch [1427/2000], Loss: 5.710511207580566\n",
      "Epoch [1428/2000], Loss: 5.690553665161133\n",
      "Epoch [1429/2000], Loss: 5.488150119781494\n",
      "Epoch [1430/2000], Loss: 5.658637523651123\n",
      "Epoch [1431/2000], Loss: 5.511962890625\n",
      "Epoch [1432/2000], Loss: 5.4626970291137695\n",
      "Epoch [1433/2000], Loss: 5.381774425506592\n",
      "Epoch [1434/2000], Loss: 5.250515460968018\n",
      "Epoch [1435/2000], Loss: 5.236591815948486\n",
      "Epoch [1436/2000], Loss: 5.5684099197387695\n",
      "Epoch [1437/2000], Loss: 5.379404544830322\n",
      "Epoch [1438/2000], Loss: 5.499235153198242\n",
      "Epoch [1439/2000], Loss: 5.665395259857178\n",
      "Epoch [1440/2000], Loss: 5.529824733734131\n",
      "Epoch [1441/2000], Loss: 5.704397201538086\n",
      "Epoch [1442/2000], Loss: 5.57765531539917\n",
      "Epoch [1443/2000], Loss: 5.249917507171631\n",
      "Epoch [1444/2000], Loss: 5.477190971374512\n",
      "Epoch [1445/2000], Loss: 5.701107501983643\n",
      "Epoch [1446/2000], Loss: 5.5752129554748535\n",
      "Epoch [1447/2000], Loss: 5.631938457489014\n",
      "Epoch [1448/2000], Loss: 5.757594585418701\n",
      "Epoch [1449/2000], Loss: 5.591508865356445\n",
      "Epoch [1450/2000], Loss: 5.6946539878845215\n",
      "Epoch [1451/2000], Loss: 5.832027435302734\n",
      "Epoch [1452/2000], Loss: 5.895252227783203\n",
      "Epoch [1453/2000], Loss: 5.633303642272949\n",
      "Epoch [1454/2000], Loss: 5.658290863037109\n",
      "Epoch [1455/2000], Loss: 5.744493007659912\n",
      "Epoch [1456/2000], Loss: 5.394834995269775\n",
      "Epoch [1457/2000], Loss: 5.488950729370117\n",
      "Epoch [1458/2000], Loss: 5.691064357757568\n",
      "Epoch [1459/2000], Loss: 5.642090797424316\n",
      "Epoch [1460/2000], Loss: 5.333322525024414\n",
      "Epoch [1461/2000], Loss: 5.417344093322754\n",
      "Epoch [1462/2000], Loss: 5.819485664367676\n",
      "Epoch [1463/2000], Loss: 5.580787181854248\n",
      "Epoch [1464/2000], Loss: 5.477725028991699\n",
      "Epoch [1465/2000], Loss: 5.540613174438477\n",
      "Epoch [1466/2000], Loss: 4.937616348266602\n",
      "Epoch [1467/2000], Loss: 5.408895969390869\n",
      "Epoch [1468/2000], Loss: 5.535519123077393\n",
      "Epoch [1469/2000], Loss: 5.453399658203125\n",
      "Epoch [1470/2000], Loss: 5.267160415649414\n",
      "Epoch [1471/2000], Loss: 5.770623683929443\n",
      "Epoch [1472/2000], Loss: 6.211406707763672\n",
      "Epoch [1473/2000], Loss: 5.330904960632324\n",
      "Epoch [1474/2000], Loss: 5.300232410430908\n",
      "Epoch [1475/2000], Loss: 5.164226531982422\n",
      "Epoch [1476/2000], Loss: 5.731074810028076\n",
      "Epoch [1477/2000], Loss: 5.681520462036133\n",
      "Epoch [1478/2000], Loss: 5.8807759284973145\n",
      "Epoch [1479/2000], Loss: 5.753746509552002\n",
      "Epoch [1480/2000], Loss: 5.41356897354126\n",
      "Epoch [1481/2000], Loss: 5.658034801483154\n",
      "Epoch [1482/2000], Loss: 5.132723331451416\n",
      "Epoch [1483/2000], Loss: 5.645705699920654\n",
      "Epoch [1484/2000], Loss: 5.3089985847473145\n",
      "Epoch [1485/2000], Loss: 5.595858573913574\n",
      "Epoch [1486/2000], Loss: 5.611867427825928\n",
      "Epoch [1487/2000], Loss: 5.828884124755859\n",
      "Epoch [1488/2000], Loss: 5.3079915046691895\n",
      "Epoch [1489/2000], Loss: 5.444850921630859\n",
      "Epoch [1490/2000], Loss: 5.439547538757324\n",
      "Epoch [1491/2000], Loss: 5.539473533630371\n",
      "Epoch [1492/2000], Loss: 5.450464248657227\n",
      "Epoch [1493/2000], Loss: 5.88370418548584\n",
      "Epoch [1494/2000], Loss: 5.636977195739746\n",
      "Epoch [1495/2000], Loss: 5.627126693725586\n",
      "Epoch [1496/2000], Loss: 5.24693489074707\n",
      "Epoch [1497/2000], Loss: 5.522580623626709\n",
      "Epoch [1498/2000], Loss: 5.558474540710449\n",
      "Epoch [1499/2000], Loss: 5.440045356750488\n",
      "Epoch [1500/2000], Loss: 5.706204891204834\n",
      "Epoch [1501/2000], Loss: 5.217566013336182\n",
      "Epoch [1502/2000], Loss: 5.413827419281006\n",
      "Epoch [1503/2000], Loss: 5.655760765075684\n",
      "Epoch [1504/2000], Loss: 5.422808647155762\n",
      "Epoch [1505/2000], Loss: 5.692707061767578\n",
      "Epoch [1506/2000], Loss: 5.527918815612793\n",
      "Epoch [1507/2000], Loss: 5.513278484344482\n",
      "Epoch [1508/2000], Loss: 5.67881441116333\n",
      "Epoch [1509/2000], Loss: 5.481461524963379\n",
      "Epoch [1510/2000], Loss: 5.459835529327393\n",
      "Epoch [1511/2000], Loss: 5.811580657958984\n",
      "Epoch [1512/2000], Loss: 5.797543048858643\n",
      "Epoch [1513/2000], Loss: 5.716549873352051\n",
      "Epoch [1514/2000], Loss: 5.539736747741699\n",
      "Epoch [1515/2000], Loss: 5.532917499542236\n",
      "Epoch [1516/2000], Loss: 5.279837131500244\n",
      "Epoch [1517/2000], Loss: 5.203295707702637\n",
      "Epoch [1518/2000], Loss: 5.559101581573486\n",
      "Epoch [1519/2000], Loss: 5.281198501586914\n",
      "Epoch [1520/2000], Loss: 5.053577899932861\n",
      "Epoch [1521/2000], Loss: 5.523663520812988\n",
      "Epoch [1522/2000], Loss: 5.3448686599731445\n",
      "Epoch [1523/2000], Loss: 5.7251715660095215\n",
      "Epoch [1524/2000], Loss: 5.53874397277832\n",
      "Epoch [1525/2000], Loss: 5.3895087242126465\n",
      "Epoch [1526/2000], Loss: 5.727242946624756\n",
      "Epoch [1527/2000], Loss: 5.574777126312256\n",
      "Epoch [1528/2000], Loss: 5.504833698272705\n",
      "Epoch [1529/2000], Loss: 5.469310760498047\n",
      "Epoch [1530/2000], Loss: 5.6391730308532715\n",
      "Epoch [1531/2000], Loss: 5.685245990753174\n",
      "Epoch [1532/2000], Loss: 5.712005615234375\n",
      "Epoch [1533/2000], Loss: 5.323606491088867\n",
      "Epoch [1534/2000], Loss: 5.60297966003418\n",
      "Epoch [1535/2000], Loss: 5.629815101623535\n",
      "Epoch [1536/2000], Loss: 5.63867712020874\n",
      "Epoch [1537/2000], Loss: 5.5622687339782715\n",
      "Epoch [1538/2000], Loss: 5.8337016105651855\n",
      "Epoch [1539/2000], Loss: 5.5565361976623535\n",
      "Epoch [1540/2000], Loss: 5.41906213760376\n",
      "Epoch [1541/2000], Loss: 5.754227161407471\n",
      "Epoch [1542/2000], Loss: 5.472082614898682\n",
      "Epoch [1543/2000], Loss: 5.555078506469727\n",
      "Epoch [1544/2000], Loss: 5.757140636444092\n",
      "Epoch [1545/2000], Loss: 5.615607261657715\n",
      "Epoch [1546/2000], Loss: 5.446372032165527\n",
      "Epoch [1547/2000], Loss: 5.557055473327637\n",
      "Epoch [1548/2000], Loss: 5.4224348068237305\n",
      "Epoch [1549/2000], Loss: 5.414572238922119\n",
      "Epoch [1550/2000], Loss: 5.697895050048828\n",
      "Epoch [1551/2000], Loss: 5.409895896911621\n",
      "Epoch [1552/2000], Loss: 5.352080821990967\n",
      "Epoch [1553/2000], Loss: 5.787807464599609\n",
      "Epoch [1554/2000], Loss: 5.334325313568115\n",
      "Epoch [1555/2000], Loss: 5.4313483238220215\n",
      "Epoch [1556/2000], Loss: 5.646420955657959\n",
      "Epoch [1557/2000], Loss: 5.575897216796875\n",
      "Epoch [1558/2000], Loss: 5.358660697937012\n",
      "Epoch [1559/2000], Loss: 5.316728591918945\n",
      "Epoch [1560/2000], Loss: 5.396223068237305\n",
      "Epoch [1561/2000], Loss: 5.642523288726807\n",
      "Epoch [1562/2000], Loss: 5.5151824951171875\n",
      "Epoch [1563/2000], Loss: 5.210621356964111\n",
      "Epoch [1564/2000], Loss: 5.051565647125244\n",
      "Epoch [1565/2000], Loss: 5.453838348388672\n",
      "Epoch [1566/2000], Loss: 5.446259498596191\n",
      "Epoch [1567/2000], Loss: 5.810078144073486\n",
      "Epoch [1568/2000], Loss: 5.643589019775391\n",
      "Epoch [1569/2000], Loss: 5.515895843505859\n",
      "Epoch [1570/2000], Loss: 5.430796146392822\n",
      "Epoch [1571/2000], Loss: 5.3641438484191895\n",
      "Epoch [1572/2000], Loss: 5.857620716094971\n",
      "Epoch [1573/2000], Loss: 5.6968302726745605\n",
      "Epoch [1574/2000], Loss: 5.521976947784424\n",
      "Epoch [1575/2000], Loss: 5.200830459594727\n",
      "Epoch [1576/2000], Loss: 5.433374881744385\n",
      "Epoch [1577/2000], Loss: 5.626037120819092\n",
      "Epoch [1578/2000], Loss: 5.244922161102295\n",
      "Epoch [1579/2000], Loss: 5.373820781707764\n",
      "Epoch [1580/2000], Loss: 5.5999884605407715\n",
      "Epoch [1581/2000], Loss: 5.696847915649414\n",
      "Epoch [1582/2000], Loss: 5.84618616104126\n",
      "Epoch [1583/2000], Loss: 5.360918998718262\n",
      "Epoch [1584/2000], Loss: 5.780118465423584\n",
      "Epoch [1585/2000], Loss: 5.685730457305908\n",
      "Epoch [1586/2000], Loss: 5.752222537994385\n",
      "Epoch [1587/2000], Loss: 5.565278053283691\n",
      "Epoch [1588/2000], Loss: 5.878473281860352\n",
      "Epoch [1589/2000], Loss: 5.6759748458862305\n",
      "Epoch [1590/2000], Loss: 5.549877166748047\n",
      "Epoch [1591/2000], Loss: 5.388620376586914\n",
      "Epoch [1592/2000], Loss: 5.594675064086914\n",
      "Epoch [1593/2000], Loss: 5.787051677703857\n",
      "Epoch [1594/2000], Loss: 5.698065280914307\n",
      "Epoch [1595/2000], Loss: 5.326632976531982\n",
      "Epoch [1596/2000], Loss: 5.390621185302734\n",
      "Epoch [1597/2000], Loss: 5.557740211486816\n",
      "Epoch [1598/2000], Loss: 5.236796855926514\n",
      "Epoch [1599/2000], Loss: 5.354239463806152\n",
      "Epoch [1600/2000], Loss: 5.305296897888184\n",
      "Epoch [1601/2000], Loss: 5.365546226501465\n",
      "Epoch [1602/2000], Loss: 5.510489463806152\n",
      "Epoch [1603/2000], Loss: 5.596978664398193\n",
      "Epoch [1604/2000], Loss: 5.7575554847717285\n",
      "Epoch [1605/2000], Loss: 5.628830432891846\n",
      "Epoch [1606/2000], Loss: 5.433862209320068\n",
      "Epoch [1607/2000], Loss: 5.707853317260742\n",
      "Epoch [1608/2000], Loss: 5.467568874359131\n",
      "Epoch [1609/2000], Loss: 5.358168125152588\n",
      "Epoch [1610/2000], Loss: 5.890417575836182\n",
      "Epoch [1611/2000], Loss: 5.4871134757995605\n",
      "Epoch [1612/2000], Loss: 5.733846187591553\n",
      "Epoch [1613/2000], Loss: 5.213226795196533\n",
      "Epoch [1614/2000], Loss: 5.540889263153076\n",
      "Epoch [1615/2000], Loss: 5.532888889312744\n",
      "Epoch [1616/2000], Loss: 5.4236369132995605\n",
      "Epoch [1617/2000], Loss: 5.44435453414917\n",
      "Epoch [1618/2000], Loss: 5.320886611938477\n",
      "Epoch [1619/2000], Loss: 6.088638782501221\n",
      "Epoch [1620/2000], Loss: 5.444883346557617\n",
      "Epoch [1621/2000], Loss: 5.3244757652282715\n",
      "Epoch [1622/2000], Loss: 6.002526760101318\n",
      "Epoch [1623/2000], Loss: 5.5648298263549805\n",
      "Epoch [1624/2000], Loss: 5.778494834899902\n",
      "Epoch [1625/2000], Loss: 5.2053704261779785\n",
      "Epoch [1626/2000], Loss: 5.6926493644714355\n",
      "Epoch [1627/2000], Loss: 5.248279571533203\n",
      "Epoch [1628/2000], Loss: 4.940606594085693\n",
      "Epoch [1629/2000], Loss: 5.496016025543213\n",
      "Epoch [1630/2000], Loss: 5.513996601104736\n",
      "Epoch [1631/2000], Loss: 5.56974458694458\n",
      "Epoch [1632/2000], Loss: 5.57703161239624\n",
      "Epoch [1633/2000], Loss: 5.429051399230957\n",
      "Epoch [1634/2000], Loss: 5.165303707122803\n",
      "Epoch [1635/2000], Loss: 5.419000148773193\n",
      "Epoch [1636/2000], Loss: 5.388743877410889\n",
      "Epoch [1637/2000], Loss: 5.683355331420898\n",
      "Epoch [1638/2000], Loss: 5.846879482269287\n",
      "Epoch [1639/2000], Loss: 5.373841762542725\n",
      "Epoch [1640/2000], Loss: 5.400390148162842\n",
      "Epoch [1641/2000], Loss: 5.415778160095215\n",
      "Epoch [1642/2000], Loss: 5.49249267578125\n",
      "Epoch [1643/2000], Loss: 5.347843647003174\n",
      "Epoch [1644/2000], Loss: 5.555543899536133\n",
      "Epoch [1645/2000], Loss: 5.279794216156006\n",
      "Epoch [1646/2000], Loss: 6.066958427429199\n",
      "Epoch [1647/2000], Loss: 5.752147197723389\n",
      "Epoch [1648/2000], Loss: 5.76393461227417\n",
      "Epoch [1649/2000], Loss: 5.324469566345215\n",
      "Epoch [1650/2000], Loss: 5.331512451171875\n",
      "Epoch [1651/2000], Loss: 5.334427356719971\n",
      "Epoch [1652/2000], Loss: 5.038349628448486\n",
      "Epoch [1653/2000], Loss: 5.540849208831787\n",
      "Epoch [1654/2000], Loss: 5.921004772186279\n",
      "Epoch [1655/2000], Loss: 5.338207244873047\n",
      "Epoch [1656/2000], Loss: 5.540558338165283\n",
      "Epoch [1657/2000], Loss: 5.684905052185059\n",
      "Epoch [1658/2000], Loss: 5.462769985198975\n",
      "Epoch [1659/2000], Loss: 5.724062919616699\n",
      "Epoch [1660/2000], Loss: 5.161868572235107\n",
      "Epoch [1661/2000], Loss: 5.576307773590088\n",
      "Epoch [1662/2000], Loss: 5.447680473327637\n",
      "Epoch [1663/2000], Loss: 5.545257091522217\n",
      "Epoch [1664/2000], Loss: 5.585550308227539\n",
      "Epoch [1665/2000], Loss: 5.423653602600098\n",
      "Epoch [1666/2000], Loss: 5.7139081954956055\n",
      "Epoch [1667/2000], Loss: 5.508358955383301\n",
      "Epoch [1668/2000], Loss: 5.738054275512695\n",
      "Epoch [1669/2000], Loss: 5.253374099731445\n",
      "Epoch [1670/2000], Loss: 5.078335285186768\n",
      "Epoch [1671/2000], Loss: 5.3331298828125\n",
      "Epoch [1672/2000], Loss: 5.560922145843506\n",
      "Epoch [1673/2000], Loss: 5.254716873168945\n",
      "Epoch [1674/2000], Loss: 5.79362154006958\n",
      "Epoch [1675/2000], Loss: 5.35988712310791\n",
      "Epoch [1676/2000], Loss: 5.931735992431641\n",
      "Epoch [1677/2000], Loss: 5.4200215339660645\n",
      "Epoch [1678/2000], Loss: 5.447624683380127\n",
      "Epoch [1679/2000], Loss: 5.500635147094727\n",
      "Epoch [1680/2000], Loss: 5.6287736892700195\n",
      "Epoch [1681/2000], Loss: 5.573966026306152\n",
      "Epoch [1682/2000], Loss: 5.792418956756592\n",
      "Epoch [1683/2000], Loss: 5.727023124694824\n",
      "Epoch [1684/2000], Loss: 5.410486221313477\n",
      "Epoch [1685/2000], Loss: 5.972546577453613\n",
      "Epoch [1686/2000], Loss: 5.488944053649902\n",
      "Epoch [1687/2000], Loss: 5.459672927856445\n",
      "Epoch [1688/2000], Loss: 5.9064812660217285\n",
      "Epoch [1689/2000], Loss: 5.5554046630859375\n",
      "Epoch [1690/2000], Loss: 5.4874982833862305\n",
      "Epoch [1691/2000], Loss: 5.345710277557373\n",
      "Epoch [1692/2000], Loss: 5.73281717300415\n",
      "Epoch [1693/2000], Loss: 5.576435565948486\n",
      "Epoch [1694/2000], Loss: 5.59064245223999\n",
      "Epoch [1695/2000], Loss: 5.296692371368408\n",
      "Epoch [1696/2000], Loss: 5.646546363830566\n",
      "Epoch [1697/2000], Loss: 5.679013252258301\n",
      "Epoch [1698/2000], Loss: 5.304656982421875\n",
      "Epoch [1699/2000], Loss: 5.398807525634766\n",
      "Epoch [1700/2000], Loss: 5.379299163818359\n",
      "Epoch [1701/2000], Loss: 5.3049092292785645\n",
      "Epoch [1702/2000], Loss: 5.804971218109131\n",
      "Epoch [1703/2000], Loss: 5.305966854095459\n",
      "Epoch [1704/2000], Loss: 5.141199588775635\n",
      "Epoch [1705/2000], Loss: 5.42034912109375\n",
      "Epoch [1706/2000], Loss: 6.111800193786621\n",
      "Epoch [1707/2000], Loss: 5.616791248321533\n",
      "Epoch [1708/2000], Loss: 5.659801006317139\n",
      "Epoch [1709/2000], Loss: 5.6161346435546875\n",
      "Epoch [1710/2000], Loss: 5.649686813354492\n",
      "Epoch [1711/2000], Loss: 5.367452621459961\n",
      "Epoch [1712/2000], Loss: 5.333735466003418\n",
      "Epoch [1713/2000], Loss: 5.178432464599609\n",
      "Epoch [1714/2000], Loss: 5.5761942863464355\n",
      "Epoch [1715/2000], Loss: 5.781834125518799\n",
      "Epoch [1716/2000], Loss: 5.460928916931152\n",
      "Epoch [1717/2000], Loss: 5.617798328399658\n",
      "Epoch [1718/2000], Loss: 5.280369281768799\n",
      "Epoch [1719/2000], Loss: 5.185205459594727\n",
      "Epoch [1720/2000], Loss: 5.132875442504883\n",
      "Epoch [1721/2000], Loss: 5.879715442657471\n",
      "Epoch [1722/2000], Loss: 5.529755115509033\n",
      "Epoch [1723/2000], Loss: 5.49129581451416\n",
      "Epoch [1724/2000], Loss: 6.203197956085205\n",
      "Epoch [1725/2000], Loss: 5.3970746994018555\n",
      "Epoch [1726/2000], Loss: 5.206859588623047\n",
      "Epoch [1727/2000], Loss: 5.4559006690979\n",
      "Epoch [1728/2000], Loss: 5.330618858337402\n",
      "Epoch [1729/2000], Loss: 5.314341068267822\n",
      "Epoch [1730/2000], Loss: 5.644199848175049\n",
      "Epoch [1731/2000], Loss: 5.285998821258545\n",
      "Epoch [1732/2000], Loss: 5.457381725311279\n",
      "Epoch [1733/2000], Loss: 5.77388858795166\n",
      "Epoch [1734/2000], Loss: 5.5080156326293945\n",
      "Epoch [1735/2000], Loss: 5.249257564544678\n",
      "Epoch [1736/2000], Loss: 5.357790470123291\n",
      "Epoch [1737/2000], Loss: 5.453955173492432\n",
      "Epoch [1738/2000], Loss: 5.418087959289551\n",
      "Epoch [1739/2000], Loss: 5.557528495788574\n",
      "Epoch [1740/2000], Loss: 5.33449125289917\n",
      "Epoch [1741/2000], Loss: 5.833328723907471\n",
      "Epoch [1742/2000], Loss: 5.467151641845703\n",
      "Epoch [1743/2000], Loss: 5.501873016357422\n",
      "Epoch [1744/2000], Loss: 5.182394504547119\n",
      "Epoch [1745/2000], Loss: 5.277251243591309\n",
      "Epoch [1746/2000], Loss: 5.3244218826293945\n",
      "Epoch [1747/2000], Loss: 5.649309158325195\n",
      "Epoch [1748/2000], Loss: 5.443991184234619\n",
      "Epoch [1749/2000], Loss: 6.014523029327393\n",
      "Epoch [1750/2000], Loss: 5.605554580688477\n",
      "Epoch [1751/2000], Loss: 5.750185012817383\n",
      "Epoch [1752/2000], Loss: 5.90512228012085\n",
      "Epoch [1753/2000], Loss: 5.705875873565674\n",
      "Epoch [1754/2000], Loss: 5.244991302490234\n",
      "Epoch [1755/2000], Loss: 5.306700229644775\n",
      "Epoch [1756/2000], Loss: 5.278401851654053\n",
      "Epoch [1757/2000], Loss: 5.175461769104004\n",
      "Epoch [1758/2000], Loss: 5.428799629211426\n",
      "Epoch [1759/2000], Loss: 5.471315860748291\n",
      "Epoch [1760/2000], Loss: 5.833134174346924\n",
      "Epoch [1761/2000], Loss: 5.283187389373779\n",
      "Epoch [1762/2000], Loss: 5.5464253425598145\n",
      "Epoch [1763/2000], Loss: 5.488091468811035\n",
      "Epoch [1764/2000], Loss: 5.4975433349609375\n",
      "Epoch [1765/2000], Loss: 5.8123321533203125\n",
      "Epoch [1766/2000], Loss: 5.53148889541626\n",
      "Epoch [1767/2000], Loss: 5.3673095703125\n",
      "Epoch [1768/2000], Loss: 5.460036754608154\n",
      "Epoch [1769/2000], Loss: 5.888128757476807\n",
      "Epoch [1770/2000], Loss: 5.29199743270874\n",
      "Epoch [1771/2000], Loss: 5.968127250671387\n",
      "Epoch [1772/2000], Loss: 5.784644603729248\n",
      "Epoch [1773/2000], Loss: 5.684970855712891\n",
      "Epoch [1774/2000], Loss: 5.597466468811035\n",
      "Epoch [1775/2000], Loss: 5.078543186187744\n",
      "Epoch [1776/2000], Loss: 5.306196689605713\n",
      "Epoch [1777/2000], Loss: 5.199974536895752\n",
      "Epoch [1778/2000], Loss: 5.542000770568848\n",
      "Epoch [1779/2000], Loss: 5.281247615814209\n",
      "Epoch [1780/2000], Loss: 5.537926197052002\n",
      "Epoch [1781/2000], Loss: 5.429965019226074\n",
      "Epoch [1782/2000], Loss: 5.234926700592041\n",
      "Epoch [1783/2000], Loss: 5.750542640686035\n",
      "Epoch [1784/2000], Loss: 5.652805328369141\n",
      "Epoch [1785/2000], Loss: 5.098965644836426\n",
      "Epoch [1786/2000], Loss: 5.365832805633545\n",
      "Epoch [1787/2000], Loss: 5.482570171356201\n",
      "Epoch [1788/2000], Loss: 5.477950572967529\n",
      "Epoch [1789/2000], Loss: 5.434113025665283\n",
      "Epoch [1790/2000], Loss: 5.4943976402282715\n",
      "Epoch [1791/2000], Loss: 5.350497245788574\n",
      "Epoch [1792/2000], Loss: 5.255735874176025\n",
      "Epoch [1793/2000], Loss: 5.421611309051514\n",
      "Epoch [1794/2000], Loss: 5.475088596343994\n",
      "Epoch [1795/2000], Loss: 5.349628448486328\n",
      "Epoch [1796/2000], Loss: 5.36807107925415\n",
      "Epoch [1797/2000], Loss: 5.512150764465332\n",
      "Epoch [1798/2000], Loss: 5.906575679779053\n",
      "Epoch [1799/2000], Loss: 5.268250465393066\n",
      "Epoch [1800/2000], Loss: 5.353738784790039\n",
      "Epoch [1801/2000], Loss: 5.417524337768555\n",
      "Epoch [1802/2000], Loss: 5.219162464141846\n",
      "Epoch [1803/2000], Loss: 5.27628231048584\n",
      "Epoch [1804/2000], Loss: 5.4215898513793945\n",
      "Epoch [1805/2000], Loss: 5.253859043121338\n",
      "Epoch [1806/2000], Loss: 5.775813102722168\n",
      "Epoch [1807/2000], Loss: 5.556857109069824\n",
      "Epoch [1808/2000], Loss: 5.121374607086182\n",
      "Epoch [1809/2000], Loss: 5.256295204162598\n",
      "Epoch [1810/2000], Loss: 5.420956134796143\n",
      "Epoch [1811/2000], Loss: 5.448854446411133\n",
      "Epoch [1812/2000], Loss: 5.618577480316162\n",
      "Epoch [1813/2000], Loss: 5.511949062347412\n",
      "Epoch [1814/2000], Loss: 5.389947414398193\n",
      "Epoch [1815/2000], Loss: 5.656223297119141\n",
      "Epoch [1816/2000], Loss: 5.735544681549072\n",
      "Epoch [1817/2000], Loss: 5.402283668518066\n",
      "Epoch [1818/2000], Loss: 5.533947944641113\n",
      "Epoch [1819/2000], Loss: 5.7603230476379395\n",
      "Epoch [1820/2000], Loss: 5.507575035095215\n",
      "Epoch [1821/2000], Loss: 5.631357192993164\n",
      "Epoch [1822/2000], Loss: 5.316921234130859\n",
      "Epoch [1823/2000], Loss: 5.586460113525391\n",
      "Epoch [1824/2000], Loss: 5.173741817474365\n",
      "Epoch [1825/2000], Loss: 5.374305248260498\n",
      "Epoch [1826/2000], Loss: 5.300882816314697\n",
      "Epoch [1827/2000], Loss: 5.519200801849365\n",
      "Epoch [1828/2000], Loss: 5.817910671234131\n",
      "Epoch [1829/2000], Loss: 5.29691743850708\n",
      "Epoch [1830/2000], Loss: 5.3517069816589355\n",
      "Epoch [1831/2000], Loss: 5.38546895980835\n",
      "Epoch [1832/2000], Loss: 5.451926231384277\n",
      "Epoch [1833/2000], Loss: 5.582987308502197\n",
      "Epoch [1834/2000], Loss: 5.138927459716797\n",
      "Epoch [1835/2000], Loss: 5.509808540344238\n",
      "Epoch [1836/2000], Loss: 5.706827640533447\n",
      "Epoch [1837/2000], Loss: 5.2818779945373535\n",
      "Epoch [1838/2000], Loss: 5.507400035858154\n",
      "Epoch [1839/2000], Loss: 5.687110424041748\n",
      "Epoch [1840/2000], Loss: 5.3973774909973145\n",
      "Epoch [1841/2000], Loss: 5.717433452606201\n",
      "Epoch [1842/2000], Loss: 5.325807094573975\n",
      "Epoch [1843/2000], Loss: 5.5043721199035645\n",
      "Epoch [1844/2000], Loss: 5.458811283111572\n",
      "Epoch [1845/2000], Loss: 5.271246910095215\n",
      "Epoch [1846/2000], Loss: 5.764307975769043\n",
      "Epoch [1847/2000], Loss: 5.793015003204346\n",
      "Epoch [1848/2000], Loss: 5.489563465118408\n",
      "Epoch [1849/2000], Loss: 5.4888763427734375\n",
      "Epoch [1850/2000], Loss: 5.352501392364502\n",
      "Epoch [1851/2000], Loss: 5.580408096313477\n",
      "Epoch [1852/2000], Loss: 5.042647361755371\n",
      "Epoch [1853/2000], Loss: 5.579118728637695\n",
      "Epoch [1854/2000], Loss: 5.116884708404541\n",
      "Epoch [1855/2000], Loss: 5.2916765213012695\n",
      "Epoch [1856/2000], Loss: 5.637159824371338\n",
      "Epoch [1857/2000], Loss: 5.086355686187744\n",
      "Epoch [1858/2000], Loss: 5.406017303466797\n",
      "Epoch [1859/2000], Loss: 5.501043319702148\n",
      "Epoch [1860/2000], Loss: 5.317368984222412\n",
      "Epoch [1861/2000], Loss: 5.253914833068848\n",
      "Epoch [1862/2000], Loss: 5.626108646392822\n",
      "Epoch [1863/2000], Loss: 5.02681303024292\n",
      "Epoch [1864/2000], Loss: 5.283753395080566\n",
      "Epoch [1865/2000], Loss: 5.4480671882629395\n",
      "Epoch [1866/2000], Loss: 5.263591289520264\n",
      "Epoch [1867/2000], Loss: 5.813964366912842\n",
      "Epoch [1868/2000], Loss: 5.401369571685791\n",
      "Epoch [1869/2000], Loss: 5.477224826812744\n",
      "Epoch [1870/2000], Loss: 5.087800025939941\n",
      "Epoch [1871/2000], Loss: 5.515524387359619\n",
      "Epoch [1872/2000], Loss: 5.783883571624756\n",
      "Epoch [1873/2000], Loss: 5.561549663543701\n",
      "Epoch [1874/2000], Loss: 5.6204833984375\n",
      "Epoch [1875/2000], Loss: 5.440808296203613\n",
      "Epoch [1876/2000], Loss: 5.309046745300293\n",
      "Epoch [1877/2000], Loss: 5.44058895111084\n",
      "Epoch [1878/2000], Loss: 5.538796424865723\n",
      "Epoch [1879/2000], Loss: 5.360761642456055\n",
      "Epoch [1880/2000], Loss: 5.406181812286377\n",
      "Epoch [1881/2000], Loss: 5.9795122146606445\n",
      "Epoch [1882/2000], Loss: 5.831697463989258\n",
      "Epoch [1883/2000], Loss: 5.919564723968506\n",
      "Epoch [1884/2000], Loss: 5.58179235458374\n",
      "Epoch [1885/2000], Loss: 5.58251953125\n",
      "Epoch [1886/2000], Loss: 5.331613540649414\n",
      "Epoch [1887/2000], Loss: 5.487576484680176\n",
      "Epoch [1888/2000], Loss: 5.389760494232178\n",
      "Epoch [1889/2000], Loss: 5.36055326461792\n",
      "Epoch [1890/2000], Loss: 5.381211757659912\n",
      "Epoch [1891/2000], Loss: 5.352166652679443\n",
      "Epoch [1892/2000], Loss: 5.594875812530518\n",
      "Epoch [1893/2000], Loss: 5.557164192199707\n",
      "Epoch [1894/2000], Loss: 5.123013019561768\n",
      "Epoch [1895/2000], Loss: 5.108209609985352\n",
      "Epoch [1896/2000], Loss: 5.080036163330078\n",
      "Epoch [1897/2000], Loss: 5.552035808563232\n",
      "Epoch [1898/2000], Loss: 5.563048362731934\n",
      "Epoch [1899/2000], Loss: 5.337748050689697\n",
      "Epoch [1900/2000], Loss: 5.733003616333008\n",
      "Epoch [1901/2000], Loss: 5.425631523132324\n",
      "Epoch [1902/2000], Loss: 5.345851898193359\n",
      "Epoch [1903/2000], Loss: 5.373558521270752\n",
      "Epoch [1904/2000], Loss: 5.670848369598389\n",
      "Epoch [1905/2000], Loss: 5.6733503341674805\n",
      "Epoch [1906/2000], Loss: 5.038317680358887\n",
      "Epoch [1907/2000], Loss: 5.364333629608154\n",
      "Epoch [1908/2000], Loss: 5.253676414489746\n",
      "Epoch [1909/2000], Loss: 5.635863780975342\n",
      "Epoch [1910/2000], Loss: 5.396130561828613\n",
      "Epoch [1911/2000], Loss: 5.172260284423828\n",
      "Epoch [1912/2000], Loss: 5.367401599884033\n",
      "Epoch [1913/2000], Loss: 6.117905616760254\n",
      "Epoch [1914/2000], Loss: 5.679535865783691\n",
      "Epoch [1915/2000], Loss: 5.823787212371826\n",
      "Epoch [1916/2000], Loss: 5.545227527618408\n",
      "Epoch [1917/2000], Loss: 5.304992198944092\n",
      "Epoch [1918/2000], Loss: 5.250254154205322\n",
      "Epoch [1919/2000], Loss: 5.522947788238525\n",
      "Epoch [1920/2000], Loss: 5.164023399353027\n",
      "Epoch [1921/2000], Loss: 5.8190693855285645\n",
      "Epoch [1922/2000], Loss: 5.6760382652282715\n",
      "Epoch [1923/2000], Loss: 5.331113338470459\n",
      "Epoch [1924/2000], Loss: 5.379002571105957\n",
      "Epoch [1925/2000], Loss: 5.4782586097717285\n",
      "Epoch [1926/2000], Loss: 5.685330390930176\n",
      "Epoch [1927/2000], Loss: 5.261232852935791\n",
      "Epoch [1928/2000], Loss: 5.532106399536133\n",
      "Epoch [1929/2000], Loss: 5.280154705047607\n",
      "Epoch [1930/2000], Loss: 5.170825958251953\n",
      "Epoch [1931/2000], Loss: 5.195740222930908\n",
      "Epoch [1932/2000], Loss: 5.674902439117432\n",
      "Epoch [1933/2000], Loss: 5.403223037719727\n",
      "Epoch [1934/2000], Loss: 5.2727789878845215\n",
      "Epoch [1935/2000], Loss: 5.400036811828613\n",
      "Epoch [1936/2000], Loss: 5.5497870445251465\n",
      "Epoch [1937/2000], Loss: 5.368426322937012\n",
      "Epoch [1938/2000], Loss: 5.135286331176758\n",
      "Epoch [1939/2000], Loss: 5.094425678253174\n",
      "Epoch [1940/2000], Loss: 5.45529317855835\n",
      "Epoch [1941/2000], Loss: 5.409769058227539\n",
      "Epoch [1942/2000], Loss: 5.920464992523193\n",
      "Epoch [1943/2000], Loss: 5.3009161949157715\n",
      "Epoch [1944/2000], Loss: 5.007885456085205\n",
      "Epoch [1945/2000], Loss: 4.916165828704834\n",
      "Epoch [1946/2000], Loss: 5.0258564949035645\n",
      "Epoch [1947/2000], Loss: 5.997498512268066\n",
      "Epoch [1948/2000], Loss: 5.802241325378418\n",
      "Epoch [1949/2000], Loss: 5.959349632263184\n",
      "Epoch [1950/2000], Loss: 5.204329013824463\n",
      "Epoch [1951/2000], Loss: 5.540678024291992\n",
      "Epoch [1952/2000], Loss: 5.378391265869141\n",
      "Epoch [1953/2000], Loss: 5.288166522979736\n",
      "Epoch [1954/2000], Loss: 5.438144683837891\n",
      "Epoch [1955/2000], Loss: 5.5957818031311035\n",
      "Epoch [1956/2000], Loss: 5.233208179473877\n",
      "Epoch [1957/2000], Loss: 5.047696590423584\n",
      "Epoch [1958/2000], Loss: 5.323944568634033\n",
      "Epoch [1959/2000], Loss: 5.444822311401367\n",
      "Epoch [1960/2000], Loss: 5.516439914703369\n",
      "Epoch [1961/2000], Loss: 5.749636650085449\n",
      "Epoch [1962/2000], Loss: 5.373671531677246\n",
      "Epoch [1963/2000], Loss: 5.58951997756958\n",
      "Epoch [1964/2000], Loss: 5.089691638946533\n",
      "Epoch [1965/2000], Loss: 5.201131820678711\n",
      "Epoch [1966/2000], Loss: 5.572421550750732\n",
      "Epoch [1967/2000], Loss: 5.288066387176514\n",
      "Epoch [1968/2000], Loss: 5.666950702667236\n",
      "Epoch [1969/2000], Loss: 5.459832191467285\n",
      "Epoch [1970/2000], Loss: 5.6473259925842285\n",
      "Epoch [1971/2000], Loss: 5.563956260681152\n",
      "Epoch [1972/2000], Loss: 5.267228603363037\n",
      "Epoch [1973/2000], Loss: 5.48810338973999\n",
      "Epoch [1974/2000], Loss: 5.826257228851318\n",
      "Epoch [1975/2000], Loss: 5.317783832550049\n",
      "Epoch [1976/2000], Loss: 5.445436477661133\n",
      "Epoch [1977/2000], Loss: 5.161139011383057\n",
      "Epoch [1978/2000], Loss: 5.7687764167785645\n",
      "Epoch [1979/2000], Loss: 5.149148464202881\n",
      "Epoch [1980/2000], Loss: 5.767756462097168\n",
      "Epoch [1981/2000], Loss: 5.576544761657715\n",
      "Epoch [1982/2000], Loss: 5.427639484405518\n",
      "Epoch [1983/2000], Loss: 5.381923198699951\n",
      "Epoch [1984/2000], Loss: 5.398637771606445\n",
      "Epoch [1985/2000], Loss: 5.354634761810303\n",
      "Epoch [1986/2000], Loss: 5.45133113861084\n",
      "Epoch [1987/2000], Loss: 5.428154945373535\n",
      "Epoch [1988/2000], Loss: 5.527498245239258\n",
      "Epoch [1989/2000], Loss: 5.694662570953369\n",
      "Epoch [1990/2000], Loss: 4.9627790451049805\n",
      "Epoch [1991/2000], Loss: 5.364996910095215\n",
      "Epoch [1992/2000], Loss: 5.726822853088379\n",
      "Epoch [1993/2000], Loss: 4.9886579513549805\n",
      "Epoch [1994/2000], Loss: 5.78061580657959\n",
      "Epoch [1995/2000], Loss: 5.034671783447266\n",
      "Epoch [1996/2000], Loss: 5.9513068199157715\n",
      "Epoch [1997/2000], Loss: 5.785465717315674\n",
      "Epoch [1998/2000], Loss: 5.629751682281494\n",
      "Epoch [1999/2000], Loss: 5.199537754058838\n",
      "Epoch [2000/2000], Loss: 5.648032188415527\n"
     ]
    }
   ],
   "source": [
    "# Call the training loop\n",
    "#num_epochs = 1000\n",
    "#batch_size = 512\n",
    "[training_loss, test_loss] = main_train(data_size, programmes_prob_distribution, batch_size, timesteps, train_ratio, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da93bfc7-7619-406d-a240-4d1b37e0a699",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracy of a model \n",
    "#number_of_samples = 2000\n",
    "def model_accuracy(model, data_size, programmes_prob_distribution, number_of_samples, timesteps):\n",
    "\n",
    "    # Note that given this infinite data training regime, new data must be generated to perform this evaluation meaningfully.\n",
    "    [train_loader, test_loader] = data_loader(data_size, programmes_prob_distribution, number_of_samples, timesteps, 0.8)\n",
    "    # Evaluation on the training dataset (relevant for overparametrisation or otherwise deep learning systems)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for data, labels in train_loader:\n",
    "            outputs = model(data)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "        accuracy = 100 * correct / total #returns the accuracy as a percentage\n",
    "        \n",
    "    return accuracy    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be0c84b-1501-4265-b12f-a037633d51a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the known accuracy\n",
    "\n",
    "accuracy = model_accuracy(model, data_size, programmes_prob_distribution, number_of_samples, timesteps)\n",
    "print(f\"Accuracy on the generated set: {accuracy}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5989f503-b3d5-40f9-b155-d8f3a584933d",
   "metadata": {},
   "source": [
    "# Note form work below\n",
    "\n",
    "-------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4ff0a3-5e33-4a32-a72d-243aaccd3e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of how to use the programmes_prob_distribution variable\n",
    "\n",
    "programmes_prob_distribution = [100,5,5,2,3,5,7,3,1,0]\n",
    "programmes_prob_distribution_norm = [x / sum(programmes_prob_distribution) for x in programmes_prob_distribution]\n",
    "programmes = np.arange(0, len(programmes_prob_distribution), 1)\n",
    "for i in range(100):\n",
    "    rule_number = np.random.choice(programmes, size=None, replace=True, p = programmes_prob_distribution_norm)\n",
    "    print(f\"Instance \" + str(i) + \": rule_number = \", rule_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a05f519-f258-4d58-b919-48ae01b55299",
   "metadata": {},
   "outputs": [],
   "source": [
    "programmes_prob_distribution = []\n",
    "for i in range(256):\n",
    "    programmes_prob_distribution.append(np.log(2 + i))\n",
    "programmes_prob_distribution = np.array(programmes_prob_distribution)\n",
    "programmes = np.arange(0,256,1)\n",
    "#print(programmes.shape)\n",
    "\n",
    "#print(programmes_prob_distribution.shape)\n",
    "\n",
    "print(data_loader(100, programmes_prob_distribution, 50, 100, 0.8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a61e54-6830-4157-80da-a8aad5a1650f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple training test  (on a single programme)\n",
    "\n",
    "programmes_prob_distribution = [0] * 256\n",
    "programmes_prob_distribution[0] = 1\n",
    "\n",
    "num_epochs = 1000\n",
    "batch_size = 256\n",
    "main_train(data_size, programmes_prob_distribution, batch_size, timesteps, train_ratio, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab74ee7-0f73-4b70-a374-bb4e7d5b763f",
   "metadata": {},
   "outputs": [],
   "source": [
    "[dataset, labels] = create_data_single(data_size, 150, 1, timesteps)\n",
    "\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11cf3dcc-f194-4ff2-ab3f-55781c1c5cfa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
