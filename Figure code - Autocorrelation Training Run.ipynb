{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e60b95e0-1e1b-4ed9-8232-b75912ff431f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: cellpylib in c:\\users\\plogo\\anaconda3\\envs\\notebook_654\\lib\\site-packages (2.4.0)\n",
      "Requirement already satisfied: numpy>=1.15.4 in c:\\users\\plogo\\anaconda3\\envs\\notebook_654\\lib\\site-packages (from cellpylib) (1.26.4)\n",
      "Requirement already satisfied: matplotlib>=3.0.2 in c:\\users\\plogo\\anaconda3\\envs\\notebook_654\\lib\\site-packages (from cellpylib) (3.8.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\plogo\\anaconda3\\envs\\notebook_654\\lib\\site-packages (from matplotlib>=3.0.2->cellpylib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\plogo\\anaconda3\\envs\\notebook_654\\lib\\site-packages (from matplotlib>=3.0.2->cellpylib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\plogo\\anaconda3\\envs\\notebook_654\\lib\\site-packages (from matplotlib>=3.0.2->cellpylib) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\plogo\\anaconda3\\envs\\notebook_654\\lib\\site-packages (from matplotlib>=3.0.2->cellpylib) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\plogo\\appdata\\roaming\\python\\python311\\site-packages (from matplotlib>=3.0.2->cellpylib) (23.0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\plogo\\anaconda3\\envs\\notebook_654\\lib\\site-packages (from matplotlib>=3.0.2->cellpylib) (10.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\plogo\\anaconda3\\envs\\notebook_654\\lib\\site-packages (from matplotlib>=3.0.2->cellpylib) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\plogo\\appdata\\roaming\\python\\python311\\site-packages (from matplotlib>=3.0.2->cellpylib) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\plogo\\appdata\\roaming\\python\\python311\\site-packages (from python-dateutil>=2.7->matplotlib>=3.0.2->cellpylib) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "# Installing necessary libraries\n",
    "\n",
    "!pip install cellpylib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c45f038-1aa4-4d2d-8297-f002f3372a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "\n",
    "import cellpylib as cpl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch import Tensor\n",
    "import seaborn as sns\n",
    "import time\n",
    "import math\n",
    "import matplotlib.cm as cm # For colourmap plots later\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7817dcef-9781-4658-8ce8-1d019d7fee94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "\n",
    "# Data generation parameters\n",
    "data_size = 100 # the number of data points in each row of data\n",
    "timesteps = 100 # the number of timesteps which each programme is run for before the output is used to train the model\n",
    "\n",
    "# Model parameters\n",
    "num_epochs = 30000  # Number of training epochs\n",
    "hidden_size = 512  # Update with the desired size of the hidden layer\n",
    "learning_rate = 0.005 # learning rate used later in the optimizer\n",
    "batch_size = 128 # Batch size used when creating the train and test datasets. 32 is more suitable for this problem.\n",
    "epochs = np.arange(0,num_epochs, 1) # Used in plotting\n",
    "accuracy_frequency = 200 # Used in the evaluation of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54f69b93-df37-4f97-807c-2c410ef836f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Initialisation / Training setup\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "        self.bn2 = nn.BatchNorm1d(num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.bn2(out)\n",
    "        return out\n",
    "\n",
    "# Define the input size, hidden size, and number of classes\n",
    "input_size = data_size  # Update with the actual input size\n",
    "num_classes = 256 #Number of potential classes, here stuck at 256\n",
    "\n",
    "# Create an instance of the neural network\n",
    "model = NeuralNetwork(input_size, hidden_size, num_classes)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19f20bd0-54d3-4f78-a130-68356c1e69d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the programme probability distribution, weighting highly autocorrelated programmes more heavily.\n",
    "\n",
    "def prog_dist_weight_autocorr():\n",
    "    autocorr_order = [255, 238, 8, 32, 40, 64, 96, 254, 128, 136, 160, 168, 192, 224, \n",
    "                      234, 235, 0, 239, 248, 249, 253, 250, 251, 252, 36, 219, 104, 233, \n",
    "                      218, 164, 172, 2, 16, 237, 191, 4, 228, 127, 72, 247, 223, 216, 202, \n",
    "                      1, 132, 130, 100, 190, 246, 44, 222, 217, 203, 144, 231, 188, 230, 66, \n",
    "                      194, 152, 24, 189, 20, 215, 159, 6, 80, 17, 48, 175, 140, 220, 34,\n",
    "                      119, 207, 12, 206, 95, 196, 221, 245, 3, 243, 68, 63, 10, 146, 187,\n",
    "                      183, 182, 18, 5, 148, 158, 211, 134, 52, 214, 38, 155, 33, 88, 108, \n",
    "                      123, 229, 186, 162, 201, 74, 173, 242, 37, 176, 49, 59, 41, 151, 22, \n",
    "                      236, 82, 35, 115, 114, 167, 121, 58, 56, 139, 118, 138, 21, 111, 31, \n",
    "                      84, 163, 112, 131, 208, 62, 46, 107, 116, 244, 145, 171, 19, 91, 9, \n",
    "                      200, 7, 87, 205, 181, 177, 174, 133, 99, 26, 97, 227, 98, 209, 76, \n",
    "                      185, 124, 42, 65, 241, 184, 198, 126, 13, 93, 81, 212, 67, 23, 141,\n",
    "                      125, 178, 55, 57, 106, 28, 110, 154, 92, 75, 25, 150, 225, 86, 29, 61, \n",
    "                      94, 157, 117, 210, 50, 45, 39, 193, 169, 147, 120, 153, 89, 78, 156,\n",
    "                      137, 15, 161, 73, 70, 103, 43, 204, 11, 122, 60, 85, 179, 170, 102, 30,\n",
    "                      79, 226, 109, 105, 54, 165, 71, 166, 90, 101, 129, 27, 149, 51, 232, 47, \n",
    "                      195, 135, 180, 83, 53, 240, 197, 199, 77, 14, 69, 143, 113, 213, 142]\n",
    "    programmes_prob_distribution = []\n",
    "    for i in range(256):\n",
    "        zipf_weight = (autocorr_order.index(i) + 10)**(-1)\n",
    "        programmes_prob_distribution.append(zipf_weight)\n",
    "    prog_prob_dist_norm = [x / sum(programmes_prob_distribution) for x in programmes_prob_distribution]\n",
    "    return prog_prob_dist_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a3122c25-9a03-442a-b108-0d28bfb06616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets the above distribution as the probability distribution used throughout the training run\n",
    "\n",
    "programmes_prob_distribution = prog_dist_weight_autocorr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ade3b793-7e25-4904-aef7-cb2495b8a6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data generation functions (where the programmes_considered have a probability distribution)\n",
    "\n",
    "def create_data(data_size, programmes_prob_distribution, number_of_samples, timesteps):\n",
    "\n",
    "    # Creating the dataset and labels variables to be populated later\n",
    "    dataset = np.empty(shape=(number_of_samples, data_size), dtype=int) # each row is data_size length, with number_of_samples rows\n",
    "    labels = np.empty(shape=(1, number_of_samples), dtype=int)\n",
    "\n",
    "    # Stating the space of considered programmes\n",
    "    programmes = np.arange(0,256,1)\n",
    "\n",
    "    # Normalising the distribution in case it is not already normalised\n",
    "    programmes_total = sum(programmes_prob_distribution)\n",
    "    programmes_prob_distribution_norm = [x / programmes_total for x in programmes_prob_distribution]\n",
    "    \n",
    "    for i in range(number_of_samples):\n",
    "\n",
    "        # Randomly selecting a rule number according to the probability distribution given\n",
    "        rule_number = np.random.choice(a = programmes, size=None, replace=True, p = programmes_prob_distribution_norm)\n",
    "        #print(f\"Considering rule_number = \", rule_number)\n",
    "        cellular_automaton = cpl.init_random(data_size)\n",
    "        cellular_automaton = cpl.evolve(cellular_automaton, timesteps=timesteps, memoize=True, apply_rule=lambda n, c, t: cpl.nks_rule(n, rule_number))\n",
    "        #print(cellular_automaton[-1])\n",
    "        dataset[i] = cellular_automaton[-1]\n",
    "        labels[:,i] = rule_number\n",
    "\n",
    "    return [dataset, labels]\n",
    "\n",
    "def data_loader(data_size, programmes_prob_distribution, number_of_samples, timesteps):\n",
    "\n",
    "    # Generate the data according to input parameters\n",
    "    [dataset, labels] = create_data(data_size, programmes_prob_distribution, number_of_samples, timesteps)\n",
    "    labels = labels[0] # Deal with the fact that the output is a list of a single list\n",
    "\n",
    "    # Shifting the labels such that they are indexed from 0. Required for cross entropy to work\n",
    "    #labels = [x - min(labels) for x in labels] #!!! Not currently shifting labels in a test to alter them later - may help with training in smaller batches\n",
    "    # Use data_split\n",
    "    data = [(data_sample, label) for data_sample, label in zip(dataset, labels)]\n",
    "\n",
    "    train_dataset, train_labels = zip(*data)\n",
    "    \n",
    "    tensor_train_dataset = TensorDataset(Tensor(train_dataset), Tensor(train_labels))\n",
    "    \n",
    "    train_loader = DataLoader(tensor_train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    return train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82b4fa12-3eda-4953-8c24-d748e9adb244",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluates a model over the space of all possible functions. Outputs a vector giving that performance\n",
    "\n",
    "def model_evaluation(model, data_size, timesteps, batch_size):\n",
    "\n",
    "    # State which programmes are being considered. In this case, it's all of them.\n",
    "    programmes_considered = np.arange(0,256,1) #NOTE: THIS DISABLING IS TEMPORARY. AIM TO REINTRODUCE IT\n",
    "    #programmes_considered = np.array([0, 10, 20, 63, 64, 95, 96, 110, 125, 195, 204, 225, 249, 255]) # The programmes used at the moment in autocorrelation\n",
    "\n",
    "    accuracy_vector = np.empty(256)\n",
    "    \n",
    "    for programme in programmes_considered:\n",
    "\n",
    "        programmes_prob_distribution = [0]*256\n",
    "        programmes_prob_distribution[programme] = 1\n",
    "\n",
    "        train_loader = data_loader(data_size, programmes_prob_distribution, batch_size, timesteps)\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for data, labels in train_loader:\n",
    "                outputs = model(data)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        \n",
    "            accuracy = 100 * correct / total #returns the accuracy as a percentage\n",
    "            #print(f\"For programme \" + str(programme) + \": Accuracy = \" + str(accuracy))\n",
    "        accuracy_vector[programme] = accuracy\n",
    "                \n",
    "    return accuracy_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f380103a-0976-4e8d-8da6-0015b41b3dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop (includes data generation). Note that here training and test loss cease to make much sense\n",
    "\n",
    "def main_train(data_size, programmes_prob_distribution, batch_size, timesteps, num_epochs):\n",
    "\n",
    "    # Initisalise training and test loss tracking variables\n",
    "    training_loss = np.empty(num_epochs)\n",
    "\n",
    "    # Initialise an array to track not only the general training and test loss, but also the accuracy on individual programme classification during training.\n",
    "    # This is to attempt to see grokking.\n",
    "    # Form: Each row of accuracy_array is an epoch, each column of accuracy_array is a binary 1 or 0 based on whether or not it was correctly classified. \n",
    "    #accuracy_frequency = 100 # Once every 100 epochs, the accuracy is measured\n",
    "    accuracy_array = np.empty((math.floor(num_epochs/accuracy_frequency), 256))\n",
    "    # Initialising variaable for tracking where in the accuracy_array to write data\n",
    "    eval_count = 0\n",
    "    # Each epoch here trains over 1 batch size of data (which at the moment is 32). Each epoch is therefore smaller and better controlled.\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "\n",
    "        # Continually monitoring accuracy of the model by adjusting the accuracy_array\n",
    "        if epoch%accuracy_frequency==0:\n",
    "            accuracy_vector = model_evaluation(model, data_size, timesteps, batch_size)\n",
    "            accuracy_array[eval_count] = accuracy_vector\n",
    "            eval_count += 1\n",
    "        \n",
    "        train_loader = data_loader(data_size, programmes_prob_distribution, batch_size, timesteps)\n",
    "        for data, labels in train_loader:\n",
    "            # Forward pass\n",
    "            outputs = model(data)\n",
    "    \n",
    "            #Shifting labels for loss calculation\n",
    "            shifted_labels = labels - torch.min(labels)\n",
    "            shifted_labels = shifted_labels.long()\n",
    "            loss = criterion(outputs, shifted_labels)\n",
    "                \n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Print the loss after each epoch\n",
    "        if epoch%1000==0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}\")\n",
    "        training_loss[epoch] = loss.item()\n",
    "\n",
    "    return [training_loss, accuracy_array]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "03e6918f-3aa1-4954-ab60-851c59657675",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/30000 [00:00<?, ?it/s]C:\\Users\\plogo\\AppData\\Local\\Temp\\ipykernel_9924\\2903999265.py:42: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\b\\abs_6fueooay2f\\croot\\pytorch-select_1707342446212\\work\\torch\\csrc\\utils\\tensor_new.cpp:278.)\n",
      "  tensor_train_dataset = TensorDataset(Tensor(train_dataset), Tensor(train_labels))\n",
      "  0%|          | 1/30000 [07:35<3792:28:02, 455.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30000], Loss: 5.58183479309082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 454/30000 [35:37<38:38:26,  4.71s/it]   \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m [training_loss, accuracy_array] \u001b[38;5;241m=\u001b[39m \u001b[43mmain_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprogrammes_prob_distribution\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimesteps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[9], line 24\u001b[0m, in \u001b[0;36mmain_train\u001b[1;34m(data_size, programmes_prob_distribution, batch_size, timesteps, num_epochs)\u001b[0m\n\u001b[0;32m     21\u001b[0m     accuracy_array[eval_count] \u001b[38;5;241m=\u001b[39m accuracy_vector\n\u001b[0;32m     22\u001b[0m     eval_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 24\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m \u001b[43mdata_loader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprogrammes_prob_distribution\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimesteps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data, labels \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m     27\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(data)\n",
      "Cell \u001b[1;32mIn[7], line 32\u001b[0m, in \u001b[0;36mdata_loader\u001b[1;34m(data_size, programmes_prob_distribution, number_of_samples, timesteps)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdata_loader\u001b[39m(data_size, programmes_prob_distribution, number_of_samples, timesteps):\n\u001b[0;32m     30\u001b[0m \n\u001b[0;32m     31\u001b[0m     \u001b[38;5;66;03m# Generate the data according to input parameters\u001b[39;00m\n\u001b[1;32m---> 32\u001b[0m     [dataset, labels] \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprogrammes_prob_distribution\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumber_of_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimesteps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m     labels \u001b[38;5;241m=\u001b[39m labels[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;66;03m# Deal with the fact that the output is a list of a single list\u001b[39;00m\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;66;03m# Shifting the labels such that they are indexed from 0. Required for cross entropy to work\u001b[39;00m\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;66;03m#labels = [x - min(labels) for x in labels] #!!! Not currently shifting labels in a test to alter them later - may help with training in smaller batches\u001b[39;00m\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;66;03m# Use data_split\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[7], line 22\u001b[0m, in \u001b[0;36mcreate_data\u001b[1;34m(data_size, programmes_prob_distribution, number_of_samples, timesteps)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m#print(f\"Considering rule_number = \", rule_number)\u001b[39;00m\n\u001b[0;32m     21\u001b[0m cellular_automaton \u001b[38;5;241m=\u001b[39m cpl\u001b[38;5;241m.\u001b[39minit_random(data_size)\n\u001b[1;32m---> 22\u001b[0m cellular_automaton \u001b[38;5;241m=\u001b[39m \u001b[43mcpl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevolve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcellular_automaton\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimesteps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemoize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mapply_rule\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcpl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnks_rule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrule_number\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m#print(cellular_automaton[-1])\u001b[39;00m\n\u001b[0;32m     24\u001b[0m dataset[i] \u001b[38;5;241m=\u001b[39m cellular_automaton[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\notebook_654\\Lib\\site-packages\\cellpylib\\ca_functions.py:154\u001b[0m, in \u001b[0;36mevolve\u001b[1;34m(cellular_automaton, timesteps, apply_rule, r, memoize)\u001b[0m\n\u001b[0;32m    152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _evolve_dynamic(cellular_automaton, timesteps, apply_rule, r, memoize)\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_evolve_fixed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcellular_automaton\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimesteps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mapply_rule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemoize\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\notebook_654\\Lib\\site-packages\\cellpylib\\ca_functions.py:199\u001b[0m, in \u001b[0;36m_evolve_fixed\u001b[1;34m(cellular_automaton, timesteps, apply_rule, r, memoize)\u001b[0m\n\u001b[0;32m    197\u001b[0m     array[t] \u001b[38;5;241m=\u001b[39m next_state\n\u001b[0;32m    198\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m memoize \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 199\u001b[0m     array[t] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([_get_memoized(n, c, t, apply_rule, memo_table) \u001b[38;5;28;01mfor\u001b[39;00m c, n \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(neighbourhoods)])\n\u001b[0;32m    200\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m memoize \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m    201\u001b[0m     array[t] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([apply_rule(n, c, t) \u001b[38;5;28;01mfor\u001b[39;00m c, n \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(neighbourhoods)])\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "[training_loss, accuracy_array] = main_train(data_size, programmes_prob_distribution, batch_size, timesteps, num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07859796",
   "metadata": {},
   "source": [
    "# Information with present params\n",
    "\n",
    "For the present parameters as of 10/05/2024, this cell would likely take around 15 hours to run. It would give a very complete display of how programme accuracy changes when all programmes are weighted with autocorrelation. I don't think this plot will be needed, but if it is it is quite easy to generate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6ea2fd-2e46-4bfb-a9d0-825cd31c8854",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot accuracy by programme.\n",
    "\n",
    "rel_epochs = [x for x, index in enumerate(epochs) if index%accuracy_frequency==0]\n",
    "\n",
    "# Create an axis object for the line plot\n",
    "fig, ax = plt.subplots()\n",
    "cmap = cm.get_cmap('rainbow')  # You can choose different colormaps\n",
    "\n",
    "# Taking values from nearby epochs and averaging\n",
    "moving_avg = 10 # The size of the averaging window being used.\n",
    "reshaped_epochs = np.reshape(rel_epochs, (-1, moving_avg))\n",
    "filtered_epochs = reshaped_epochs[:,0]\n",
    "repeated_filtered_epochs = np.repeat(filtered_epochs, moving_avg)\n",
    "\n",
    "for i in tqdm(lines_plotted):\n",
    "    line = accuracy_array[:,i]\n",
    "    #color = cmap(lines_plotted[np.where(lines_plotted==i)[0][0]] / (len(lines_plotted) - 1))\n",
    "    #color = cmap(index / (len(lines_plotted) - 1))\n",
    "    color = cmap(np.where(lines_plotted == i)[0][0] / (len(lines_plotted) - 1))\n",
    "    reshaped_line = np.reshape(line, (-1, moving_avg))\n",
    "    programme_label = 'Programme: ' + str(uncorr_array[np.where(lines_plotted==i)][0])\n",
    "    pandas_df = pd.DataFrame({programme_label: reshaped_line.flatten(), 'Epochs': repeated_filtered_epochs})\n",
    "    pandas_df_melted = pd.melt(pandas_df, id_vars = 'Epochs', value_vars = [programme_label], var_name='line', value_name = 'Values')\n",
    "    #fig, ax = plt.subplots()\n",
    "    sns.lineplot(data=pandas_df_melted, x='Epochs', y='Values', hue='line', ax=ax, palette = [color])\n",
    "\n",
    "# Create a ScalarMappable object for the colorbar\n",
    "norm = plt.Normalize(0, len(lines_plotted) - 1)\n",
    "sm = cm.ScalarMappable(norm=norm, cmap=cmap)\n",
    "sm.set_array([])  # Set an empty array to associate with the colorbar\n",
    "cbar = plt.colorbar(sm, ax=ax)\n",
    "\n",
    "cbar.set_label('Program Number')\n",
    "\n",
    "num_ticks = len(lines_plotted)  # Number of desired ticks\n",
    "indices = np.linspace(0, len(lines_plotted) - 1, num_ticks, dtype=int)\n",
    "tick_positions = indices\n",
    "tick_labels = lines_plotted[indices]\n",
    "\n",
    "cbar.set_ticks(tick_positions)\n",
    "cbar.set_ticklabels(tick_labels)\n",
    "\n",
    "ax.get_legend().remove()\n",
    "\n",
    "plt.ylabel('Accuracy (%)')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
